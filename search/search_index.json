{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PanoOCR # OCR for equirectangular panorama images. Handles perspective projection, coordinate conversion, and text deduplication. Installation # # Lightweight (bring your own engine) pip install \"panoocr @ git+https://github.com/yz3440/panoocr.git\" # With MacOCR (macOS only) pip install \"panoocr[macocr]\" # With EasyOCR (cross-platform) pip install \"panoocr[easyocr]\" # With all engines pip install \"panoocr[full] @ git+https://github.com/yz3440/panoocr.git\" Quick Start # from panoocr import PanoOCR from panoocr.engines.macocr import MacOCREngine engine = MacOCREngine () pano = PanoOCR ( engine ) result = pano . recognize ( \"panorama.jpg\" ) result . save_json ( \"results.ocr.json\" ) Demo # The preview tool visualizes OCR results on an interactive 3D sphere. cd preview && python -m http.server Open http://localhost:8000 and drag in the JSON result file and panorama image. Next # Examples - Working scripts API Reference - Full documentation","title":"Home"},{"location":"#panoocr","text":"OCR for equirectangular panorama images. Handles perspective projection, coordinate conversion, and text deduplication.","title":"PanoOCR"},{"location":"#installation","text":"# Lightweight (bring your own engine) pip install \"panoocr @ git+https://github.com/yz3440/panoocr.git\" # With MacOCR (macOS only) pip install \"panoocr[macocr]\" # With EasyOCR (cross-platform) pip install \"panoocr[easyocr]\" # With all engines pip install \"panoocr[full] @ git+https://github.com/yz3440/panoocr.git\"","title":"Installation"},{"location":"#quick-start","text":"from panoocr import PanoOCR from panoocr.engines.macocr import MacOCREngine engine = MacOCREngine () pano = PanoOCR ( engine ) result = pano . recognize ( \"panorama.jpg\" ) result . save_json ( \"results.ocr.json\" )","title":"Quick Start"},{"location":"#demo","text":"The preview tool visualizes OCR results on an interactive 3D sphere. cd preview && python -m http.server Open http://localhost:8000 and drag in the JSON result file and panorama image.","title":"Demo"},{"location":"#next","text":"Examples - Working scripts API Reference - Full documentation","title":"Next"},{"location":"examples/","text":"Examples # Working scripts in examples/ . basic_usage.py # #!/usr/bin/env python3 \"\"\"Basic usage example for PanoOCR. This script demonstrates the simplest way to run OCR on a panorama image. Usage: python basic_usage.py path/to/panorama.jpg Prerequisites: pip install \"panoocr[macocr]\" # For macOS # or pip install \"panoocr[easyocr]\" # For cross-platform \"\"\" import sys from pathlib import Path def main (): if len ( sys . argv ) < 2 : print ( \"Usage: python basic_usage.py <panorama_image>\" ) print ( \" \\n Example:\" ) print ( \" python basic_usage.py panorama.jpg\" ) sys . exit ( 1 ) image_path = Path ( sys . argv [ 1 ]) if not image_path . exists (): print ( f \"Error: File not found: { image_path } \" ) sys . exit ( 1 ) # Import panoocr from panoocr import PanoOCR , PerspectivePreset # Try to import an OCR engine (prefer MacOCR on macOS, fall back to EasyOCR) engine = None try : from panoocr.engines.macocr import MacOCREngine print ( \"Using MacOCR engine (Apple Vision Framework)\" ) engine = MacOCREngine () except ImportError : pass if engine is None : try : from panoocr.engines.easyocr import EasyOCREngine print ( \"Using EasyOCR engine\" ) engine = EasyOCREngine () except ImportError : pass if engine is None : print ( \"Error: No OCR engine available.\" ) print ( \" \\n Install an OCR engine with:\" ) print ( \" pip install 'panoocr[macocr]' # For macOS\" ) print ( \" pip install 'panoocr[easyocr]' # For cross-platform\" ) sys . exit ( 1 ) # Create the PanoOCR pipeline pano = PanoOCR ( engine , perspectives = PerspectivePreset . DEFAULT , ) # Run OCR print ( f \" \\n Processing: { image_path } \" ) result = pano . recognize ( str ( image_path )) # Print results print ( f \" \\n Found { len ( result . results ) } text regions:\" ) print ( \"-\" * 60 ) for i , r in enumerate ( result . results , 1 ): print ( f \" \\n [ { i } ] { r . text } \" ) print ( f \" Position: yaw= { r . yaw : .1f } \u00b0, pitch= { r . pitch : .1f } \u00b0\" ) print ( f \" Size: { r . width : .1f } \u00b0 x { r . height : .1f } \u00b0\" ) print ( f \" Confidence: { r . confidence : .2f } \" ) # Save results output_path = image_path . with_suffix ( \".ocr.json\" ) result . save_json ( str ( output_path )) print ( f \" \\n Results saved to: { output_path } \" ) print ( f \"View with: preview/index.html (drag and drop image + JSON)\" ) if __name__ == \"__main__\" : main () multi_engine.py # Compare results from different OCR engines on the same panorama. #!/usr/bin/env python3 \"\"\"Multi-engine comparison example for PanoOCR. This script compares OCR results from different engines on the same panorama. Usage: python multi_engine.py path/to/panorama.jpg Prerequisites: pip install \"panoocr[full]\" # Install all engines \"\"\" import sys from pathlib import Path from typing import Dict , Any def get_available_engines () -> Dict [ str , Any ]: \"\"\"Get all available OCR engines.\"\"\" engines = {} try : from panoocr.engines.macocr import MacOCREngine engines [ \"macocr\" ] = MacOCREngine () except ImportError : print ( \"MacOCR not available (requires macOS)\" ) try : from panoocr.engines.easyocr import EasyOCREngine engines [ \"easyocr\" ] = EasyOCREngine () except ImportError : print ( \"EasyOCR not available (pip install 'panoocr[easyocr]')\" ) try : from panoocr.engines.paddleocr import PaddleOCREngine engines [ \"paddleocr\" ] = PaddleOCREngine () except ImportError : print ( \"PaddleOCR not available (pip install 'panoocr[paddleocr]')\" ) try : from panoocr.engines.florence2 import Florence2OCREngine engines [ \"florence2\" ] = Florence2OCREngine () except ImportError : print ( \"Florence2 not available (pip install 'panoocr[florence2]')\" ) return engines def main (): if len ( sys . argv ) < 2 : print ( \"Usage: python multi_engine.py <panorama_image>\" ) print ( \" \\n Example:\" ) print ( \" python multi_engine.py panorama.jpg\" ) sys . exit ( 1 ) image_path = Path ( sys . argv [ 1 ]) if not image_path . exists (): print ( f \"Error: File not found: { image_path } \" ) sys . exit ( 1 ) from panoocr import PanoOCR , PerspectivePreset # Get available engines print ( \"Detecting available OCR engines...\" ) engines = get_available_engines () if not engines : print ( \" \\n Error: No OCR engines available.\" ) print ( \"Install engines with: pip install 'panoocr[full]'\" ) sys . exit ( 1 ) print ( f \" \\n Available engines: { ', ' . join ( engines . keys ()) } \" ) print ( \"=\" * 60 ) # Run OCR with each engine for name , engine in engines . items (): print ( f \" \\n [ { name . upper () } ]\" ) print ( \"-\" * 40 ) pano = PanoOCR ( engine , perspectives = PerspectivePreset . DEFAULT , ) result = pano . recognize ( str ( image_path ), show_progress = True ) print ( f \"Found { len ( result . results ) } text regions\" ) # Show first 5 results for i , r in enumerate ( result . results [: 5 ], 1 ): text_preview = r . text [: 40 ] + \"...\" if len ( r . text ) > 40 else r . text print ( f \" { i } . { text_preview } (conf: { r . confidence : .2f } )\" ) if len ( result . results ) > 5 : print ( f \" ... and { len ( result . results ) - 5 } more\" ) # Save results output_path = image_path . with_suffix ( f \". { name } .json\" ) result . save_json ( str ( output_path )) print ( f \"Saved: { output_path } \" ) print ( \" \\n \" + \"=\" * 60 ) print ( \"Comparison complete!\" ) print ( \" \\n View results with: preview/index.html\" ) print ( \"Drag and drop the panorama image + any JSON results file\" ) if __name__ == \"__main__\" : main ()","title":"Examples"},{"location":"examples/#examples","text":"Working scripts in examples/ .","title":"Examples"},{"location":"examples/#basic_usagepy","text":"#!/usr/bin/env python3 \"\"\"Basic usage example for PanoOCR. This script demonstrates the simplest way to run OCR on a panorama image. Usage: python basic_usage.py path/to/panorama.jpg Prerequisites: pip install \"panoocr[macocr]\" # For macOS # or pip install \"panoocr[easyocr]\" # For cross-platform \"\"\" import sys from pathlib import Path def main (): if len ( sys . argv ) < 2 : print ( \"Usage: python basic_usage.py <panorama_image>\" ) print ( \" \\n Example:\" ) print ( \" python basic_usage.py panorama.jpg\" ) sys . exit ( 1 ) image_path = Path ( sys . argv [ 1 ]) if not image_path . exists (): print ( f \"Error: File not found: { image_path } \" ) sys . exit ( 1 ) # Import panoocr from panoocr import PanoOCR , PerspectivePreset # Try to import an OCR engine (prefer MacOCR on macOS, fall back to EasyOCR) engine = None try : from panoocr.engines.macocr import MacOCREngine print ( \"Using MacOCR engine (Apple Vision Framework)\" ) engine = MacOCREngine () except ImportError : pass if engine is None : try : from panoocr.engines.easyocr import EasyOCREngine print ( \"Using EasyOCR engine\" ) engine = EasyOCREngine () except ImportError : pass if engine is None : print ( \"Error: No OCR engine available.\" ) print ( \" \\n Install an OCR engine with:\" ) print ( \" pip install 'panoocr[macocr]' # For macOS\" ) print ( \" pip install 'panoocr[easyocr]' # For cross-platform\" ) sys . exit ( 1 ) # Create the PanoOCR pipeline pano = PanoOCR ( engine , perspectives = PerspectivePreset . DEFAULT , ) # Run OCR print ( f \" \\n Processing: { image_path } \" ) result = pano . recognize ( str ( image_path )) # Print results print ( f \" \\n Found { len ( result . results ) } text regions:\" ) print ( \"-\" * 60 ) for i , r in enumerate ( result . results , 1 ): print ( f \" \\n [ { i } ] { r . text } \" ) print ( f \" Position: yaw= { r . yaw : .1f } \u00b0, pitch= { r . pitch : .1f } \u00b0\" ) print ( f \" Size: { r . width : .1f } \u00b0 x { r . height : .1f } \u00b0\" ) print ( f \" Confidence: { r . confidence : .2f } \" ) # Save results output_path = image_path . with_suffix ( \".ocr.json\" ) result . save_json ( str ( output_path )) print ( f \" \\n Results saved to: { output_path } \" ) print ( f \"View with: preview/index.html (drag and drop image + JSON)\" ) if __name__ == \"__main__\" : main ()","title":"basic_usage.py"},{"location":"examples/#multi_enginepy","text":"Compare results from different OCR engines on the same panorama. #!/usr/bin/env python3 \"\"\"Multi-engine comparison example for PanoOCR. This script compares OCR results from different engines on the same panorama. Usage: python multi_engine.py path/to/panorama.jpg Prerequisites: pip install \"panoocr[full]\" # Install all engines \"\"\" import sys from pathlib import Path from typing import Dict , Any def get_available_engines () -> Dict [ str , Any ]: \"\"\"Get all available OCR engines.\"\"\" engines = {} try : from panoocr.engines.macocr import MacOCREngine engines [ \"macocr\" ] = MacOCREngine () except ImportError : print ( \"MacOCR not available (requires macOS)\" ) try : from panoocr.engines.easyocr import EasyOCREngine engines [ \"easyocr\" ] = EasyOCREngine () except ImportError : print ( \"EasyOCR not available (pip install 'panoocr[easyocr]')\" ) try : from panoocr.engines.paddleocr import PaddleOCREngine engines [ \"paddleocr\" ] = PaddleOCREngine () except ImportError : print ( \"PaddleOCR not available (pip install 'panoocr[paddleocr]')\" ) try : from panoocr.engines.florence2 import Florence2OCREngine engines [ \"florence2\" ] = Florence2OCREngine () except ImportError : print ( \"Florence2 not available (pip install 'panoocr[florence2]')\" ) return engines def main (): if len ( sys . argv ) < 2 : print ( \"Usage: python multi_engine.py <panorama_image>\" ) print ( \" \\n Example:\" ) print ( \" python multi_engine.py panorama.jpg\" ) sys . exit ( 1 ) image_path = Path ( sys . argv [ 1 ]) if not image_path . exists (): print ( f \"Error: File not found: { image_path } \" ) sys . exit ( 1 ) from panoocr import PanoOCR , PerspectivePreset # Get available engines print ( \"Detecting available OCR engines...\" ) engines = get_available_engines () if not engines : print ( \" \\n Error: No OCR engines available.\" ) print ( \"Install engines with: pip install 'panoocr[full]'\" ) sys . exit ( 1 ) print ( f \" \\n Available engines: { ', ' . join ( engines . keys ()) } \" ) print ( \"=\" * 60 ) # Run OCR with each engine for name , engine in engines . items (): print ( f \" \\n [ { name . upper () } ]\" ) print ( \"-\" * 40 ) pano = PanoOCR ( engine , perspectives = PerspectivePreset . DEFAULT , ) result = pano . recognize ( str ( image_path ), show_progress = True ) print ( f \"Found { len ( result . results ) } text regions\" ) # Show first 5 results for i , r in enumerate ( result . results [: 5 ], 1 ): text_preview = r . text [: 40 ] + \"...\" if len ( r . text ) > 40 else r . text print ( f \" { i } . { text_preview } (conf: { r . confidence : .2f } )\" ) if len ( result . results ) > 5 : print ( f \" ... and { len ( result . results ) - 5 } more\" ) # Save results output_path = image_path . with_suffix ( f \". { name } .json\" ) result . save_json ( str ( output_path )) print ( f \"Saved: { output_path } \" ) print ( \" \\n \" + \"=\" * 60 ) print ( \"Comparison complete!\" ) print ( \" \\n View results with: preview/index.html\" ) print ( \"Drag and drop the panorama image + any JSON results file\" ) if __name__ == \"__main__\" : main ()","title":"multi_engine.py"},{"location":"api/","text":"API Reference # Client API # The PanoOCR class is the main entry point. PanoOCR # PanoOCR ( engine : OCREngine , perspectives : Optional [ Union [ PerspectivePreset , List [ PerspectiveMetadata ]]] = None , dedup_options : Optional [ DedupOptions ] = None ) Pipeline-first API for panorama OCR. This class provides a high-level interface for running OCR on equirectangular panorama images with automatic perspective projection and deduplication. Example from panoocr import PanoOCR from panoocr.engines.macocr import MacOCREngine engine = MacOCREngine() pano = PanoOCR(engine) result = pano.recognize(\"panorama.jpg\") result.save_json(\"results.json\") Attributes: Name Type Description engine The OCR engine to use for text recognition. perspectives List of perspective configurations. dedup_options Deduplication options. Initialize PanoOCR. Parameters: Name Type Description Default engine OCREngine OCR engine implementing the OCREngine protocol. required perspectives Optional [ Union [ PerspectivePreset , List [ PerspectiveMetadata ]]] Perspective configuration - either a preset name or custom list of PerspectiveMetadata. Defaults to DEFAULT. None dedup_options Optional [ DedupOptions ] Deduplication options. Uses defaults if not provided. None Source code in src/panoocr/api/client.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def __init__ ( self , engine : OCREngine , perspectives : Optional [ Union [ PerspectivePreset , List [ PerspectiveMetadata ]]] = None , dedup_options : Optional [ DedupOptions ] = None , ): \"\"\"Initialize PanoOCR. Args: engine: OCR engine implementing the OCREngine protocol. perspectives: Perspective configuration - either a preset name or custom list of PerspectiveMetadata. Defaults to DEFAULT. dedup_options: Deduplication options. Uses defaults if not provided. \"\"\" self . engine = engine # Set up perspectives if perspectives is None : self . perspectives = DEFAULT_IMAGE_PERSPECTIVES self . _preset_name = PerspectivePreset . DEFAULT . value elif isinstance ( perspectives , PerspectivePreset ): self . perspectives = _get_perspectives_for_preset ( perspectives ) self . _preset_name = perspectives . value else : self . perspectives = perspectives self . _preset_name = \"custom\" # Set up deduplication self . dedup_options = dedup_options or DedupOptions () self . _dedup_engine = SphereOCRDuplicationDetectionEngine ( min_text_similarity = self . dedup_options . min_text_similarity , min_intersection_ratio_for_similar_text = self . dedup_options . min_intersection_ratio_for_similar_text , min_text_overlap = self . dedup_options . min_text_overlap , min_intersection_ratio_for_overlapping_text = self . dedup_options . min_intersection_ratio_for_overlapping_text , min_intersection_ratio = self . dedup_options . min_intersection_ratio , ) recognize # recognize ( image : Union [ str , Image ], panorama_id : Optional [ str ] = None , show_progress : bool = True ) -> OCRResult Run OCR on a panorama image. Parameters: Name Type Description Default image Union [ str , Image ] Path to panorama image or PIL Image. required panorama_id Optional [ str ] Optional identifier for the panorama. None show_progress bool Whether to show a progress bar. True Returns: Type Description OCRResult OCRResult containing deduplicated sphere OCR results. Source code in src/panoocr/api/client.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def recognize ( self , image : Union [ str , Image . Image ], panorama_id : Optional [ str ] = None , show_progress : bool = True , ) -> OCRResult : \"\"\"Run OCR on a panorama image. Args: image: Path to panorama image or PIL Image. panorama_id: Optional identifier for the panorama. show_progress: Whether to show a progress bar. Returns: OCRResult containing deduplicated sphere OCR results. \"\"\" # Get image path for result metadata image_path = image if isinstance ( image , str ) else None if panorama_id is None : panorama_id = image_path or \"panorama\" # Load panorama pano = PanoramaImage ( panorama_id = panorama_id , image = image ) # Run OCR on each perspective all_sphere_results : List [ List [ SphereOCRResult ]] = [] perspective_iter = self . perspectives if show_progress : perspective_iter = tqdm ( self . perspectives , desc = \"Processing perspectives\" , unit = \"perspective\" , ) for perspective in perspective_iter : # Generate perspective view persp_image = pano . generate_perspective_image ( perspective ) # Run OCR flat_results = self . engine . recognize ( persp_image . get_perspective_image ()) # Convert to sphere coordinates sphere_results = [ result . to_sphere ( horizontal_fov = perspective . horizontal_fov , vertical_fov = perspective . vertical_fov , yaw_offset = perspective . yaw_offset , pitch_offset = perspective . pitch_offset , ) for result in flat_results ] all_sphere_results . append ( sphere_results ) # Deduplicate across adjacent perspectives deduplicated = self . _deduplicate_results ( all_sphere_results ) return OCRResult ( results = deduplicated , image_path = image_path , perspective_preset = self . _preset_name , ) recognize_multi # recognize_multi ( image : Union [ str , Image ], presets : Sequence [ PerspectivePreset ], panorama_id : Optional [ str ] = None , show_progress : bool = True ) -> OCRResult Run OCR on a panorama using multiple perspective presets. Useful for multi-scale detection to catch both small and large text. Parameters: Name Type Description Default image Union [ str , Image ] Path to panorama image or PIL Image. required presets Sequence [ PerspectivePreset ] List of perspective presets to use. required panorama_id Optional [ str ] Optional identifier for the panorama. None show_progress bool Whether to show a progress bar. True Returns: Type Description OCRResult OCRResult containing deduplicated sphere OCR results. Source code in src/panoocr/api/client.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def recognize_multi ( self , image : Union [ str , Image . Image ], presets : Sequence [ PerspectivePreset ], panorama_id : Optional [ str ] = None , show_progress : bool = True , ) -> OCRResult : \"\"\"Run OCR on a panorama using multiple perspective presets. Useful for multi-scale detection to catch both small and large text. Args: image: Path to panorama image or PIL Image. presets: List of perspective presets to use. panorama_id: Optional identifier for the panorama. show_progress: Whether to show a progress bar. Returns: OCRResult containing deduplicated sphere OCR results. \"\"\" # Get image path for result metadata image_path = image if isinstance ( image , str ) else None if panorama_id is None : panorama_id = image_path or \"panorama\" # Combine perspectives from all presets combined_perspectives = combine_perspectives ( * [ _get_perspectives_for_preset ( preset ) for preset in presets ] ) # Temporarily swap perspectives original_perspectives = self . perspectives original_preset_name = self . _preset_name self . perspectives = combined_perspectives self . _preset_name = None try : result = self . recognize ( image = image , panorama_id = panorama_id , show_progress = show_progress , ) # Update result with multi-preset info return OCRResult ( results = result . results , image_path = image_path , perspective_preset = None , perspective_presets = [ preset . value for preset in presets ], ) finally : # Restore original perspectives self . perspectives = original_perspectives self . _preset_name = original_preset_name OCRResult dataclass # OCRResult ( results : Sequence [ SphereOCRResult ], image_path : Optional [ str ] = None , perspective_preset : Optional [ str ] = None , perspective_presets : Optional [ Sequence [ str ]] = None ) OCR output plus metadata, with preview-tool-friendly JSON export. Attributes: Name Type Description results Sequence [ SphereOCRResult ] List of deduplicated sphere OCR results. image_path Optional [ str ] Optional path to the source image. perspective_preset Optional [ str ] Name of the perspective preset used. perspective_presets Optional [ Sequence [ str ]] List of perspective preset names if multiple were used. to_dict # to_dict () -> dict Convert to a dictionary for JSON serialization. Source code in src/panoocr/api/models.py 93 94 95 96 97 98 99 100 101 102 def to_dict ( self ) -> dict : \"\"\"Convert to a dictionary for JSON serialization.\"\"\" return { \"image_path\" : self . image_path , \"perspective_preset\" : self . perspective_preset , \"perspective_presets\" : list ( self . perspective_presets ) if self . perspective_presets is not None else None , \"results\" : [ r . to_dict () for r in self . results ], } save_json # save_json ( path : str ) -> None Save OCR results in a JSON file suitable for the preview tool. Parameters: Name Type Description Default path str Output file path. required Source code in src/panoocr/api/models.py 104 105 106 107 108 109 110 111 def save_json ( self , path : str ) -> None : \"\"\"Save OCR results in a JSON file suitable for the preview tool. Args: path: Output file path. \"\"\" with open ( path , \"w\" ) as f : json . dump ( self . to_dict (), f , indent = 2 ) from_dict classmethod # from_dict ( data : dict ) -> 'OCRResult' Create an OCRResult from a dictionary. Parameters: Name Type Description Default data dict Dictionary with OCR result data. required Returns: Type Description 'OCRResult' OCRResult instance. Source code in src/panoocr/api/models.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 @classmethod def from_dict ( cls , data : dict ) -> \"OCRResult\" : \"\"\"Create an OCRResult from a dictionary. Args: data: Dictionary with OCR result data. Returns: OCRResult instance. \"\"\" results = [ SphereOCRResult . from_dict ( r ) for r in data . get ( \"results\" , [])] return cls ( results = results , image_path = data . get ( \"image_path\" ), perspective_preset = data . get ( \"perspective_preset\" ), perspective_presets = data . get ( \"perspective_presets\" ), ) load_json classmethod # load_json ( path : str ) -> 'OCRResult' Load OCR results from a JSON file. Parameters: Name Type Description Default path str Input file path. required Returns: Type Description 'OCRResult' OCRResult instance. Source code in src/panoocr/api/models.py 131 132 133 134 135 136 137 138 139 140 141 142 143 @classmethod def load_json ( cls , path : str ) -> \"OCRResult\" : \"\"\"Load OCR results from a JSON file. Args: path: Input file path. Returns: OCRResult instance. \"\"\" with open ( path , \"r\" ) as f : data = json . load ( f ) return cls . from_dict ( data ) PerspectivePreset # Bases: str , Enum Pre-defined perspective configurations for common text scales. OCROptions dataclass # OCROptions ( config : dict | None = None ) Options passed to the underlying OCR engine. Attributes: Name Type Description config dict | None Engine-specific configuration dictionary. DedupOptions dataclass # DedupOptions ( min_text_similarity : float = 0.5 , min_intersection_ratio_for_similar_text : float = 0.5 , min_text_overlap : float = 0.5 , min_intersection_ratio_for_overlapping_text : float = 0.15 , min_intersection_ratio : float = 0.1 ) Deduplication options applied after multi-view OCR. Attributes: Name Type Description min_text_similarity float Minimum Levenshtein similarity for text comparison. min_intersection_ratio_for_similar_text float Minimum region overlap for similar texts. min_text_overlap float Minimum overlap similarity for text comparison. min_intersection_ratio_for_overlapping_text float Minimum region overlap for overlapping texts. min_intersection_ratio float Minimum region intersection ratio threshold. Module Structure # panoocr/ \u251c\u2500\u2500 api/ # Client API \u2502 \u251c\u2500\u2500 client.py # PanoOCR \u2502 \u2514\u2500\u2500 models.py # OCRResult, options, OCREngine protocol \u251c\u2500\u2500 engines/ # OCR engines (lazily imported) \u2502 \u251c\u2500\u2500 macocr.py # MacOCREngine (requires [macocr]) \u2502 \u251c\u2500\u2500 easyocr.py # EasyOCREngine (requires [easyocr]) \u2502 \u251c\u2500\u2500 paddleocr.py # PaddleOCREngine (requires [paddleocr]) \u2502 \u251c\u2500\u2500 florence2.py # Florence2OCREngine (requires [florence2]) \u2502 \u2514\u2500\u2500 trocr.py # TrOCREngine (requires [trocr]) \u251c\u2500\u2500 ocr/ # OCR result models \u2502 \u251c\u2500\u2500 models.py # FlatOCRResult, SphereOCRResult \u2502 \u2514\u2500\u2500 utils.py # Visualization (requires [viz]) \u251c\u2500\u2500 dedup/ # Deduplication \u2502 \u2514\u2500\u2500 detection.py # SphereOCRDuplicationDetectionEngine \u251c\u2500\u2500 image/ # Panorama handling \u2502 \u251c\u2500\u2500 models.py # PanoramaImage, PerspectiveMetadata \u2502 \u2514\u2500\u2500 perspectives.py # Presets, generate_perspectives() \u2514\u2500\u2500 geometry.py # Coordinate conversion utilities Submodules # Engines - OCREngine protocol and built-in engines Image - Panorama and perspective classes OCR Models - OCR result types Deduplication - Text deduplication Geometry - Coordinate conversion Visualization - OCR visualization (requires [viz] )","title":"Overview"},{"location":"api/#api-reference","text":"","title":"API Reference"},{"location":"api/#client-api","text":"The PanoOCR class is the main entry point.","title":"Client API"},{"location":"api/#panoocr.api.client.PanoOCR","text":"PanoOCR ( engine : OCREngine , perspectives : Optional [ Union [ PerspectivePreset , List [ PerspectiveMetadata ]]] = None , dedup_options : Optional [ DedupOptions ] = None ) Pipeline-first API for panorama OCR. This class provides a high-level interface for running OCR on equirectangular panorama images with automatic perspective projection and deduplication. Example from panoocr import PanoOCR from panoocr.engines.macocr import MacOCREngine engine = MacOCREngine() pano = PanoOCR(engine) result = pano.recognize(\"panorama.jpg\") result.save_json(\"results.json\") Attributes: Name Type Description engine The OCR engine to use for text recognition. perspectives List of perspective configurations. dedup_options Deduplication options. Initialize PanoOCR. Parameters: Name Type Description Default engine OCREngine OCR engine implementing the OCREngine protocol. required perspectives Optional [ Union [ PerspectivePreset , List [ PerspectiveMetadata ]]] Perspective configuration - either a preset name or custom list of PerspectiveMetadata. Defaults to DEFAULT. None dedup_options Optional [ DedupOptions ] Deduplication options. Uses defaults if not provided. None Source code in src/panoocr/api/client.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def __init__ ( self , engine : OCREngine , perspectives : Optional [ Union [ PerspectivePreset , List [ PerspectiveMetadata ]]] = None , dedup_options : Optional [ DedupOptions ] = None , ): \"\"\"Initialize PanoOCR. Args: engine: OCR engine implementing the OCREngine protocol. perspectives: Perspective configuration - either a preset name or custom list of PerspectiveMetadata. Defaults to DEFAULT. dedup_options: Deduplication options. Uses defaults if not provided. \"\"\" self . engine = engine # Set up perspectives if perspectives is None : self . perspectives = DEFAULT_IMAGE_PERSPECTIVES self . _preset_name = PerspectivePreset . DEFAULT . value elif isinstance ( perspectives , PerspectivePreset ): self . perspectives = _get_perspectives_for_preset ( perspectives ) self . _preset_name = perspectives . value else : self . perspectives = perspectives self . _preset_name = \"custom\" # Set up deduplication self . dedup_options = dedup_options or DedupOptions () self . _dedup_engine = SphereOCRDuplicationDetectionEngine ( min_text_similarity = self . dedup_options . min_text_similarity , min_intersection_ratio_for_similar_text = self . dedup_options . min_intersection_ratio_for_similar_text , min_text_overlap = self . dedup_options . min_text_overlap , min_intersection_ratio_for_overlapping_text = self . dedup_options . min_intersection_ratio_for_overlapping_text , min_intersection_ratio = self . dedup_options . min_intersection_ratio , )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;PanoOCR"},{"location":"api/#panoocr.api.client.PanoOCR.recognize","text":"recognize ( image : Union [ str , Image ], panorama_id : Optional [ str ] = None , show_progress : bool = True ) -> OCRResult Run OCR on a panorama image. Parameters: Name Type Description Default image Union [ str , Image ] Path to panorama image or PIL Image. required panorama_id Optional [ str ] Optional identifier for the panorama. None show_progress bool Whether to show a progress bar. True Returns: Type Description OCRResult OCRResult containing deduplicated sphere OCR results. Source code in src/panoocr/api/client.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def recognize ( self , image : Union [ str , Image . Image ], panorama_id : Optional [ str ] = None , show_progress : bool = True , ) -> OCRResult : \"\"\"Run OCR on a panorama image. Args: image: Path to panorama image or PIL Image. panorama_id: Optional identifier for the panorama. show_progress: Whether to show a progress bar. Returns: OCRResult containing deduplicated sphere OCR results. \"\"\" # Get image path for result metadata image_path = image if isinstance ( image , str ) else None if panorama_id is None : panorama_id = image_path or \"panorama\" # Load panorama pano = PanoramaImage ( panorama_id = panorama_id , image = image ) # Run OCR on each perspective all_sphere_results : List [ List [ SphereOCRResult ]] = [] perspective_iter = self . perspectives if show_progress : perspective_iter = tqdm ( self . perspectives , desc = \"Processing perspectives\" , unit = \"perspective\" , ) for perspective in perspective_iter : # Generate perspective view persp_image = pano . generate_perspective_image ( perspective ) # Run OCR flat_results = self . engine . recognize ( persp_image . get_perspective_image ()) # Convert to sphere coordinates sphere_results = [ result . to_sphere ( horizontal_fov = perspective . horizontal_fov , vertical_fov = perspective . vertical_fov , yaw_offset = perspective . yaw_offset , pitch_offset = perspective . pitch_offset , ) for result in flat_results ] all_sphere_results . append ( sphere_results ) # Deduplicate across adjacent perspectives deduplicated = self . _deduplicate_results ( all_sphere_results ) return OCRResult ( results = deduplicated , image_path = image_path , perspective_preset = self . _preset_name , )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;recognize"},{"location":"api/#panoocr.api.client.PanoOCR.recognize_multi","text":"recognize_multi ( image : Union [ str , Image ], presets : Sequence [ PerspectivePreset ], panorama_id : Optional [ str ] = None , show_progress : bool = True ) -> OCRResult Run OCR on a panorama using multiple perspective presets. Useful for multi-scale detection to catch both small and large text. Parameters: Name Type Description Default image Union [ str , Image ] Path to panorama image or PIL Image. required presets Sequence [ PerspectivePreset ] List of perspective presets to use. required panorama_id Optional [ str ] Optional identifier for the panorama. None show_progress bool Whether to show a progress bar. True Returns: Type Description OCRResult OCRResult containing deduplicated sphere OCR results. Source code in src/panoocr/api/client.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def recognize_multi ( self , image : Union [ str , Image . Image ], presets : Sequence [ PerspectivePreset ], panorama_id : Optional [ str ] = None , show_progress : bool = True , ) -> OCRResult : \"\"\"Run OCR on a panorama using multiple perspective presets. Useful for multi-scale detection to catch both small and large text. Args: image: Path to panorama image or PIL Image. presets: List of perspective presets to use. panorama_id: Optional identifier for the panorama. show_progress: Whether to show a progress bar. Returns: OCRResult containing deduplicated sphere OCR results. \"\"\" # Get image path for result metadata image_path = image if isinstance ( image , str ) else None if panorama_id is None : panorama_id = image_path or \"panorama\" # Combine perspectives from all presets combined_perspectives = combine_perspectives ( * [ _get_perspectives_for_preset ( preset ) for preset in presets ] ) # Temporarily swap perspectives original_perspectives = self . perspectives original_preset_name = self . _preset_name self . perspectives = combined_perspectives self . _preset_name = None try : result = self . recognize ( image = image , panorama_id = panorama_id , show_progress = show_progress , ) # Update result with multi-preset info return OCRResult ( results = result . results , image_path = image_path , perspective_preset = None , perspective_presets = [ preset . value for preset in presets ], ) finally : # Restore original perspectives self . perspectives = original_perspectives self . _preset_name = original_preset_name","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;recognize_multi"},{"location":"api/#panoocr.api.models.OCRResult","text":"OCRResult ( results : Sequence [ SphereOCRResult ], image_path : Optional [ str ] = None , perspective_preset : Optional [ str ] = None , perspective_presets : Optional [ Sequence [ str ]] = None ) OCR output plus metadata, with preview-tool-friendly JSON export. Attributes: Name Type Description results Sequence [ SphereOCRResult ] List of deduplicated sphere OCR results. image_path Optional [ str ] Optional path to the source image. perspective_preset Optional [ str ] Name of the perspective preset used. perspective_presets Optional [ Sequence [ str ]] List of perspective preset names if multiple were used.","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;OCRResult"},{"location":"api/#panoocr.api.models.OCRResult.to_dict","text":"to_dict () -> dict Convert to a dictionary for JSON serialization. Source code in src/panoocr/api/models.py 93 94 95 96 97 98 99 100 101 102 def to_dict ( self ) -> dict : \"\"\"Convert to a dictionary for JSON serialization.\"\"\" return { \"image_path\" : self . image_path , \"perspective_preset\" : self . perspective_preset , \"perspective_presets\" : list ( self . perspective_presets ) if self . perspective_presets is not None else None , \"results\" : [ r . to_dict () for r in self . results ], }","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;to_dict"},{"location":"api/#panoocr.api.models.OCRResult.save_json","text":"save_json ( path : str ) -> None Save OCR results in a JSON file suitable for the preview tool. Parameters: Name Type Description Default path str Output file path. required Source code in src/panoocr/api/models.py 104 105 106 107 108 109 110 111 def save_json ( self , path : str ) -> None : \"\"\"Save OCR results in a JSON file suitable for the preview tool. Args: path: Output file path. \"\"\" with open ( path , \"w\" ) as f : json . dump ( self . to_dict (), f , indent = 2 )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;save_json"},{"location":"api/#panoocr.api.models.OCRResult.from_dict","text":"from_dict ( data : dict ) -> 'OCRResult' Create an OCRResult from a dictionary. Parameters: Name Type Description Default data dict Dictionary with OCR result data. required Returns: Type Description 'OCRResult' OCRResult instance. Source code in src/panoocr/api/models.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 @classmethod def from_dict ( cls , data : dict ) -> \"OCRResult\" : \"\"\"Create an OCRResult from a dictionary. Args: data: Dictionary with OCR result data. Returns: OCRResult instance. \"\"\" results = [ SphereOCRResult . from_dict ( r ) for r in data . get ( \"results\" , [])] return cls ( results = results , image_path = data . get ( \"image_path\" ), perspective_preset = data . get ( \"perspective_preset\" ), perspective_presets = data . get ( \"perspective_presets\" ), )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;from_dict"},{"location":"api/#panoocr.api.models.OCRResult.load_json","text":"load_json ( path : str ) -> 'OCRResult' Load OCR results from a JSON file. Parameters: Name Type Description Default path str Input file path. required Returns: Type Description 'OCRResult' OCRResult instance. Source code in src/panoocr/api/models.py 131 132 133 134 135 136 137 138 139 140 141 142 143 @classmethod def load_json ( cls , path : str ) -> \"OCRResult\" : \"\"\"Load OCR results from a JSON file. Args: path: Input file path. Returns: OCRResult instance. \"\"\" with open ( path , \"r\" ) as f : data = json . load ( f ) return cls . from_dict ( data )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;load_json"},{"location":"api/#panoocr.api.models.PerspectivePreset","text":"Bases: str , Enum Pre-defined perspective configurations for common text scales.","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;PerspectivePreset"},{"location":"api/#panoocr.api.models.OCROptions","text":"OCROptions ( config : dict | None = None ) Options passed to the underlying OCR engine. Attributes: Name Type Description config dict | None Engine-specific configuration dictionary.","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;OCROptions"},{"location":"api/#panoocr.api.models.DedupOptions","text":"DedupOptions ( min_text_similarity : float = 0.5 , min_intersection_ratio_for_similar_text : float = 0.5 , min_text_overlap : float = 0.5 , min_intersection_ratio_for_overlapping_text : float = 0.15 , min_intersection_ratio : float = 0.1 ) Deduplication options applied after multi-view OCR. Attributes: Name Type Description min_text_similarity float Minimum Levenshtein similarity for text comparison. min_intersection_ratio_for_similar_text float Minimum region overlap for similar texts. min_text_overlap float Minimum overlap similarity for text comparison. min_intersection_ratio_for_overlapping_text float Minimum region overlap for overlapping texts. min_intersection_ratio float Minimum region intersection ratio threshold.","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;DedupOptions"},{"location":"api/#module-structure","text":"panoocr/ \u251c\u2500\u2500 api/ # Client API \u2502 \u251c\u2500\u2500 client.py # PanoOCR \u2502 \u2514\u2500\u2500 models.py # OCRResult, options, OCREngine protocol \u251c\u2500\u2500 engines/ # OCR engines (lazily imported) \u2502 \u251c\u2500\u2500 macocr.py # MacOCREngine (requires [macocr]) \u2502 \u251c\u2500\u2500 easyocr.py # EasyOCREngine (requires [easyocr]) \u2502 \u251c\u2500\u2500 paddleocr.py # PaddleOCREngine (requires [paddleocr]) \u2502 \u251c\u2500\u2500 florence2.py # Florence2OCREngine (requires [florence2]) \u2502 \u2514\u2500\u2500 trocr.py # TrOCREngine (requires [trocr]) \u251c\u2500\u2500 ocr/ # OCR result models \u2502 \u251c\u2500\u2500 models.py # FlatOCRResult, SphereOCRResult \u2502 \u2514\u2500\u2500 utils.py # Visualization (requires [viz]) \u251c\u2500\u2500 dedup/ # Deduplication \u2502 \u2514\u2500\u2500 detection.py # SphereOCRDuplicationDetectionEngine \u251c\u2500\u2500 image/ # Panorama handling \u2502 \u251c\u2500\u2500 models.py # PanoramaImage, PerspectiveMetadata \u2502 \u2514\u2500\u2500 perspectives.py # Presets, generate_perspectives() \u2514\u2500\u2500 geometry.py # Coordinate conversion utilities","title":"Module Structure"},{"location":"api/#submodules","text":"Engines - OCREngine protocol and built-in engines Image - Panorama and perspective classes OCR Models - OCR result types Deduplication - Text deduplication Geometry - Coordinate conversion Visualization - OCR visualization (requires [viz] )","title":"Submodules"},{"location":"api/dedup/","text":"Deduplication # Removes duplicate text detections when the same text appears in multiple perspective views. Algorithm # OCR results are deduplicated using spatial overlap and text similarity: For each result pair, check both text similarity and region overlap If texts are similar (Levenshtein) or overlapping, and regions intersect sufficiently, mark as duplicate Keep the result with longer text, or higher confidence if equal length Adaptive Processing Strategy # PanoOCR automatically selects the optimal deduplication strategy based on perspective arrangement: Sequential pairwise (fast) : Used when perspectives form a simple horizontal ring (same pitch, sorted by yaw). Compares only adjacent perspective pairs plus wrap-around. Incremental master list (thorough) : Used for arbitrary perspective arrangements (multiple pitch levels, custom arrangements). Compares each result against all previous results. This is handled automatically - no configuration needed. SphereOCRDuplicationDetectionEngine # SphereOCRDuplicationDetectionEngine # SphereOCRDuplicationDetectionEngine ( min_text_similarity : float = DEFAULT_MIN_TEXT_SIMILARITY , min_intersection_ratio_for_similar_text : float = DEFAULT_MIN_INTERSECTION_RATIO_FOR_SIMILAR_TEXT , min_text_overlap : float = DEFAULT_MIN_TEXT_OVERLAP , min_intersection_ratio_for_overlapping_text : float = DEFAULT_MIN_INTERSECTION_RATIO_FOR_OVERLAPPING_TEXT , min_intersection_ratio : float = DEFAULT_MIN_INTERSECTION_RATIO ) Engine for detecting and removing duplicate OCR results. Duplicates are identified by comparing both spatial overlap and text similarity between results from overlapping perspective views. Attributes: Name Type Description min_text_similarity Minimum Levenshtein similarity threshold. min_intersection_ratio_for_similar_text Required overlap for similar texts. min_text_overlap Minimum overlap similarity threshold. min_intersection_ratio_for_overlapping_text Required overlap for overlapping texts. min_intersection_ratio Minimum intersection ratio to consider. Initialize the deduplication engine. Parameters: Name Type Description Default min_text_similarity float Minimum Levenshtein normalized similarity (0-1). DEFAULT_MIN_TEXT_SIMILARITY min_intersection_ratio_for_similar_text float Minimum region overlap for text matches based on similarity. DEFAULT_MIN_INTERSECTION_RATIO_FOR_SIMILAR_TEXT min_text_overlap float Minimum overlap normalized similarity (0-1). DEFAULT_MIN_TEXT_OVERLAP min_intersection_ratio_for_overlapping_text float Minimum region overlap for text matches based on overlap. DEFAULT_MIN_INTERSECTION_RATIO_FOR_OVERLAPPING_TEXT min_intersection_ratio float Absolute minimum intersection ratio to consider any match. DEFAULT_MIN_INTERSECTION_RATIO Source code in src/panoocr/dedup/detection.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , min_text_similarity : float = DEFAULT_MIN_TEXT_SIMILARITY , min_intersection_ratio_for_similar_text : float = DEFAULT_MIN_INTERSECTION_RATIO_FOR_SIMILAR_TEXT , min_text_overlap : float = DEFAULT_MIN_TEXT_OVERLAP , min_intersection_ratio_for_overlapping_text : float = DEFAULT_MIN_INTERSECTION_RATIO_FOR_OVERLAPPING_TEXT , min_intersection_ratio : float = DEFAULT_MIN_INTERSECTION_RATIO , ): \"\"\"Initialize the deduplication engine. Args: min_text_similarity: Minimum Levenshtein normalized similarity (0-1). min_intersection_ratio_for_similar_text: Minimum region overlap for text matches based on similarity. min_text_overlap: Minimum overlap normalized similarity (0-1). min_intersection_ratio_for_overlapping_text: Minimum region overlap for text matches based on overlap. min_intersection_ratio: Absolute minimum intersection ratio to consider any match. \"\"\" self . min_text_similarity = min_text_similarity self . min_intersection_ratio_for_similar_text = ( min_intersection_ratio_for_similar_text ) self . min_text_overlap = min_text_overlap self . min_intersection_ratio_for_overlapping_text = ( min_intersection_ratio_for_overlapping_text ) self . min_intersection_ratio = min_intersection_ratio check_duplication # check_duplication ( ocr_result_1 : SphereOCRResult , ocr_result_2 : SphereOCRResult ) -> bool Check if two OCR results are duplicates. Parameters: Name Type Description Default ocr_result_1 SphereOCRResult First OCR result. required ocr_result_2 SphereOCRResult Second OCR result. required Returns: Type Description bool True if the results are considered duplicates. Source code in src/panoocr/dedup/detection.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def check_duplication ( self , ocr_result_1 : SphereOCRResult , ocr_result_2 : SphereOCRResult ) -> bool : \"\"\"Check if two OCR results are duplicates. Args: ocr_result_1: First OCR result. ocr_result_2: Second OCR result. Returns: True if the results are considered duplicates. \"\"\" text_similarity = self . _get_texts_similarity ( ocr_result_1 . text , ocr_result_2 . text ) text_overlap = self . _get_texts_overlap ( ocr_result_1 . text , ocr_result_2 . text ) # If texts are neither similar nor overlapping, not duplicates if ( text_similarity < self . min_text_similarity ) and ( text_overlap < self . min_text_overlap ): return False # Check spatial intersection intersection = self . _intersect_ocr_results ( ocr_result_1 , ocr_result_2 ) if intersection is None : return False if intersection . intersection_ratio < self . min_intersection_ratio : return False # Check if texts overlap and regions overlap sufficiently if ( text_overlap >= self . min_text_overlap and intersection . intersection_ratio >= self . min_intersection_ratio_for_overlapping_text ): return True # Check if texts are similar and regions overlap sufficiently if ( text_similarity >= self . min_text_similarity and intersection . intersection_ratio >= self . min_intersection_ratio_for_similar_text ): return True return False remove_duplication_for_two_lists # remove_duplication_for_two_lists ( ocr_results_0 : List [ SphereOCRResult ], ocr_results_1 : List [ SphereOCRResult ]) -> Tuple [ List [ SphereOCRResult ], List [ SphereOCRResult ]] Remove duplicates between two lists of OCR results. When duplicates are found, keeps the result with longer text, or higher confidence if texts are equal length. Parameters: Name Type Description Default ocr_results_0 List [ SphereOCRResult ] First list of OCR results (modified in place). required ocr_results_1 List [ SphereOCRResult ] Second list of OCR results (modified in place). required Returns: Type Description Tuple [ List [ SphereOCRResult ], List [ SphereOCRResult ]] Tuple of the two lists with duplicates removed. Source code in src/panoocr/dedup/detection.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def remove_duplication_for_two_lists ( self , ocr_results_0 : List [ SphereOCRResult ], ocr_results_1 : List [ SphereOCRResult ], ) -> Tuple [ List [ SphereOCRResult ], List [ SphereOCRResult ]]: \"\"\"Remove duplicates between two lists of OCR results. When duplicates are found, keeps the result with longer text, or higher confidence if texts are equal length. Args: ocr_results_0: First list of OCR results (modified in place). ocr_results_1: Second list of OCR results (modified in place). Returns: Tuple of the two lists with duplicates removed. \"\"\" # Find all duplicate pairs duplications = [] for i , ocr_result_0 in enumerate ( ocr_results_0 ): for j , ocr_result_1 in enumerate ( ocr_results_1 ): if self . check_duplication ( ocr_result_0 , ocr_result_1 ): duplications . append (( i , j )) # Determine which to remove from each list indices_to_remove_from_0 : List [ int ] = [] indices_to_remove_from_1 : List [ int ] = [] for i , j in duplications : candidate_0 = ocr_results_0 [ i ] candidate_1 = ocr_results_1 [ j ] if len ( candidate_0 . text ) == len ( candidate_1 . text ): # Equal length: prefer higher confidence if candidate_0 . confidence < candidate_1 . confidence : indices_to_remove_from_0 . append ( i ) else : indices_to_remove_from_1 . append ( j ) elif len ( candidate_0 . text ) > len ( candidate_1 . text ): # Prefer longer text indices_to_remove_from_1 . append ( j ) else : indices_to_remove_from_0 . append ( i ) # Remove duplicates (in reverse order to preserve indices) indices_to_remove_from_0 = sorted ( set ( indices_to_remove_from_0 ), reverse = True ) indices_to_remove_from_1 = sorted ( set ( indices_to_remove_from_1 ), reverse = True ) for index in indices_to_remove_from_0 : ocr_results_0 . pop ( index ) for index in indices_to_remove_from_1 : ocr_results_1 . pop ( index ) return ocr_results_0 , ocr_results_1 deduplicate_frames # deduplicate_frames ( frames : List [ List [ SphereOCRResult ]]) -> List [ SphereOCRResult ] Deduplicate OCR results across multiple frames using incremental merging. Processes frames one by one, maintaining a master list. Each result from a new frame is compared against the entire master list. When duplicates are found, keeps the result with longer text or higher confidence. This approach is slower than pairwise deduplication but handles arbitrary perspective arrangements correctly (e.g., multiple pitch levels, custom arrangements). Parameters: Name Type Description Default frames List [ List [ SphereOCRResult ]] List of frames, each containing a list of OCR results. required Returns: Type Description List [ SphereOCRResult ] Deduplicated list of sphere OCR results. Source code in src/panoocr/dedup/detection.py 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def deduplicate_frames ( self , frames : List [ List [ SphereOCRResult ]], ) -> List [ SphereOCRResult ]: \"\"\"Deduplicate OCR results across multiple frames using incremental merging. Processes frames one by one, maintaining a master list. Each result from a new frame is compared against the entire master list. When duplicates are found, keeps the result with longer text or higher confidence. This approach is slower than pairwise deduplication but handles arbitrary perspective arrangements correctly (e.g., multiple pitch levels, custom arrangements). Args: frames: List of frames, each containing a list of OCR results. Returns: Deduplicated list of sphere OCR results. \"\"\" if not frames : return [] # Start with the first frame as the master list master_list : List [ SphereOCRResult ] = list ( frames [ 0 ]) # Process each subsequent frame for frame_results in frames [ 1 :]: for new_result in frame_results : # Check for overlaps with existing master list duplicate_indices = [] for i , master_result in enumerate ( master_list ): if self . check_duplication ( new_result , master_result ): duplicate_indices . append ( i ) if not duplicate_indices : # No duplicates found, add to master list master_list . append ( new_result ) else : # Found duplicates - select best result among all candidates candidates = [ new_result ] + [ master_list [ i ] for i in duplicate_indices ] # Find the best result best_result = candidates [ 0 ] for candidate in candidates [ 1 :]: best_result = self . _select_best_result ( best_result , candidate ) # Remove all duplicates from master list (in reverse order) for i in sorted ( duplicate_indices , reverse = True ): master_list . pop ( i ) # Add the best result master_list . append ( best_result ) return master_list Data Classes # RegionIntersection dataclass # RegionIntersection ( region_1_area : float , region_2_area : float , intersection_area : float , intersection_ratio : float ) Result of intersecting two regions. Attributes: Name Type Description region_1_area float Area of the first region. region_2_area float Area of the second region. intersection_area float Area of the intersection. intersection_ratio float Intersection area divided by minimum region area.","title":"Deduplication"},{"location":"api/dedup/#deduplication","text":"Removes duplicate text detections when the same text appears in multiple perspective views.","title":"Deduplication"},{"location":"api/dedup/#algorithm","text":"OCR results are deduplicated using spatial overlap and text similarity: For each result pair, check both text similarity and region overlap If texts are similar (Levenshtein) or overlapping, and regions intersect sufficiently, mark as duplicate Keep the result with longer text, or higher confidence if equal length","title":"Algorithm"},{"location":"api/dedup/#adaptive-processing-strategy","text":"PanoOCR automatically selects the optimal deduplication strategy based on perspective arrangement: Sequential pairwise (fast) : Used when perspectives form a simple horizontal ring (same pitch, sorted by yaw). Compares only adjacent perspective pairs plus wrap-around. Incremental master list (thorough) : Used for arbitrary perspective arrangements (multiple pitch levels, custom arrangements). Compares each result against all previous results. This is handled automatically - no configuration needed.","title":"Adaptive Processing Strategy"},{"location":"api/dedup/#sphereocrduplicationdetectionengine","text":"","title":"SphereOCRDuplicationDetectionEngine"},{"location":"api/dedup/#panoocr.dedup.detection.SphereOCRDuplicationDetectionEngine","text":"SphereOCRDuplicationDetectionEngine ( min_text_similarity : float = DEFAULT_MIN_TEXT_SIMILARITY , min_intersection_ratio_for_similar_text : float = DEFAULT_MIN_INTERSECTION_RATIO_FOR_SIMILAR_TEXT , min_text_overlap : float = DEFAULT_MIN_TEXT_OVERLAP , min_intersection_ratio_for_overlapping_text : float = DEFAULT_MIN_INTERSECTION_RATIO_FOR_OVERLAPPING_TEXT , min_intersection_ratio : float = DEFAULT_MIN_INTERSECTION_RATIO ) Engine for detecting and removing duplicate OCR results. Duplicates are identified by comparing both spatial overlap and text similarity between results from overlapping perspective views. Attributes: Name Type Description min_text_similarity Minimum Levenshtein similarity threshold. min_intersection_ratio_for_similar_text Required overlap for similar texts. min_text_overlap Minimum overlap similarity threshold. min_intersection_ratio_for_overlapping_text Required overlap for overlapping texts. min_intersection_ratio Minimum intersection ratio to consider. Initialize the deduplication engine. Parameters: Name Type Description Default min_text_similarity float Minimum Levenshtein normalized similarity (0-1). DEFAULT_MIN_TEXT_SIMILARITY min_intersection_ratio_for_similar_text float Minimum region overlap for text matches based on similarity. DEFAULT_MIN_INTERSECTION_RATIO_FOR_SIMILAR_TEXT min_text_overlap float Minimum overlap normalized similarity (0-1). DEFAULT_MIN_TEXT_OVERLAP min_intersection_ratio_for_overlapping_text float Minimum region overlap for text matches based on overlap. DEFAULT_MIN_INTERSECTION_RATIO_FOR_OVERLAPPING_TEXT min_intersection_ratio float Absolute minimum intersection ratio to consider any match. DEFAULT_MIN_INTERSECTION_RATIO Source code in src/panoocr/dedup/detection.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , min_text_similarity : float = DEFAULT_MIN_TEXT_SIMILARITY , min_intersection_ratio_for_similar_text : float = DEFAULT_MIN_INTERSECTION_RATIO_FOR_SIMILAR_TEXT , min_text_overlap : float = DEFAULT_MIN_TEXT_OVERLAP , min_intersection_ratio_for_overlapping_text : float = DEFAULT_MIN_INTERSECTION_RATIO_FOR_OVERLAPPING_TEXT , min_intersection_ratio : float = DEFAULT_MIN_INTERSECTION_RATIO , ): \"\"\"Initialize the deduplication engine. Args: min_text_similarity: Minimum Levenshtein normalized similarity (0-1). min_intersection_ratio_for_similar_text: Minimum region overlap for text matches based on similarity. min_text_overlap: Minimum overlap normalized similarity (0-1). min_intersection_ratio_for_overlapping_text: Minimum region overlap for text matches based on overlap. min_intersection_ratio: Absolute minimum intersection ratio to consider any match. \"\"\" self . min_text_similarity = min_text_similarity self . min_intersection_ratio_for_similar_text = ( min_intersection_ratio_for_similar_text ) self . min_text_overlap = min_text_overlap self . min_intersection_ratio_for_overlapping_text = ( min_intersection_ratio_for_overlapping_text ) self . min_intersection_ratio = min_intersection_ratio","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;SphereOCRDuplicationDetectionEngine"},{"location":"api/dedup/#panoocr.dedup.detection.SphereOCRDuplicationDetectionEngine.check_duplication","text":"check_duplication ( ocr_result_1 : SphereOCRResult , ocr_result_2 : SphereOCRResult ) -> bool Check if two OCR results are duplicates. Parameters: Name Type Description Default ocr_result_1 SphereOCRResult First OCR result. required ocr_result_2 SphereOCRResult Second OCR result. required Returns: Type Description bool True if the results are considered duplicates. Source code in src/panoocr/dedup/detection.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def check_duplication ( self , ocr_result_1 : SphereOCRResult , ocr_result_2 : SphereOCRResult ) -> bool : \"\"\"Check if two OCR results are duplicates. Args: ocr_result_1: First OCR result. ocr_result_2: Second OCR result. Returns: True if the results are considered duplicates. \"\"\" text_similarity = self . _get_texts_similarity ( ocr_result_1 . text , ocr_result_2 . text ) text_overlap = self . _get_texts_overlap ( ocr_result_1 . text , ocr_result_2 . text ) # If texts are neither similar nor overlapping, not duplicates if ( text_similarity < self . min_text_similarity ) and ( text_overlap < self . min_text_overlap ): return False # Check spatial intersection intersection = self . _intersect_ocr_results ( ocr_result_1 , ocr_result_2 ) if intersection is None : return False if intersection . intersection_ratio < self . min_intersection_ratio : return False # Check if texts overlap and regions overlap sufficiently if ( text_overlap >= self . min_text_overlap and intersection . intersection_ratio >= self . min_intersection_ratio_for_overlapping_text ): return True # Check if texts are similar and regions overlap sufficiently if ( text_similarity >= self . min_text_similarity and intersection . intersection_ratio >= self . min_intersection_ratio_for_similar_text ): return True return False","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;check_duplication"},{"location":"api/dedup/#panoocr.dedup.detection.SphereOCRDuplicationDetectionEngine.remove_duplication_for_two_lists","text":"remove_duplication_for_two_lists ( ocr_results_0 : List [ SphereOCRResult ], ocr_results_1 : List [ SphereOCRResult ]) -> Tuple [ List [ SphereOCRResult ], List [ SphereOCRResult ]] Remove duplicates between two lists of OCR results. When duplicates are found, keeps the result with longer text, or higher confidence if texts are equal length. Parameters: Name Type Description Default ocr_results_0 List [ SphereOCRResult ] First list of OCR results (modified in place). required ocr_results_1 List [ SphereOCRResult ] Second list of OCR results (modified in place). required Returns: Type Description Tuple [ List [ SphereOCRResult ], List [ SphereOCRResult ]] Tuple of the two lists with duplicates removed. Source code in src/panoocr/dedup/detection.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def remove_duplication_for_two_lists ( self , ocr_results_0 : List [ SphereOCRResult ], ocr_results_1 : List [ SphereOCRResult ], ) -> Tuple [ List [ SphereOCRResult ], List [ SphereOCRResult ]]: \"\"\"Remove duplicates between two lists of OCR results. When duplicates are found, keeps the result with longer text, or higher confidence if texts are equal length. Args: ocr_results_0: First list of OCR results (modified in place). ocr_results_1: Second list of OCR results (modified in place). Returns: Tuple of the two lists with duplicates removed. \"\"\" # Find all duplicate pairs duplications = [] for i , ocr_result_0 in enumerate ( ocr_results_0 ): for j , ocr_result_1 in enumerate ( ocr_results_1 ): if self . check_duplication ( ocr_result_0 , ocr_result_1 ): duplications . append (( i , j )) # Determine which to remove from each list indices_to_remove_from_0 : List [ int ] = [] indices_to_remove_from_1 : List [ int ] = [] for i , j in duplications : candidate_0 = ocr_results_0 [ i ] candidate_1 = ocr_results_1 [ j ] if len ( candidate_0 . text ) == len ( candidate_1 . text ): # Equal length: prefer higher confidence if candidate_0 . confidence < candidate_1 . confidence : indices_to_remove_from_0 . append ( i ) else : indices_to_remove_from_1 . append ( j ) elif len ( candidate_0 . text ) > len ( candidate_1 . text ): # Prefer longer text indices_to_remove_from_1 . append ( j ) else : indices_to_remove_from_0 . append ( i ) # Remove duplicates (in reverse order to preserve indices) indices_to_remove_from_0 = sorted ( set ( indices_to_remove_from_0 ), reverse = True ) indices_to_remove_from_1 = sorted ( set ( indices_to_remove_from_1 ), reverse = True ) for index in indices_to_remove_from_0 : ocr_results_0 . pop ( index ) for index in indices_to_remove_from_1 : ocr_results_1 . pop ( index ) return ocr_results_0 , ocr_results_1","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;remove_duplication_for_two_lists"},{"location":"api/dedup/#panoocr.dedup.detection.SphereOCRDuplicationDetectionEngine.deduplicate_frames","text":"deduplicate_frames ( frames : List [ List [ SphereOCRResult ]]) -> List [ SphereOCRResult ] Deduplicate OCR results across multiple frames using incremental merging. Processes frames one by one, maintaining a master list. Each result from a new frame is compared against the entire master list. When duplicates are found, keeps the result with longer text or higher confidence. This approach is slower than pairwise deduplication but handles arbitrary perspective arrangements correctly (e.g., multiple pitch levels, custom arrangements). Parameters: Name Type Description Default frames List [ List [ SphereOCRResult ]] List of frames, each containing a list of OCR results. required Returns: Type Description List [ SphereOCRResult ] Deduplicated list of sphere OCR results. Source code in src/panoocr/dedup/detection.py 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 def deduplicate_frames ( self , frames : List [ List [ SphereOCRResult ]], ) -> List [ SphereOCRResult ]: \"\"\"Deduplicate OCR results across multiple frames using incremental merging. Processes frames one by one, maintaining a master list. Each result from a new frame is compared against the entire master list. When duplicates are found, keeps the result with longer text or higher confidence. This approach is slower than pairwise deduplication but handles arbitrary perspective arrangements correctly (e.g., multiple pitch levels, custom arrangements). Args: frames: List of frames, each containing a list of OCR results. Returns: Deduplicated list of sphere OCR results. \"\"\" if not frames : return [] # Start with the first frame as the master list master_list : List [ SphereOCRResult ] = list ( frames [ 0 ]) # Process each subsequent frame for frame_results in frames [ 1 :]: for new_result in frame_results : # Check for overlaps with existing master list duplicate_indices = [] for i , master_result in enumerate ( master_list ): if self . check_duplication ( new_result , master_result ): duplicate_indices . append ( i ) if not duplicate_indices : # No duplicates found, add to master list master_list . append ( new_result ) else : # Found duplicates - select best result among all candidates candidates = [ new_result ] + [ master_list [ i ] for i in duplicate_indices ] # Find the best result best_result = candidates [ 0 ] for candidate in candidates [ 1 :]: best_result = self . _select_best_result ( best_result , candidate ) # Remove all duplicates from master list (in reverse order) for i in sorted ( duplicate_indices , reverse = True ): master_list . pop ( i ) # Add the best result master_list . append ( best_result ) return master_list","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;deduplicate_frames"},{"location":"api/dedup/#data-classes","text":"","title":"Data Classes"},{"location":"api/dedup/#panoocr.dedup.detection.RegionIntersection","text":"RegionIntersection ( region_1_area : float , region_2_area : float , intersection_area : float , intersection_ratio : float ) Result of intersecting two regions. Attributes: Name Type Description region_1_area float Area of the first region. region_2_area float Area of the second region. intersection_area float Area of the intersection. intersection_ratio float Intersection area divided by minimum region area.","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;RegionIntersection"},{"location":"api/engines/","text":"Engines # PanoOCR uses dependency injection for OCR engines. Provide any object with a matching recognize() method. OCREngine Protocol # OCREngine # Bases: Protocol Protocol for OCR engines (structural typing). Any class with a matching recognize() method can be used. No inheritance required. recognize # recognize ( image : Image ) -> list [ FlatOCRResult ] Recognize text in an image. Parameters: Name Type Description Default image Image Input image as PIL Image. required Returns: Type Description list [ FlatOCRResult ] List of FlatOCRResult objects with normalized bounding boxes (0-1 range). Source code in src/panoocr/api/models.py 65 66 67 68 69 70 71 72 73 74 def recognize ( self , image : Image . Image ) -> list [ FlatOCRResult ]: \"\"\"Recognize text in an image. Args: image: Input image as PIL Image. Returns: List of FlatOCRResult objects with normalized bounding boxes (0-1 range). \"\"\" ... MacOCREngine # Uses Apple's Vision Framework for fast, accurate OCR on macOS. Requires the [macocr] extra. pip install \"panoocr[macocr]\" MacOCREngine # MacOCREngine ( config : Dict [ str , Any ] | None = None ) OCR engine using Apple Vision Framework via ocrmac. This engine uses macOS's built-in Vision Framework for text recognition. It provides excellent accuracy for many languages on Apple Silicon. Attributes: Name Type Description language_preference List of language codes to use for recognition. recognition_level Recognition accuracy level (\"fast\" or \"accurate\"). Example from panoocr.engines.macocr import MacOCREngine, MacOCRLanguageCode engine = MacOCREngine(config={ ... \"language_preference\": [MacOCRLanguageCode.ENGLISH_US], ... \"recognition_level\": MacOCRRecognitionLevel.ACCURATE, ... }) results = engine.recognize(image) Note Requires macOS and the ocrmac package. Install with: pip install \"panoocr[macocr]\" Initialize the MacOCR engine. Parameters: Name Type Description Default config Dict [ str , Any ] | None Configuration dictionary with optional keys: - language_preference: List of MacOCRLanguageCode values. - recognition_level: MacOCRRecognitionLevel value. None Raises: Type Description ImportError If ocrmac is not installed. ValueError If configuration values are invalid. Source code in src/panoocr/engines/macocr.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def __init__ ( self , config : Dict [ str , Any ] | None = None ) -> None : \"\"\"Initialize the MacOCR engine. Args: config: Configuration dictionary with optional keys: - language_preference: List of MacOCRLanguageCode values. - recognition_level: MacOCRRecognitionLevel value. Raises: ImportError: If ocrmac is not installed. ValueError: If configuration values are invalid. \"\"\" # Check dependencies first _check_macocr_dependencies () config = config or {} # Parse language preference language_preference = config . get ( \"language_preference\" , DEFAULT_LANGUAGE_PREFERENCE ) try : self . language_preference = [ lang . value if isinstance ( lang , MacOCRLanguageCode ) else lang for lang in language_preference ] except ( KeyError , AttributeError ): raise ValueError ( \"Invalid language code in language_preference\" ) # Parse recognition level recognition_level = config . get ( \"recognition_level\" , DEFAULT_RECOGNITION_LEVEL ) try : self . recognition_level = ( recognition_level . value if isinstance ( recognition_level , MacOCRRecognitionLevel ) else recognition_level ) except ( KeyError , AttributeError ): raise ValueError ( \"Invalid recognition level\" ) recognize # recognize ( image : Image ) -> List [ FlatOCRResult ] Recognize text in an image. Parameters: Name Type Description Default image Image Input image as PIL Image. required Returns: Type Description List [ FlatOCRResult ] List of FlatOCRResult with normalized bounding boxes. Source code in src/panoocr/engines/macocr.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def recognize ( self , image : Image . Image ) -> List [ FlatOCRResult ]: \"\"\"Recognize text in an image. Args: image: Input image as PIL Image. Returns: List of FlatOCRResult with normalized bounding boxes. \"\"\" import ocrmac.ocrmac annotations = ocrmac . ocrmac . OCR ( image , recognition_level = self . recognition_level , language_preference = self . language_preference , ) . recognize () mac_ocr_results = [ MacOCRResult ( text = annotation [ 0 ], confidence = annotation [ 1 ], bounding_box = annotation [ 2 ], ) for annotation in annotations ] return [ result . to_flat () for result in mac_ocr_results ] EasyOCREngine # Cross-platform OCR supporting 80+ languages. Requires the [easyocr] extra. pip install \"panoocr[easyocr]\" EasyOCREngine # EasyOCREngine ( config : Dict [ str , Any ] | None = None ) OCR engine using EasyOCR library. EasyOCR supports 80+ languages and can run on CPU or GPU. It provides good accuracy for many scripts including CJK. Attributes: Name Type Description language_preference List of language codes to use. reader EasyOCR Reader instance. Example from panoocr.engines.easyocr import EasyOCREngine, EasyOCRLanguageCode engine = EasyOCREngine(config={ ... \"language_preference\": [EasyOCRLanguageCode.ENGLISH], ... \"gpu\": True, ... }) results = engine.recognize(image) Note Install with: pip install \"panoocr[easyocr]\" For GPU support, install PyTorch with CUDA. Initialize the EasyOCR engine. Parameters: Name Type Description Default config Dict [ str , Any ] | None Configuration dictionary with optional keys: - language_preference: List of EasyOCRLanguageCode values. - gpu: Whether to use GPU (default: True). None Raises: Type Description ImportError If easyocr is not installed. ValueError If configuration values are invalid. Source code in src/panoocr/engines/easyocr.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def __init__ ( self , config : Dict [ str , Any ] | None = None ) -> None : \"\"\"Initialize the EasyOCR engine. Args: config: Configuration dictionary with optional keys: - language_preference: List of EasyOCRLanguageCode values. - gpu: Whether to use GPU (default: True). Raises: ImportError: If easyocr is not installed. ValueError: If configuration values are invalid. \"\"\" # Check dependencies first _check_easyocr_dependencies () import easyocr config = config or {} # Parse language preference language_preference = config . get ( \"language_preference\" , DEFAULT_LANGUAGE_PREFERENCE ) try : self . language_preference = [ lang . value if isinstance ( lang , EasyOCRLanguageCode ) else lang for lang in language_preference ] except ( KeyError , AttributeError ): raise ValueError ( \"Invalid language code in language_preference\" ) # Parse GPU setting use_gpu = config . get ( \"gpu\" , True ) # Initialize reader self . reader = easyocr . Reader ( self . language_preference , gpu = use_gpu ) recognize # recognize ( image : Image ) -> List [ FlatOCRResult ] Recognize text in an image. Parameters: Name Type Description Default image Image Input image as PIL Image. required Returns: Type Description List [ FlatOCRResult ] List of FlatOCRResult with normalized bounding boxes. Source code in src/panoocr/engines/easyocr.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def recognize ( self , image : Image . Image ) -> List [ FlatOCRResult ]: \"\"\"Recognize text in an image. Args: image: Input image as PIL Image. Returns: List of FlatOCRResult with normalized bounding boxes. \"\"\" image_array = np . array ( image ) annotations = self . reader . readtext ( image_array ) easyocr_results = [] for annotation in annotations : bounding_box = annotation [ 0 ] text = annotation [ 1 ] confidence = annotation [ 2 ] easyocr_results . append ( EasyOCRResult ( text = text , confidence = confidence , bounding_box = bounding_box , image_width = image . width , image_height = image . height , ) ) return [ result . to_flat () for result in easyocr_results ] PaddleOCREngine # PaddlePaddle-based OCR with optional V4 server model for Chinese text. Requires the [paddleocr] extra. pip install \"panoocr[paddleocr]\" PaddleOCREngine # PaddleOCREngine ( config : Dict [ str , Any ] | None = None ) OCR engine using PaddleOCR library. PaddleOCR is developed by PaddlePaddle and supports multiple languages. It provides good accuracy and can optionally use the V4 server model for better results on Chinese text. Attributes: Name Type Description language_preference Language code for recognition. recognize_upside_down Whether to use angle classifier. use_v4_server Whether to use the V4 server model. Example from panoocr.engines.paddleocr import PaddleOCREngine, PaddleOCRLanguageCode engine = PaddleOCREngine(config={ ... \"language_preference\": PaddleOCRLanguageCode.CHINESE, ... \"use_gpu\": True, ... }) results = engine.recognize(image) Note Install with: pip install \"panoocr[paddleocr]\" For GPU support, install paddlepaddle-gpu. Initialize the PaddleOCR engine. Parameters: Name Type Description Default config Dict [ str , Any ] | None Configuration dictionary with optional keys: - language_preference: PaddleOCRLanguageCode value. - recognize_upside_down: Enable angle classifier (default: False). - use_v4_server: Use V4 server model for better Chinese OCR. - use_gpu: Whether to use GPU (default: True). - model_dir: Custom directory for V4 server models. None Raises: Type Description ImportError If paddleocr is not installed. ValueError If configuration values are invalid. Source code in src/panoocr/engines/paddleocr.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def __init__ ( self , config : Dict [ str , Any ] | None = None ) -> None : \"\"\"Initialize the PaddleOCR engine. Args: config: Configuration dictionary with optional keys: - language_preference: PaddleOCRLanguageCode value. - recognize_upside_down: Enable angle classifier (default: False). - use_v4_server: Use V4 server model for better Chinese OCR. - use_gpu: Whether to use GPU (default: True). - model_dir: Custom directory for V4 server models. Raises: ImportError: If paddleocr is not installed. ValueError: If configuration values are invalid. \"\"\" # Check dependencies first _check_paddleocr_dependencies () from paddleocr import PaddleOCR config = config or {} # Parse language preference language = config . get ( \"language_preference\" , DEFAULT_LANGUAGE ) try : self . language_preference = ( language . value if isinstance ( language , PaddleOCRLanguageCode ) else language ) except ( KeyError , AttributeError ): raise ValueError ( \"Invalid language code\" ) # Parse other settings self . recognize_upside_down = config . get ( \"recognize_upside_down\" , DEFAULT_RECOGNIZE_UPSIDE_DOWN ) if not isinstance ( self . recognize_upside_down , bool ): raise ValueError ( \"recognize_upside_down must be a boolean\" ) self . use_v4_server = config . get ( \"use_v4_server\" , False ) if not isinstance ( self . use_v4_server , bool ): raise ValueError ( \"use_v4_server must be a boolean\" ) use_gpu = config . get ( \"use_gpu\" , True ) self . model_dir = config . get ( \"model_dir\" , \"./models\" ) # Initialize OCR engine if not self . use_v4_server : self . ocr = PaddleOCR ( use_angle_cls = self . recognize_upside_down , lang = self . language_preference , use_gpu = use_gpu , ) else : # Download and setup V4 server models self . _download_v4_server_models () model_base = Path ( self . model_dir ) / \"PP-OCRv4\" / \"chinese\" self . ocr = PaddleOCR ( use_angle_cls = self . recognize_upside_down , det_model_dir = str ( model_base / \"ch_PP-OCRv4_det_server_infer\" ), det_algorithm = \"DB\" , rec_model_dir = str ( model_base / \"ch_PP-OCRv4_rec_server_infer\" ), rec_algorithm = \"CRNN\" , cls_model_dir = str ( model_base / \"ch_ppocr_mobile_v2.0_cls_slim_infer\" ), use_gpu = use_gpu , ) recognize # recognize ( image : Image ) -> List [ FlatOCRResult ] Recognize text in an image. Parameters: Name Type Description Default image Image Input image as PIL Image. required Returns: Type Description List [ FlatOCRResult ] List of FlatOCRResult with normalized bounding boxes. Source code in src/panoocr/engines/paddleocr.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def recognize ( self , image : Image . Image ) -> List [ FlatOCRResult ]: \"\"\"Recognize text in an image. Args: image: Input image as PIL Image. Returns: List of FlatOCRResult with normalized bounding boxes. \"\"\" image_array = np . array ( image ) # Use slicing for large images slice_config = { \"horizontal_stride\" : 300 , \"vertical_stride\" : 500 , \"merge_x_thres\" : 50 , \"merge_y_thres\" : 35 , } annotations = self . ocr . ocr ( image_array , cls = True , slice = slice_config ) paddle_results = [] for annotation in annotations : if not isinstance ( annotation , list ): continue for res in annotation : bounding_box = res [ 0 ] text = res [ 1 ][ 0 ] confidence = res [ 1 ][ 1 ] paddle_results . append ( PaddleOCRResult ( text = text , confidence = confidence , bounding_box = bounding_box , image_width = image . width , image_height = image . height , use_v4_server = self . use_v4_server , ) ) return [ result . to_flat () for result in paddle_results ] Florence2OCREngine # Microsoft's Florence-2 vision-language model for OCR. Requires the [florence2] extra. pip install \"panoocr[florence2]\" Florence2OCREngine # Florence2OCREngine ( config : Dict [ str , Any ] | None = None ) OCR engine using Microsoft's Florence-2 model. Florence-2 is a vision-language model that can perform OCR with region detection. It provides good accuracy across many languages and can detect text in various orientations. Attributes: Name Type Description device Device to run inference on (cuda, mps, or cpu). model The Florence-2 model. processor The Florence-2 processor. Example from panoocr.engines.florence2 import Florence2OCREngine engine = Florence2OCREngine(config={ ... \"model_id\": \"microsoft/Florence-2-large\", ... }) results = engine.recognize(image) Note Install with: pip install \"panoocr[florence2]\" For GPU support, install PyTorch with CUDA. Initialize the Florence-2 engine. Parameters: Name Type Description Default config Dict [ str , Any ] | None Configuration dictionary with optional keys: - model_id: HuggingFace model ID (default: \"microsoft/Florence-2-large\"). - device: Device to use (\"cuda\", \"mps\", \"cpu\", or None for auto). None Raises: Type Description ImportError If dependencies are not installed. Source code in src/panoocr/engines/florence2.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def __init__ ( self , config : Dict [ str , Any ] | None = None ) -> None : \"\"\"Initialize the Florence-2 engine. Args: config: Configuration dictionary with optional keys: - model_id: HuggingFace model ID (default: \"microsoft/Florence-2-large\"). - device: Device to use (\"cuda\", \"mps\", \"cpu\", or None for auto). Raises: ImportError: If dependencies are not installed. \"\"\" # Check dependencies first _check_florence2_dependencies () import torch from transformers import AutoProcessor , AutoModelForCausalLM config = config or {} model_id = config . get ( \"model_id\" , \"microsoft/Florence-2-large\" ) # Auto-detect device device = config . get ( \"device\" ) if device is None : if torch . cuda . is_available (): device = \"cuda\" elif torch . backends . mps . is_available (): device = \"mps\" else : device = \"cpu\" self . device = device # Select dtype based on device if torch . cuda . is_available () and device == \"cuda\" : self . dtype = torch . float16 else : self . dtype = torch . float32 print ( f \"Loading Florence-2 model on { device } ...\" ) self . model = AutoModelForCausalLM . from_pretrained ( model_id , torch_dtype = self . dtype , trust_remote_code = True ) . to ( device ) self . processor = AutoProcessor . from_pretrained ( model_id , trust_remote_code = True ) print ( \"Florence-2 model loaded successfully.\" ) self . prompt = \"<OCR_WITH_REGION>\" recognize # recognize ( image : Image ) -> List [ FlatOCRResult ] Recognize text in an image. Parameters: Name Type Description Default image Image Input image as PIL Image. required Returns: Type Description List [ FlatOCRResult ] List of FlatOCRResult with normalized bounding boxes. Source code in src/panoocr/engines/florence2.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def recognize ( self , image : Image . Image ) -> List [ FlatOCRResult ]: \"\"\"Recognize text in an image. Args: image: Input image as PIL Image. Returns: List of FlatOCRResult with normalized bounding boxes. \"\"\" import torch inputs = self . processor ( text = self . prompt , images = image , return_tensors = \"pt\" ) . to ( self . device , self . dtype ) generated_ids = self . model . generate ( input_ids = inputs [ \"input_ids\" ], pixel_values = inputs [ \"pixel_values\" ], max_new_tokens = 1024 , num_beams = 3 , do_sample = False , ) generated_text = self . processor . batch_decode ( generated_ids , skip_special_tokens = False )[ 0 ] parsed_answer = self . processor . post_process_generation ( generated_text , task = \"<OCR_WITH_REGION>\" , image_size = ( image . width , image . height ), ) florence2_results = [] try : ocr_data = parsed_answer . get ( \"<OCR_WITH_REGION>\" , {}) quad_boxes = ocr_data . get ( \"quad_boxes\" , []) labels = ocr_data . get ( \"labels\" , []) for quad_box , label in zip ( quad_boxes , labels ): # Clean up text label = label . replace ( \"</s>\" , \"\" ) . replace ( \"<s>\" , \"\" ) # Convert quad_box [x1,y1,x2,y2,x3,y3,x4,y4] to corner points bounding_box = [ [ quad_box [ 0 ], quad_box [ 1 ]], [ quad_box [ 2 ], quad_box [ 3 ]], [ quad_box [ 4 ], quad_box [ 5 ]], [ quad_box [ 6 ], quad_box [ 7 ]], ] florence2_results . append ( Florence2OCRResult ( text = label , bounding_box = bounding_box , image_width = image . width , image_height = image . height , ) ) except KeyError : print ( \"Error parsing OCR results, returning empty list\" ) # Clean up to prevent memory leak del inputs del generated_ids del generated_text del parsed_answer gc . collect () if str ( self . device ) . startswith ( \"cuda\" ): import torch torch . cuda . empty_cache () return [ result . to_flat () for result in florence2_results ] TrOCREngine # Microsoft's TrOCR transformer-based OCR. Requires the [trocr] extra. pip install \"panoocr[trocr]\" TrOCREngine # TrOCREngine ( config : Dict [ str , Any ] | None = None ) OCR engine using Microsoft's TrOCR model. TrOCR is a transformer-based OCR model that excels at single-line text recognition. It does NOT provide bounding boxes - it reads the entire image as a single text line. WARNING: This engine is experimental and may not work well for panorama OCR since it doesn't detect text regions. Consider using Florence2OCREngine or other engines that provide region detection. Attributes: Name Type Description model The TrOCR model. processor The TrOCR processor. Example from panoocr.engines.trocr import TrOCREngine, TrOCRModel engine = TrOCREngine(config={ ... \"model\": TrOCRModel.MICROSOFT_TROCR_LARGE_PRINTED, ... }) Note: Returns single result for entire image # results = engine.recognize(cropped_text_image) Note Install with: pip install \"panoocr[trocr]\" For GPU support, install PyTorch with CUDA. Initialize the TrOCR engine. Parameters: Name Type Description Default config Dict [ str , Any ] | None Configuration dictionary with optional keys: - model: TrOCRModel enum value or model ID string. - device: Device to use (\"cuda\", \"mps\", \"cpu\", or None for auto). None Raises: Type Description ImportError If dependencies are not installed. ValueError If configuration values are invalid. Source code in src/panoocr/engines/trocr.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def __init__ ( self , config : Dict [ str , Any ] | None = None ) -> None : \"\"\"Initialize the TrOCR engine. Args: config: Configuration dictionary with optional keys: - model: TrOCRModel enum value or model ID string. - device: Device to use (\"cuda\", \"mps\", \"cpu\", or None for auto). Raises: ImportError: If dependencies are not installed. ValueError: If configuration values are invalid. \"\"\" # Check dependencies first _check_trocr_dependencies () import torch from transformers import TrOCRProcessor , VisionEncoderDecoderModel config = config or {} # Parse model model = config . get ( \"model\" , DEFAULT_MODEL ) try : model_id = model . value if isinstance ( model , TrOCRModel ) else model except ( KeyError , AttributeError ): raise ValueError ( \"Invalid model specified\" ) # Auto-detect device device = config . get ( \"device\" ) if device is None : if torch . cuda . is_available (): device = \"cuda\" elif torch . backends . mps . is_available (): device = \"mps\" else : device = \"cpu\" self . device = device print ( f \"Loading TrOCR model { model_id } on { device } ...\" ) self . processor = TrOCRProcessor . from_pretrained ( model_id ) self . model = VisionEncoderDecoderModel . from_pretrained ( model_id ) . to ( device ) print ( \"TrOCR model loaded successfully.\" ) recognize # recognize ( image : Image ) -> List [ FlatOCRResult ] Recognize text in an image. NOTE: TrOCR treats the entire image as a single text line and does not provide bounding boxes. This makes it unsuitable for most panorama OCR use cases. The result will have a bounding box covering the entire image. Parameters: Name Type Description Default image Image Input image as PIL Image. required Returns: Type Description List [ FlatOCRResult ] List with single FlatOCRResult covering the entire image, or empty List [ FlatOCRResult ] list if no text is recognized. Source code in src/panoocr/engines/trocr.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def recognize ( self , image : Image . Image ) -> List [ FlatOCRResult ]: \"\"\"Recognize text in an image. NOTE: TrOCR treats the entire image as a single text line and does not provide bounding boxes. This makes it unsuitable for most panorama OCR use cases. The result will have a bounding box covering the entire image. Args: image: Input image as PIL Image. Returns: List with single FlatOCRResult covering the entire image, or empty list if no text is recognized. \"\"\" import torch # Convert to RGB if needed if image . mode != \"RGB\" : image = image . convert ( \"RGB\" ) pixel_values = self . processor ( images = image , return_tensors = \"pt\" ) . pixel_values pixel_values = pixel_values . to ( self . device ) with torch . no_grad (): generated_ids = self . model . generate ( pixel_values ) generated_text = self . processor . batch_decode ( generated_ids , skip_special_tokens = True )[ 0 ] # TrOCR doesn't provide bounding boxes - return full image bbox if generated_text . strip (): return [ FlatOCRResult ( text = generated_text . strip (), confidence = 1.0 , # TrOCR doesn't provide confidence bounding_box = BoundingBox ( left = 0.0 , top = 0.0 , right = 1.0 , bottom = 1.0 , width = 1.0 , height = 1.0 , ), engine = \"TROCR\" , ) ] return [] Custom Engines # Any class with a compatible recognize() method works: from panoocr import PanoOCR , FlatOCRResult , BoundingBox from PIL import Image class MyEngine : def recognize ( self , image : Image . Image ) -> list [ FlatOCRResult ]: # Return list of FlatOCRResult with normalized bounding boxes (0-1) return [ FlatOCRResult ( text = \"Hello\" , confidence = 0.95 , bounding_box = BoundingBox ( left = 0.1 , top = 0.2 , right = 0.4 , bottom = 0.3 , width = 0.3 , height = 0.1 ), engine = \"my_engine\" , ) ] pano = PanoOCR ( engine = MyEngine ())","title":"Engines"},{"location":"api/engines/#engines","text":"PanoOCR uses dependency injection for OCR engines. Provide any object with a matching recognize() method.","title":"Engines"},{"location":"api/engines/#ocrengine-protocol","text":"","title":"OCREngine Protocol"},{"location":"api/engines/#panoocr.api.models.OCREngine","text":"Bases: Protocol Protocol for OCR engines (structural typing). Any class with a matching recognize() method can be used. No inheritance required.","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;OCREngine"},{"location":"api/engines/#panoocr.api.models.OCREngine.recognize","text":"recognize ( image : Image ) -> list [ FlatOCRResult ] Recognize text in an image. Parameters: Name Type Description Default image Image Input image as PIL Image. required Returns: Type Description list [ FlatOCRResult ] List of FlatOCRResult objects with normalized bounding boxes (0-1 range). Source code in src/panoocr/api/models.py 65 66 67 68 69 70 71 72 73 74 def recognize ( self , image : Image . Image ) -> list [ FlatOCRResult ]: \"\"\"Recognize text in an image. Args: image: Input image as PIL Image. Returns: List of FlatOCRResult objects with normalized bounding boxes (0-1 range). \"\"\" ...","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;recognize"},{"location":"api/engines/#macocrengine","text":"Uses Apple's Vision Framework for fast, accurate OCR on macOS. Requires the [macocr] extra. pip install \"panoocr[macocr]\"","title":"MacOCREngine"},{"location":"api/engines/#panoocr.engines.macocr.MacOCREngine","text":"MacOCREngine ( config : Dict [ str , Any ] | None = None ) OCR engine using Apple Vision Framework via ocrmac. This engine uses macOS's built-in Vision Framework for text recognition. It provides excellent accuracy for many languages on Apple Silicon. Attributes: Name Type Description language_preference List of language codes to use for recognition. recognition_level Recognition accuracy level (\"fast\" or \"accurate\"). Example from panoocr.engines.macocr import MacOCREngine, MacOCRLanguageCode engine = MacOCREngine(config={ ... \"language_preference\": [MacOCRLanguageCode.ENGLISH_US], ... \"recognition_level\": MacOCRRecognitionLevel.ACCURATE, ... }) results = engine.recognize(image) Note Requires macOS and the ocrmac package. Install with: pip install \"panoocr[macocr]\" Initialize the MacOCR engine. Parameters: Name Type Description Default config Dict [ str , Any ] | None Configuration dictionary with optional keys: - language_preference: List of MacOCRLanguageCode values. - recognition_level: MacOCRRecognitionLevel value. None Raises: Type Description ImportError If ocrmac is not installed. ValueError If configuration values are invalid. Source code in src/panoocr/engines/macocr.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def __init__ ( self , config : Dict [ str , Any ] | None = None ) -> None : \"\"\"Initialize the MacOCR engine. Args: config: Configuration dictionary with optional keys: - language_preference: List of MacOCRLanguageCode values. - recognition_level: MacOCRRecognitionLevel value. Raises: ImportError: If ocrmac is not installed. ValueError: If configuration values are invalid. \"\"\" # Check dependencies first _check_macocr_dependencies () config = config or {} # Parse language preference language_preference = config . get ( \"language_preference\" , DEFAULT_LANGUAGE_PREFERENCE ) try : self . language_preference = [ lang . value if isinstance ( lang , MacOCRLanguageCode ) else lang for lang in language_preference ] except ( KeyError , AttributeError ): raise ValueError ( \"Invalid language code in language_preference\" ) # Parse recognition level recognition_level = config . get ( \"recognition_level\" , DEFAULT_RECOGNITION_LEVEL ) try : self . recognition_level = ( recognition_level . value if isinstance ( recognition_level , MacOCRRecognitionLevel ) else recognition_level ) except ( KeyError , AttributeError ): raise ValueError ( \"Invalid recognition level\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;MacOCREngine"},{"location":"api/engines/#panoocr.engines.macocr.MacOCREngine.recognize","text":"recognize ( image : Image ) -> List [ FlatOCRResult ] Recognize text in an image. Parameters: Name Type Description Default image Image Input image as PIL Image. required Returns: Type Description List [ FlatOCRResult ] List of FlatOCRResult with normalized bounding boxes. Source code in src/panoocr/engines/macocr.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def recognize ( self , image : Image . Image ) -> List [ FlatOCRResult ]: \"\"\"Recognize text in an image. Args: image: Input image as PIL Image. Returns: List of FlatOCRResult with normalized bounding boxes. \"\"\" import ocrmac.ocrmac annotations = ocrmac . ocrmac . OCR ( image , recognition_level = self . recognition_level , language_preference = self . language_preference , ) . recognize () mac_ocr_results = [ MacOCRResult ( text = annotation [ 0 ], confidence = annotation [ 1 ], bounding_box = annotation [ 2 ], ) for annotation in annotations ] return [ result . to_flat () for result in mac_ocr_results ]","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;recognize"},{"location":"api/engines/#easyocrengine","text":"Cross-platform OCR supporting 80+ languages. Requires the [easyocr] extra. pip install \"panoocr[easyocr]\"","title":"EasyOCREngine"},{"location":"api/engines/#panoocr.engines.easyocr.EasyOCREngine","text":"EasyOCREngine ( config : Dict [ str , Any ] | None = None ) OCR engine using EasyOCR library. EasyOCR supports 80+ languages and can run on CPU or GPU. It provides good accuracy for many scripts including CJK. Attributes: Name Type Description language_preference List of language codes to use. reader EasyOCR Reader instance. Example from panoocr.engines.easyocr import EasyOCREngine, EasyOCRLanguageCode engine = EasyOCREngine(config={ ... \"language_preference\": [EasyOCRLanguageCode.ENGLISH], ... \"gpu\": True, ... }) results = engine.recognize(image) Note Install with: pip install \"panoocr[easyocr]\" For GPU support, install PyTorch with CUDA. Initialize the EasyOCR engine. Parameters: Name Type Description Default config Dict [ str , Any ] | None Configuration dictionary with optional keys: - language_preference: List of EasyOCRLanguageCode values. - gpu: Whether to use GPU (default: True). None Raises: Type Description ImportError If easyocr is not installed. ValueError If configuration values are invalid. Source code in src/panoocr/engines/easyocr.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def __init__ ( self , config : Dict [ str , Any ] | None = None ) -> None : \"\"\"Initialize the EasyOCR engine. Args: config: Configuration dictionary with optional keys: - language_preference: List of EasyOCRLanguageCode values. - gpu: Whether to use GPU (default: True). Raises: ImportError: If easyocr is not installed. ValueError: If configuration values are invalid. \"\"\" # Check dependencies first _check_easyocr_dependencies () import easyocr config = config or {} # Parse language preference language_preference = config . get ( \"language_preference\" , DEFAULT_LANGUAGE_PREFERENCE ) try : self . language_preference = [ lang . value if isinstance ( lang , EasyOCRLanguageCode ) else lang for lang in language_preference ] except ( KeyError , AttributeError ): raise ValueError ( \"Invalid language code in language_preference\" ) # Parse GPU setting use_gpu = config . get ( \"gpu\" , True ) # Initialize reader self . reader = easyocr . Reader ( self . language_preference , gpu = use_gpu )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;EasyOCREngine"},{"location":"api/engines/#panoocr.engines.easyocr.EasyOCREngine.recognize","text":"recognize ( image : Image ) -> List [ FlatOCRResult ] Recognize text in an image. Parameters: Name Type Description Default image Image Input image as PIL Image. required Returns: Type Description List [ FlatOCRResult ] List of FlatOCRResult with normalized bounding boxes. Source code in src/panoocr/engines/easyocr.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def recognize ( self , image : Image . Image ) -> List [ FlatOCRResult ]: \"\"\"Recognize text in an image. Args: image: Input image as PIL Image. Returns: List of FlatOCRResult with normalized bounding boxes. \"\"\" image_array = np . array ( image ) annotations = self . reader . readtext ( image_array ) easyocr_results = [] for annotation in annotations : bounding_box = annotation [ 0 ] text = annotation [ 1 ] confidence = annotation [ 2 ] easyocr_results . append ( EasyOCRResult ( text = text , confidence = confidence , bounding_box = bounding_box , image_width = image . width , image_height = image . height , ) ) return [ result . to_flat () for result in easyocr_results ]","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;recognize"},{"location":"api/engines/#paddleocrengine","text":"PaddlePaddle-based OCR with optional V4 server model for Chinese text. Requires the [paddleocr] extra. pip install \"panoocr[paddleocr]\"","title":"PaddleOCREngine"},{"location":"api/engines/#panoocr.engines.paddleocr.PaddleOCREngine","text":"PaddleOCREngine ( config : Dict [ str , Any ] | None = None ) OCR engine using PaddleOCR library. PaddleOCR is developed by PaddlePaddle and supports multiple languages. It provides good accuracy and can optionally use the V4 server model for better results on Chinese text. Attributes: Name Type Description language_preference Language code for recognition. recognize_upside_down Whether to use angle classifier. use_v4_server Whether to use the V4 server model. Example from panoocr.engines.paddleocr import PaddleOCREngine, PaddleOCRLanguageCode engine = PaddleOCREngine(config={ ... \"language_preference\": PaddleOCRLanguageCode.CHINESE, ... \"use_gpu\": True, ... }) results = engine.recognize(image) Note Install with: pip install \"panoocr[paddleocr]\" For GPU support, install paddlepaddle-gpu. Initialize the PaddleOCR engine. Parameters: Name Type Description Default config Dict [ str , Any ] | None Configuration dictionary with optional keys: - language_preference: PaddleOCRLanguageCode value. - recognize_upside_down: Enable angle classifier (default: False). - use_v4_server: Use V4 server model for better Chinese OCR. - use_gpu: Whether to use GPU (default: True). - model_dir: Custom directory for V4 server models. None Raises: Type Description ImportError If paddleocr is not installed. ValueError If configuration values are invalid. Source code in src/panoocr/engines/paddleocr.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def __init__ ( self , config : Dict [ str , Any ] | None = None ) -> None : \"\"\"Initialize the PaddleOCR engine. Args: config: Configuration dictionary with optional keys: - language_preference: PaddleOCRLanguageCode value. - recognize_upside_down: Enable angle classifier (default: False). - use_v4_server: Use V4 server model for better Chinese OCR. - use_gpu: Whether to use GPU (default: True). - model_dir: Custom directory for V4 server models. Raises: ImportError: If paddleocr is not installed. ValueError: If configuration values are invalid. \"\"\" # Check dependencies first _check_paddleocr_dependencies () from paddleocr import PaddleOCR config = config or {} # Parse language preference language = config . get ( \"language_preference\" , DEFAULT_LANGUAGE ) try : self . language_preference = ( language . value if isinstance ( language , PaddleOCRLanguageCode ) else language ) except ( KeyError , AttributeError ): raise ValueError ( \"Invalid language code\" ) # Parse other settings self . recognize_upside_down = config . get ( \"recognize_upside_down\" , DEFAULT_RECOGNIZE_UPSIDE_DOWN ) if not isinstance ( self . recognize_upside_down , bool ): raise ValueError ( \"recognize_upside_down must be a boolean\" ) self . use_v4_server = config . get ( \"use_v4_server\" , False ) if not isinstance ( self . use_v4_server , bool ): raise ValueError ( \"use_v4_server must be a boolean\" ) use_gpu = config . get ( \"use_gpu\" , True ) self . model_dir = config . get ( \"model_dir\" , \"./models\" ) # Initialize OCR engine if not self . use_v4_server : self . ocr = PaddleOCR ( use_angle_cls = self . recognize_upside_down , lang = self . language_preference , use_gpu = use_gpu , ) else : # Download and setup V4 server models self . _download_v4_server_models () model_base = Path ( self . model_dir ) / \"PP-OCRv4\" / \"chinese\" self . ocr = PaddleOCR ( use_angle_cls = self . recognize_upside_down , det_model_dir = str ( model_base / \"ch_PP-OCRv4_det_server_infer\" ), det_algorithm = \"DB\" , rec_model_dir = str ( model_base / \"ch_PP-OCRv4_rec_server_infer\" ), rec_algorithm = \"CRNN\" , cls_model_dir = str ( model_base / \"ch_ppocr_mobile_v2.0_cls_slim_infer\" ), use_gpu = use_gpu , )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;PaddleOCREngine"},{"location":"api/engines/#panoocr.engines.paddleocr.PaddleOCREngine.recognize","text":"recognize ( image : Image ) -> List [ FlatOCRResult ] Recognize text in an image. Parameters: Name Type Description Default image Image Input image as PIL Image. required Returns: Type Description List [ FlatOCRResult ] List of FlatOCRResult with normalized bounding boxes. Source code in src/panoocr/engines/paddleocr.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def recognize ( self , image : Image . Image ) -> List [ FlatOCRResult ]: \"\"\"Recognize text in an image. Args: image: Input image as PIL Image. Returns: List of FlatOCRResult with normalized bounding boxes. \"\"\" image_array = np . array ( image ) # Use slicing for large images slice_config = { \"horizontal_stride\" : 300 , \"vertical_stride\" : 500 , \"merge_x_thres\" : 50 , \"merge_y_thres\" : 35 , } annotations = self . ocr . ocr ( image_array , cls = True , slice = slice_config ) paddle_results = [] for annotation in annotations : if not isinstance ( annotation , list ): continue for res in annotation : bounding_box = res [ 0 ] text = res [ 1 ][ 0 ] confidence = res [ 1 ][ 1 ] paddle_results . append ( PaddleOCRResult ( text = text , confidence = confidence , bounding_box = bounding_box , image_width = image . width , image_height = image . height , use_v4_server = self . use_v4_server , ) ) return [ result . to_flat () for result in paddle_results ]","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;recognize"},{"location":"api/engines/#florence2ocrengine","text":"Microsoft's Florence-2 vision-language model for OCR. Requires the [florence2] extra. pip install \"panoocr[florence2]\"","title":"Florence2OCREngine"},{"location":"api/engines/#panoocr.engines.florence2.Florence2OCREngine","text":"Florence2OCREngine ( config : Dict [ str , Any ] | None = None ) OCR engine using Microsoft's Florence-2 model. Florence-2 is a vision-language model that can perform OCR with region detection. It provides good accuracy across many languages and can detect text in various orientations. Attributes: Name Type Description device Device to run inference on (cuda, mps, or cpu). model The Florence-2 model. processor The Florence-2 processor. Example from panoocr.engines.florence2 import Florence2OCREngine engine = Florence2OCREngine(config={ ... \"model_id\": \"microsoft/Florence-2-large\", ... }) results = engine.recognize(image) Note Install with: pip install \"panoocr[florence2]\" For GPU support, install PyTorch with CUDA. Initialize the Florence-2 engine. Parameters: Name Type Description Default config Dict [ str , Any ] | None Configuration dictionary with optional keys: - model_id: HuggingFace model ID (default: \"microsoft/Florence-2-large\"). - device: Device to use (\"cuda\", \"mps\", \"cpu\", or None for auto). None Raises: Type Description ImportError If dependencies are not installed. Source code in src/panoocr/engines/florence2.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def __init__ ( self , config : Dict [ str , Any ] | None = None ) -> None : \"\"\"Initialize the Florence-2 engine. Args: config: Configuration dictionary with optional keys: - model_id: HuggingFace model ID (default: \"microsoft/Florence-2-large\"). - device: Device to use (\"cuda\", \"mps\", \"cpu\", or None for auto). Raises: ImportError: If dependencies are not installed. \"\"\" # Check dependencies first _check_florence2_dependencies () import torch from transformers import AutoProcessor , AutoModelForCausalLM config = config or {} model_id = config . get ( \"model_id\" , \"microsoft/Florence-2-large\" ) # Auto-detect device device = config . get ( \"device\" ) if device is None : if torch . cuda . is_available (): device = \"cuda\" elif torch . backends . mps . is_available (): device = \"mps\" else : device = \"cpu\" self . device = device # Select dtype based on device if torch . cuda . is_available () and device == \"cuda\" : self . dtype = torch . float16 else : self . dtype = torch . float32 print ( f \"Loading Florence-2 model on { device } ...\" ) self . model = AutoModelForCausalLM . from_pretrained ( model_id , torch_dtype = self . dtype , trust_remote_code = True ) . to ( device ) self . processor = AutoProcessor . from_pretrained ( model_id , trust_remote_code = True ) print ( \"Florence-2 model loaded successfully.\" ) self . prompt = \"<OCR_WITH_REGION>\"","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;Florence2OCREngine"},{"location":"api/engines/#panoocr.engines.florence2.Florence2OCREngine.recognize","text":"recognize ( image : Image ) -> List [ FlatOCRResult ] Recognize text in an image. Parameters: Name Type Description Default image Image Input image as PIL Image. required Returns: Type Description List [ FlatOCRResult ] List of FlatOCRResult with normalized bounding boxes. Source code in src/panoocr/engines/florence2.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def recognize ( self , image : Image . Image ) -> List [ FlatOCRResult ]: \"\"\"Recognize text in an image. Args: image: Input image as PIL Image. Returns: List of FlatOCRResult with normalized bounding boxes. \"\"\" import torch inputs = self . processor ( text = self . prompt , images = image , return_tensors = \"pt\" ) . to ( self . device , self . dtype ) generated_ids = self . model . generate ( input_ids = inputs [ \"input_ids\" ], pixel_values = inputs [ \"pixel_values\" ], max_new_tokens = 1024 , num_beams = 3 , do_sample = False , ) generated_text = self . processor . batch_decode ( generated_ids , skip_special_tokens = False )[ 0 ] parsed_answer = self . processor . post_process_generation ( generated_text , task = \"<OCR_WITH_REGION>\" , image_size = ( image . width , image . height ), ) florence2_results = [] try : ocr_data = parsed_answer . get ( \"<OCR_WITH_REGION>\" , {}) quad_boxes = ocr_data . get ( \"quad_boxes\" , []) labels = ocr_data . get ( \"labels\" , []) for quad_box , label in zip ( quad_boxes , labels ): # Clean up text label = label . replace ( \"</s>\" , \"\" ) . replace ( \"<s>\" , \"\" ) # Convert quad_box [x1,y1,x2,y2,x3,y3,x4,y4] to corner points bounding_box = [ [ quad_box [ 0 ], quad_box [ 1 ]], [ quad_box [ 2 ], quad_box [ 3 ]], [ quad_box [ 4 ], quad_box [ 5 ]], [ quad_box [ 6 ], quad_box [ 7 ]], ] florence2_results . append ( Florence2OCRResult ( text = label , bounding_box = bounding_box , image_width = image . width , image_height = image . height , ) ) except KeyError : print ( \"Error parsing OCR results, returning empty list\" ) # Clean up to prevent memory leak del inputs del generated_ids del generated_text del parsed_answer gc . collect () if str ( self . device ) . startswith ( \"cuda\" ): import torch torch . cuda . empty_cache () return [ result . to_flat () for result in florence2_results ]","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;recognize"},{"location":"api/engines/#trocrengine","text":"Microsoft's TrOCR transformer-based OCR. Requires the [trocr] extra. pip install \"panoocr[trocr]\"","title":"TrOCREngine"},{"location":"api/engines/#panoocr.engines.trocr.TrOCREngine","text":"TrOCREngine ( config : Dict [ str , Any ] | None = None ) OCR engine using Microsoft's TrOCR model. TrOCR is a transformer-based OCR model that excels at single-line text recognition. It does NOT provide bounding boxes - it reads the entire image as a single text line. WARNING: This engine is experimental and may not work well for panorama OCR since it doesn't detect text regions. Consider using Florence2OCREngine or other engines that provide region detection. Attributes: Name Type Description model The TrOCR model. processor The TrOCR processor. Example from panoocr.engines.trocr import TrOCREngine, TrOCRModel engine = TrOCREngine(config={ ... \"model\": TrOCRModel.MICROSOFT_TROCR_LARGE_PRINTED, ... })","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;TrOCREngine"},{"location":"api/engines/#panoocr.engines.trocr.TrOCREngine--note-returns-single-result-for-entire-image","text":"results = engine.recognize(cropped_text_image) Note Install with: pip install \"panoocr[trocr]\" For GPU support, install PyTorch with CUDA. Initialize the TrOCR engine. Parameters: Name Type Description Default config Dict [ str , Any ] | None Configuration dictionary with optional keys: - model: TrOCRModel enum value or model ID string. - device: Device to use (\"cuda\", \"mps\", \"cpu\", or None for auto). None Raises: Type Description ImportError If dependencies are not installed. ValueError If configuration values are invalid. Source code in src/panoocr/engines/trocr.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def __init__ ( self , config : Dict [ str , Any ] | None = None ) -> None : \"\"\"Initialize the TrOCR engine. Args: config: Configuration dictionary with optional keys: - model: TrOCRModel enum value or model ID string. - device: Device to use (\"cuda\", \"mps\", \"cpu\", or None for auto). Raises: ImportError: If dependencies are not installed. ValueError: If configuration values are invalid. \"\"\" # Check dependencies first _check_trocr_dependencies () import torch from transformers import TrOCRProcessor , VisionEncoderDecoderModel config = config or {} # Parse model model = config . get ( \"model\" , DEFAULT_MODEL ) try : model_id = model . value if isinstance ( model , TrOCRModel ) else model except ( KeyError , AttributeError ): raise ValueError ( \"Invalid model specified\" ) # Auto-detect device device = config . get ( \"device\" ) if device is None : if torch . cuda . is_available (): device = \"cuda\" elif torch . backends . mps . is_available (): device = \"mps\" else : device = \"cpu\" self . device = device print ( f \"Loading TrOCR model { model_id } on { device } ...\" ) self . processor = TrOCRProcessor . from_pretrained ( model_id ) self . model = VisionEncoderDecoderModel . from_pretrained ( model_id ) . to ( device ) print ( \"TrOCR model loaded successfully.\" )","title":"Note: Returns single result for entire image"},{"location":"api/engines/#panoocr.engines.trocr.TrOCREngine.recognize","text":"recognize ( image : Image ) -> List [ FlatOCRResult ] Recognize text in an image. NOTE: TrOCR treats the entire image as a single text line and does not provide bounding boxes. This makes it unsuitable for most panorama OCR use cases. The result will have a bounding box covering the entire image. Parameters: Name Type Description Default image Image Input image as PIL Image. required Returns: Type Description List [ FlatOCRResult ] List with single FlatOCRResult covering the entire image, or empty List [ FlatOCRResult ] list if no text is recognized. Source code in src/panoocr/engines/trocr.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def recognize ( self , image : Image . Image ) -> List [ FlatOCRResult ]: \"\"\"Recognize text in an image. NOTE: TrOCR treats the entire image as a single text line and does not provide bounding boxes. This makes it unsuitable for most panorama OCR use cases. The result will have a bounding box covering the entire image. Args: image: Input image as PIL Image. Returns: List with single FlatOCRResult covering the entire image, or empty list if no text is recognized. \"\"\" import torch # Convert to RGB if needed if image . mode != \"RGB\" : image = image . convert ( \"RGB\" ) pixel_values = self . processor ( images = image , return_tensors = \"pt\" ) . pixel_values pixel_values = pixel_values . to ( self . device ) with torch . no_grad (): generated_ids = self . model . generate ( pixel_values ) generated_text = self . processor . batch_decode ( generated_ids , skip_special_tokens = True )[ 0 ] # TrOCR doesn't provide bounding boxes - return full image bbox if generated_text . strip (): return [ FlatOCRResult ( text = generated_text . strip (), confidence = 1.0 , # TrOCR doesn't provide confidence bounding_box = BoundingBox ( left = 0.0 , top = 0.0 , right = 1.0 , bottom = 1.0 , width = 1.0 , height = 1.0 , ), engine = \"TROCR\" , ) ] return []","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;recognize"},{"location":"api/engines/#custom-engines","text":"Any class with a compatible recognize() method works: from panoocr import PanoOCR , FlatOCRResult , BoundingBox from PIL import Image class MyEngine : def recognize ( self , image : Image . Image ) -> list [ FlatOCRResult ]: # Return list of FlatOCRResult with normalized bounding boxes (0-1) return [ FlatOCRResult ( text = \"Hello\" , confidence = 0.95 , bounding_box = BoundingBox ( left = 0.1 , top = 0.2 , right = 0.4 , bottom = 0.3 , width = 0.3 , height = 0.1 ), engine = \"my_engine\" , ) ] pano = PanoOCR ( engine = MyEngine ())","title":"Custom Engines"},{"location":"api/geometry/","text":"Geometry # Coordinate conversion utilities for spherical geometry. UV to Spherical (Camera-Relative) # uv_to_yaw_pitch # uv_to_yaw_pitch ( u : float , v : float , horizontal_fov : float , vertical_fov : float ) -> Tuple [ float , float ] Convert UV coordinates to yaw and pitch angles. Converts normalized image coordinates (0-1 range) to spherical coordinates using the camera field of view parameters. Parameters: Name Type Description Default u float Horizontal coordinate (0-1, left to right). required v float Vertical coordinate (0-1, top to bottom). required horizontal_fov float Horizontal field of view in degrees. required vertical_fov float Vertical field of view in degrees. required Returns: Type Description float Tuple of (yaw, pitch) in degrees. float yaw: Horizontal angle (-fov/2 to fov/2) Tuple [ float , float ] pitch: Vertical angle (-fov/2 to fov/2) Raises: Type Description ValueError If FOV values are not positive. Source code in src/panoocr/geometry.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def uv_to_yaw_pitch ( u : float , v : float , horizontal_fov : float , vertical_fov : float , ) -> Tuple [ float , float ]: \"\"\"Convert UV coordinates to yaw and pitch angles. Converts normalized image coordinates (0-1 range) to spherical coordinates using the camera field of view parameters. Args: u: Horizontal coordinate (0-1, left to right). v: Vertical coordinate (0-1, top to bottom). horizontal_fov: Horizontal field of view in degrees. vertical_fov: Vertical field of view in degrees. Returns: Tuple of (yaw, pitch) in degrees. - yaw: Horizontal angle (-fov/2 to fov/2) - pitch: Vertical angle (-fov/2 to fov/2) Raises: ValueError: If FOV values are not positive. \"\"\" if horizontal_fov <= 0 or vertical_fov <= 0 : raise ValueError ( \"FOV must be positive\" ) # Translate origin to center of image u_centered = u - 0.5 v_centered = 0.5 - v # Flip vertical axis # Convert to angles using perspective projection yaw = math . atan2 ( 2 * u_centered * math . tan ( math . radians ( horizontal_fov ) / 2 ), 1 ) pitch = math . atan2 ( 2 * v_centered * math . tan ( math . radians ( vertical_fov ) / 2 ), 1 ) return math . degrees ( yaw ), math . degrees ( pitch ) yaw_pitch_to_uv # yaw_pitch_to_uv ( yaw : float , pitch : float , horizontal_fov : float , vertical_fov : float ) -> Tuple [ float , float ] Convert yaw and pitch angles to UV coordinates. Converts spherical coordinates to normalized image coordinates using the camera field of view parameters. Parameters: Name Type Description Default yaw float Horizontal angle in degrees. required pitch float Vertical angle in degrees. required horizontal_fov float Horizontal field of view in degrees. required vertical_fov float Vertical field of view in degrees. required Returns: Type Description float Tuple of (u, v) in 0-1 range. float u: Horizontal coordinate (0 = left, 1 = right) Tuple [ float , float ] v: Vertical coordinate (0 = top, 1 = bottom) Raises: Type Description ValueError If FOV values are not positive. Source code in src/panoocr/geometry.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def yaw_pitch_to_uv ( yaw : float , pitch : float , horizontal_fov : float , vertical_fov : float , ) -> Tuple [ float , float ]: \"\"\"Convert yaw and pitch angles to UV coordinates. Converts spherical coordinates to normalized image coordinates using the camera field of view parameters. Args: yaw: Horizontal angle in degrees. pitch: Vertical angle in degrees. horizontal_fov: Horizontal field of view in degrees. vertical_fov: Vertical field of view in degrees. Returns: Tuple of (u, v) in 0-1 range. - u: Horizontal coordinate (0 = left, 1 = right) - v: Vertical coordinate (0 = top, 1 = bottom) Raises: ValueError: If FOV values are not positive. \"\"\" if horizontal_fov <= 0 or vertical_fov <= 0 : raise ValueError ( \"FOV must be positive\" ) # Convert angles to centered coordinates u_centered = math . tan ( math . radians ( yaw )) / ( 2 * math . tan ( math . radians ( horizontal_fov ) / 2 ) ) v_centered = math . tan ( math . radians ( pitch )) / ( 2 * math . tan ( math . radians ( vertical_fov ) / 2 ) ) # Translate back to image coordinates u = u_centered + 0.5 v = 0.5 - v_centered # Flip vertical axis return u , v Perspective to Sphere (World Coordinates) # perspective_to_sphere # perspective_to_sphere ( u : float , v : float , horizontal_fov : float , vertical_fov : float , yaw_offset : float , pitch_offset : float ) -> Tuple [ float , float ] Convert perspective image coordinates to spherical coordinates. Uses proper 3D rotation to handle camera orientation correctly. This is the inverse of py360convert's e2p transformation. Parameters: Name Type Description Default u float Horizontal coordinate (0-1, left to right). required v float Vertical coordinate (0-1, top to bottom). required horizontal_fov float Horizontal field of view in degrees. required vertical_fov float Vertical field of view in degrees. required yaw_offset float Camera yaw (horizontal rotation) in degrees. required pitch_offset float Camera pitch (vertical rotation) in degrees. required Returns: Type Description Tuple [ float , float ] Tuple of (yaw, pitch) in degrees representing world spherical coordinates. Raises: Type Description ValueError If FOV values are not positive. Source code in src/panoocr/geometry.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def perspective_to_sphere ( u : float , v : float , horizontal_fov : float , vertical_fov : float , yaw_offset : float , pitch_offset : float , ) -> Tuple [ float , float ]: \"\"\"Convert perspective image coordinates to spherical coordinates. Uses proper 3D rotation to handle camera orientation correctly. This is the inverse of py360convert's e2p transformation. Args: u: Horizontal coordinate (0-1, left to right). v: Vertical coordinate (0-1, top to bottom). horizontal_fov: Horizontal field of view in degrees. vertical_fov: Vertical field of view in degrees. yaw_offset: Camera yaw (horizontal rotation) in degrees. pitch_offset: Camera pitch (vertical rotation) in degrees. Returns: Tuple of (yaw, pitch) in degrees representing world spherical coordinates. Raises: ValueError: If FOV values are not positive. \"\"\" if horizontal_fov <= 0 or vertical_fov <= 0 : raise ValueError ( \"FOV must be positive\" ) # Convert to centered coordinates (-0.5 to 0.5) # x: positive = right, y: positive = up x = u - 0.5 y = 0.5 - v half_h_fov = math . radians ( horizontal_fov ) / 2 half_v_fov = math . radians ( vertical_fov ) / 2 # Direction in camera local coordinates (camera looks along +Z) X_local = x * 2 * math . tan ( half_h_fov ) Y_local = y * 2 * math . tan ( half_v_fov ) Z_local = 1.0 # Normalize to unit vector r = math . sqrt ( X_local ** 2 + Y_local ** 2 + Z_local ** 2 ) X_local /= r Y_local /= r Z_local /= r # Rotate by camera orientation to get world coordinates pitch_rad = math . radians ( pitch_offset ) yaw_rad = math . radians ( yaw_offset ) cos_pitch = math . cos ( pitch_rad ) sin_pitch = math . sin ( pitch_rad ) cos_yaw = math . cos ( yaw_rad ) sin_yaw = math . sin ( yaw_rad ) # Rotation by pitch (around X axis) X_pitched = X_local Y_pitched = Y_local * cos_pitch + Z_local * sin_pitch Z_pitched = - Y_local * sin_pitch + Z_local * cos_pitch # Rotation by yaw (around Y axis) X_world = X_pitched * cos_yaw + Z_pitched * sin_yaw Y_world = Y_pitched Z_world = - X_pitched * sin_yaw + Z_pitched * cos_yaw world_yaw = math . degrees ( math . atan2 ( X_world , Z_world )) world_pitch = math . degrees ( math . asin ( np . clip ( Y_world , - 1.0 , 1.0 ))) return world_yaw , world_pitch sphere_to_perspective # sphere_to_perspective ( yaw : float , pitch : float , horizontal_fov : float , vertical_fov : float , yaw_offset : float , pitch_offset : float ) -> Tuple [ float , float ] | None Convert spherical coordinates to perspective image coordinates. Uses proper 3D rotation to handle camera orientation correctly. This is the forward transformation matching py360convert's e2p. Parameters: Name Type Description Default yaw float World yaw angle in degrees. required pitch float World pitch angle in degrees. required horizontal_fov float Horizontal field of view in degrees. required vertical_fov float Vertical field of view in degrees. required yaw_offset float Camera yaw (horizontal rotation) in degrees. required pitch_offset float Camera pitch (vertical rotation) in degrees. required Returns: Type Description Tuple [ float , float ] | None Tuple of (u, v) in 0-1 range if the point is within the FOV, Tuple [ float , float ] | None None if the point is outside the perspective view. Raises: Type Description ValueError If FOV values are not positive. Source code in src/panoocr/geometry.py 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def sphere_to_perspective ( yaw : float , pitch : float , horizontal_fov : float , vertical_fov : float , yaw_offset : float , pitch_offset : float , ) -> Tuple [ float , float ] | None : \"\"\"Convert spherical coordinates to perspective image coordinates. Uses proper 3D rotation to handle camera orientation correctly. This is the forward transformation matching py360convert's e2p. Args: yaw: World yaw angle in degrees. pitch: World pitch angle in degrees. horizontal_fov: Horizontal field of view in degrees. vertical_fov: Vertical field of view in degrees. yaw_offset: Camera yaw (horizontal rotation) in degrees. pitch_offset: Camera pitch (vertical rotation) in degrees. Returns: Tuple of (u, v) in 0-1 range if the point is within the FOV, None if the point is outside the perspective view. Raises: ValueError: If FOV values are not positive. \"\"\" if horizontal_fov <= 0 or vertical_fov <= 0 : raise ValueError ( \"FOV must be positive\" ) # Convert world spherical to 3D Cartesian yaw_rad = math . radians ( yaw ) pitch_rad = math . radians ( pitch ) X_world = math . cos ( pitch_rad ) * math . sin ( yaw_rad ) Y_world = math . sin ( pitch_rad ) Z_world = math . cos ( pitch_rad ) * math . cos ( yaw_rad ) # Apply inverse camera rotation to get local coordinates pitch_offset_rad = math . radians ( pitch_offset ) yaw_offset_rad = math . radians ( yaw_offset ) cos_pitch = math . cos ( pitch_offset_rad ) sin_pitch = math . sin ( pitch_offset_rad ) cos_yaw = math . cos ( yaw_offset_rad ) sin_yaw = math . sin ( yaw_offset_rad ) # Inverse yaw rotation (around Y axis) X_yawed = X_world * cos_yaw - Z_world * sin_yaw Y_yawed = Y_world Z_yawed = X_world * sin_yaw + Z_world * cos_yaw # Inverse pitch rotation (around X axis) X_local = X_yawed Y_local = Y_yawed * cos_pitch - Z_yawed * sin_pitch Z_local = Y_yawed * sin_pitch + Z_yawed * cos_pitch # Check if point is in front of camera if Z_local <= 0 : return None # Project to image plane half_h_fov = math . radians ( horizontal_fov ) / 2 half_v_fov = math . radians ( vertical_fov ) / 2 x = X_local / ( Z_local * 2 * math . tan ( half_h_fov )) y = Y_local / ( Z_local * 2 * math . tan ( half_v_fov )) # Check if within FOV bounds if abs ( x ) > 0.5 or abs ( y ) > 0.5 : return None # Convert to UV coordinates u = x + 0.5 v = 0.5 - y return u , v Spherical Centroid # calculate_spherical_centroid # calculate_spherical_centroid ( polygons : List [ List [ Tuple [ float , float ]]]) -> Tuple [ float , float ] Calculate the centroid of spherical polygon(s) using 3D averaging. This handles wrap-around at \u00b1180\u00b0 correctly by converting to 3D Cartesian coordinates, averaging in 3D space, and converting back. Parameters: Name Type Description Default polygons List [ List [ Tuple [ float , float ]]] List of polygons, each polygon is a list of (yaw, pitch) tuples in degrees. required Returns: Type Description Tuple [ float , float ] Tuple of (center_yaw, center_pitch) in degrees. Source code in src/panoocr/geometry.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 def calculate_spherical_centroid ( polygons : List [ List [ Tuple [ float , float ]]], ) -> Tuple [ float , float ]: \"\"\"Calculate the centroid of spherical polygon(s) using 3D averaging. This handles wrap-around at \u00b1180\u00b0 correctly by converting to 3D Cartesian coordinates, averaging in 3D space, and converting back. Args: polygons: List of polygons, each polygon is a list of (yaw, pitch) tuples in degrees. Returns: Tuple of (center_yaw, center_pitch) in degrees. \"\"\" all_points = [ pt for polygon in polygons for pt in polygon ] if not all_points : return 0.0 , 0.0 sum_x , sum_y , sum_z = 0.0 , 0.0 , 0.0 for yaw_deg , pitch_deg in all_points : yaw_rad = math . radians ( yaw_deg ) pitch_rad = math . radians ( pitch_deg ) # Spherical to Cartesian (pitch = latitude, yaw = longitude) # x = cos(pitch) * sin(yaw) [East direction] # y = sin(pitch) [Up direction] # z = cos(pitch) * cos(yaw) [North direction] x = math . cos ( pitch_rad ) * math . sin ( yaw_rad ) y = math . sin ( pitch_rad ) z = math . cos ( pitch_rad ) * math . cos ( yaw_rad ) sum_x += x sum_y += y sum_z += z n = len ( all_points ) avg_x = sum_x / n avg_y = sum_y / n avg_z = sum_z / n magnitude = math . sqrt ( avg_x ** 2 + avg_y ** 2 + avg_z ** 2 ) if magnitude < 1e-10 : # Degenerate case: points are symmetrically distributed. center_yaw = sum ( p [ 0 ] for p in all_points ) / n center_pitch = sum ( p [ 1 ] for p in all_points ) / n return center_yaw , center_pitch avg_x /= magnitude avg_y /= magnitude avg_z /= magnitude center_yaw = math . degrees ( math . atan2 ( avg_x , avg_z )) center_pitch = math . degrees ( math . asin ( max ( - 1.0 , min ( 1.0 , avg_y )))) return center_yaw , center_pitch Yaw Normalization # normalize_yaw # normalize_yaw ( yaw : float ) -> float Normalize yaw angle to -180 to 180 range. Parameters: Name Type Description Default yaw float Yaw angle in degrees. required Returns: Type Description float Normalized yaw in -180 to 180 range. Source code in src/panoocr/geometry.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def normalize_yaw ( yaw : float ) -> float : \"\"\"Normalize yaw angle to -180 to 180 range. Args: yaw: Yaw angle in degrees. Returns: Normalized yaw in -180 to 180 range. \"\"\" while yaw > 180 : yaw -= 360 while yaw < - 180 : yaw += 360 return yaw Equirectangular Conversion # yaw_to_equirectangular_x # yaw_to_equirectangular_x ( yaw : float , image_width : int ) -> float Convert yaw angle to equirectangular image x coordinate. Parameters: Name Type Description Default yaw float Yaw angle in degrees (-180 to 180). required image_width int Width of the equirectangular image. required Returns: Type Description float X coordinate in pixels. Source code in src/panoocr/geometry.py 117 118 119 120 121 122 123 124 125 126 127 128 def yaw_to_equirectangular_x ( yaw : float , image_width : int ) -> float : \"\"\"Convert yaw angle to equirectangular image x coordinate. Args: yaw: Yaw angle in degrees (-180 to 180). image_width: Width of the equirectangular image. Returns: X coordinate in pixels. \"\"\" normalized_yaw = normalize_yaw ( yaw ) return ( normalized_yaw + 180 ) / 360 * image_width pitch_to_equirectangular_y # pitch_to_equirectangular_y ( pitch : float , image_height : int ) -> float Convert pitch angle to equirectangular image y coordinate. Parameters: Name Type Description Default pitch float Pitch angle in degrees (-90 to 90). required image_height int Height of the equirectangular image. required Returns: Type Description float Y coordinate in pixels. Source code in src/panoocr/geometry.py 131 132 133 134 135 136 137 138 139 140 141 142 143 def pitch_to_equirectangular_y ( pitch : float , image_height : int ) -> float : \"\"\"Convert pitch angle to equirectangular image y coordinate. Args: pitch: Pitch angle in degrees (-90 to 90). image_height: Height of the equirectangular image. Returns: Y coordinate in pixels. \"\"\" # Clamp pitch to valid range pitch = max ( - 90 , min ( 90 , pitch )) return ( 90 - pitch ) / 180 * image_height equirectangular_x_to_yaw # equirectangular_x_to_yaw ( x : float , image_width : int ) -> float Convert equirectangular image x coordinate to yaw angle. Parameters: Name Type Description Default x float X coordinate in pixels. required image_width int Width of the equirectangular image. required Returns: Type Description float Yaw angle in degrees (-180 to 180). Source code in src/panoocr/geometry.py 146 147 148 149 150 151 152 153 154 155 156 def equirectangular_x_to_yaw ( x : float , image_width : int ) -> float : \"\"\"Convert equirectangular image x coordinate to yaw angle. Args: x: X coordinate in pixels. image_width: Width of the equirectangular image. Returns: Yaw angle in degrees (-180 to 180). \"\"\" return ( x / image_width ) * 360 - 180 equirectangular_y_to_pitch # equirectangular_y_to_pitch ( y : float , image_height : int ) -> float Convert equirectangular image y coordinate to pitch angle. Parameters: Name Type Description Default y float Y coordinate in pixels. required image_height int Height of the equirectangular image. required Returns: Type Description float Pitch angle in degrees (-90 to 90). Source code in src/panoocr/geometry.py 159 160 161 162 163 164 165 166 167 168 169 def equirectangular_y_to_pitch ( y : float , image_height : int ) -> float : \"\"\"Convert equirectangular image y coordinate to pitch angle. Args: y: Y coordinate in pixels. image_height: Height of the equirectangular image. Returns: Pitch angle in degrees (-90 to 90). \"\"\" return 90 - ( y / image_height ) * 180","title":"Geometry"},{"location":"api/geometry/#geometry","text":"Coordinate conversion utilities for spherical geometry.","title":"Geometry"},{"location":"api/geometry/#uv-to-spherical-camera-relative","text":"","title":"UV to Spherical (Camera-Relative)"},{"location":"api/geometry/#panoocr.geometry.uv_to_yaw_pitch","text":"uv_to_yaw_pitch ( u : float , v : float , horizontal_fov : float , vertical_fov : float ) -> Tuple [ float , float ] Convert UV coordinates to yaw and pitch angles. Converts normalized image coordinates (0-1 range) to spherical coordinates using the camera field of view parameters. Parameters: Name Type Description Default u float Horizontal coordinate (0-1, left to right). required v float Vertical coordinate (0-1, top to bottom). required horizontal_fov float Horizontal field of view in degrees. required vertical_fov float Vertical field of view in degrees. required Returns: Type Description float Tuple of (yaw, pitch) in degrees. float yaw: Horizontal angle (-fov/2 to fov/2) Tuple [ float , float ] pitch: Vertical angle (-fov/2 to fov/2) Raises: Type Description ValueError If FOV values are not positive. Source code in src/panoocr/geometry.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def uv_to_yaw_pitch ( u : float , v : float , horizontal_fov : float , vertical_fov : float , ) -> Tuple [ float , float ]: \"\"\"Convert UV coordinates to yaw and pitch angles. Converts normalized image coordinates (0-1 range) to spherical coordinates using the camera field of view parameters. Args: u: Horizontal coordinate (0-1, left to right). v: Vertical coordinate (0-1, top to bottom). horizontal_fov: Horizontal field of view in degrees. vertical_fov: Vertical field of view in degrees. Returns: Tuple of (yaw, pitch) in degrees. - yaw: Horizontal angle (-fov/2 to fov/2) - pitch: Vertical angle (-fov/2 to fov/2) Raises: ValueError: If FOV values are not positive. \"\"\" if horizontal_fov <= 0 or vertical_fov <= 0 : raise ValueError ( \"FOV must be positive\" ) # Translate origin to center of image u_centered = u - 0.5 v_centered = 0.5 - v # Flip vertical axis # Convert to angles using perspective projection yaw = math . atan2 ( 2 * u_centered * math . tan ( math . radians ( horizontal_fov ) / 2 ), 1 ) pitch = math . atan2 ( 2 * v_centered * math . tan ( math . radians ( vertical_fov ) / 2 ), 1 ) return math . degrees ( yaw ), math . degrees ( pitch )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;uv_to_yaw_pitch"},{"location":"api/geometry/#panoocr.geometry.yaw_pitch_to_uv","text":"yaw_pitch_to_uv ( yaw : float , pitch : float , horizontal_fov : float , vertical_fov : float ) -> Tuple [ float , float ] Convert yaw and pitch angles to UV coordinates. Converts spherical coordinates to normalized image coordinates using the camera field of view parameters. Parameters: Name Type Description Default yaw float Horizontal angle in degrees. required pitch float Vertical angle in degrees. required horizontal_fov float Horizontal field of view in degrees. required vertical_fov float Vertical field of view in degrees. required Returns: Type Description float Tuple of (u, v) in 0-1 range. float u: Horizontal coordinate (0 = left, 1 = right) Tuple [ float , float ] v: Vertical coordinate (0 = top, 1 = bottom) Raises: Type Description ValueError If FOV values are not positive. Source code in src/panoocr/geometry.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def yaw_pitch_to_uv ( yaw : float , pitch : float , horizontal_fov : float , vertical_fov : float , ) -> Tuple [ float , float ]: \"\"\"Convert yaw and pitch angles to UV coordinates. Converts spherical coordinates to normalized image coordinates using the camera field of view parameters. Args: yaw: Horizontal angle in degrees. pitch: Vertical angle in degrees. horizontal_fov: Horizontal field of view in degrees. vertical_fov: Vertical field of view in degrees. Returns: Tuple of (u, v) in 0-1 range. - u: Horizontal coordinate (0 = left, 1 = right) - v: Vertical coordinate (0 = top, 1 = bottom) Raises: ValueError: If FOV values are not positive. \"\"\" if horizontal_fov <= 0 or vertical_fov <= 0 : raise ValueError ( \"FOV must be positive\" ) # Convert angles to centered coordinates u_centered = math . tan ( math . radians ( yaw )) / ( 2 * math . tan ( math . radians ( horizontal_fov ) / 2 ) ) v_centered = math . tan ( math . radians ( pitch )) / ( 2 * math . tan ( math . radians ( vertical_fov ) / 2 ) ) # Translate back to image coordinates u = u_centered + 0.5 v = 0.5 - v_centered # Flip vertical axis return u , v","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;yaw_pitch_to_uv"},{"location":"api/geometry/#perspective-to-sphere-world-coordinates","text":"","title":"Perspective to Sphere (World Coordinates)"},{"location":"api/geometry/#panoocr.geometry.perspective_to_sphere","text":"perspective_to_sphere ( u : float , v : float , horizontal_fov : float , vertical_fov : float , yaw_offset : float , pitch_offset : float ) -> Tuple [ float , float ] Convert perspective image coordinates to spherical coordinates. Uses proper 3D rotation to handle camera orientation correctly. This is the inverse of py360convert's e2p transformation. Parameters: Name Type Description Default u float Horizontal coordinate (0-1, left to right). required v float Vertical coordinate (0-1, top to bottom). required horizontal_fov float Horizontal field of view in degrees. required vertical_fov float Vertical field of view in degrees. required yaw_offset float Camera yaw (horizontal rotation) in degrees. required pitch_offset float Camera pitch (vertical rotation) in degrees. required Returns: Type Description Tuple [ float , float ] Tuple of (yaw, pitch) in degrees representing world spherical coordinates. Raises: Type Description ValueError If FOV values are not positive. Source code in src/panoocr/geometry.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def perspective_to_sphere ( u : float , v : float , horizontal_fov : float , vertical_fov : float , yaw_offset : float , pitch_offset : float , ) -> Tuple [ float , float ]: \"\"\"Convert perspective image coordinates to spherical coordinates. Uses proper 3D rotation to handle camera orientation correctly. This is the inverse of py360convert's e2p transformation. Args: u: Horizontal coordinate (0-1, left to right). v: Vertical coordinate (0-1, top to bottom). horizontal_fov: Horizontal field of view in degrees. vertical_fov: Vertical field of view in degrees. yaw_offset: Camera yaw (horizontal rotation) in degrees. pitch_offset: Camera pitch (vertical rotation) in degrees. Returns: Tuple of (yaw, pitch) in degrees representing world spherical coordinates. Raises: ValueError: If FOV values are not positive. \"\"\" if horizontal_fov <= 0 or vertical_fov <= 0 : raise ValueError ( \"FOV must be positive\" ) # Convert to centered coordinates (-0.5 to 0.5) # x: positive = right, y: positive = up x = u - 0.5 y = 0.5 - v half_h_fov = math . radians ( horizontal_fov ) / 2 half_v_fov = math . radians ( vertical_fov ) / 2 # Direction in camera local coordinates (camera looks along +Z) X_local = x * 2 * math . tan ( half_h_fov ) Y_local = y * 2 * math . tan ( half_v_fov ) Z_local = 1.0 # Normalize to unit vector r = math . sqrt ( X_local ** 2 + Y_local ** 2 + Z_local ** 2 ) X_local /= r Y_local /= r Z_local /= r # Rotate by camera orientation to get world coordinates pitch_rad = math . radians ( pitch_offset ) yaw_rad = math . radians ( yaw_offset ) cos_pitch = math . cos ( pitch_rad ) sin_pitch = math . sin ( pitch_rad ) cos_yaw = math . cos ( yaw_rad ) sin_yaw = math . sin ( yaw_rad ) # Rotation by pitch (around X axis) X_pitched = X_local Y_pitched = Y_local * cos_pitch + Z_local * sin_pitch Z_pitched = - Y_local * sin_pitch + Z_local * cos_pitch # Rotation by yaw (around Y axis) X_world = X_pitched * cos_yaw + Z_pitched * sin_yaw Y_world = Y_pitched Z_world = - X_pitched * sin_yaw + Z_pitched * cos_yaw world_yaw = math . degrees ( math . atan2 ( X_world , Z_world )) world_pitch = math . degrees ( math . asin ( np . clip ( Y_world , - 1.0 , 1.0 ))) return world_yaw , world_pitch","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;perspective_to_sphere"},{"location":"api/geometry/#panoocr.geometry.sphere_to_perspective","text":"sphere_to_perspective ( yaw : float , pitch : float , horizontal_fov : float , vertical_fov : float , yaw_offset : float , pitch_offset : float ) -> Tuple [ float , float ] | None Convert spherical coordinates to perspective image coordinates. Uses proper 3D rotation to handle camera orientation correctly. This is the forward transformation matching py360convert's e2p. Parameters: Name Type Description Default yaw float World yaw angle in degrees. required pitch float World pitch angle in degrees. required horizontal_fov float Horizontal field of view in degrees. required vertical_fov float Vertical field of view in degrees. required yaw_offset float Camera yaw (horizontal rotation) in degrees. required pitch_offset float Camera pitch (vertical rotation) in degrees. required Returns: Type Description Tuple [ float , float ] | None Tuple of (u, v) in 0-1 range if the point is within the FOV, Tuple [ float , float ] | None None if the point is outside the perspective view. Raises: Type Description ValueError If FOV values are not positive. Source code in src/panoocr/geometry.py 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def sphere_to_perspective ( yaw : float , pitch : float , horizontal_fov : float , vertical_fov : float , yaw_offset : float , pitch_offset : float , ) -> Tuple [ float , float ] | None : \"\"\"Convert spherical coordinates to perspective image coordinates. Uses proper 3D rotation to handle camera orientation correctly. This is the forward transformation matching py360convert's e2p. Args: yaw: World yaw angle in degrees. pitch: World pitch angle in degrees. horizontal_fov: Horizontal field of view in degrees. vertical_fov: Vertical field of view in degrees. yaw_offset: Camera yaw (horizontal rotation) in degrees. pitch_offset: Camera pitch (vertical rotation) in degrees. Returns: Tuple of (u, v) in 0-1 range if the point is within the FOV, None if the point is outside the perspective view. Raises: ValueError: If FOV values are not positive. \"\"\" if horizontal_fov <= 0 or vertical_fov <= 0 : raise ValueError ( \"FOV must be positive\" ) # Convert world spherical to 3D Cartesian yaw_rad = math . radians ( yaw ) pitch_rad = math . radians ( pitch ) X_world = math . cos ( pitch_rad ) * math . sin ( yaw_rad ) Y_world = math . sin ( pitch_rad ) Z_world = math . cos ( pitch_rad ) * math . cos ( yaw_rad ) # Apply inverse camera rotation to get local coordinates pitch_offset_rad = math . radians ( pitch_offset ) yaw_offset_rad = math . radians ( yaw_offset ) cos_pitch = math . cos ( pitch_offset_rad ) sin_pitch = math . sin ( pitch_offset_rad ) cos_yaw = math . cos ( yaw_offset_rad ) sin_yaw = math . sin ( yaw_offset_rad ) # Inverse yaw rotation (around Y axis) X_yawed = X_world * cos_yaw - Z_world * sin_yaw Y_yawed = Y_world Z_yawed = X_world * sin_yaw + Z_world * cos_yaw # Inverse pitch rotation (around X axis) X_local = X_yawed Y_local = Y_yawed * cos_pitch - Z_yawed * sin_pitch Z_local = Y_yawed * sin_pitch + Z_yawed * cos_pitch # Check if point is in front of camera if Z_local <= 0 : return None # Project to image plane half_h_fov = math . radians ( horizontal_fov ) / 2 half_v_fov = math . radians ( vertical_fov ) / 2 x = X_local / ( Z_local * 2 * math . tan ( half_h_fov )) y = Y_local / ( Z_local * 2 * math . tan ( half_v_fov )) # Check if within FOV bounds if abs ( x ) > 0.5 or abs ( y ) > 0.5 : return None # Convert to UV coordinates u = x + 0.5 v = 0.5 - y return u , v","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;sphere_to_perspective"},{"location":"api/geometry/#spherical-centroid","text":"","title":"Spherical Centroid"},{"location":"api/geometry/#panoocr.geometry.calculate_spherical_centroid","text":"calculate_spherical_centroid ( polygons : List [ List [ Tuple [ float , float ]]]) -> Tuple [ float , float ] Calculate the centroid of spherical polygon(s) using 3D averaging. This handles wrap-around at \u00b1180\u00b0 correctly by converting to 3D Cartesian coordinates, averaging in 3D space, and converting back. Parameters: Name Type Description Default polygons List [ List [ Tuple [ float , float ]]] List of polygons, each polygon is a list of (yaw, pitch) tuples in degrees. required Returns: Type Description Tuple [ float , float ] Tuple of (center_yaw, center_pitch) in degrees. Source code in src/panoocr/geometry.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 def calculate_spherical_centroid ( polygons : List [ List [ Tuple [ float , float ]]], ) -> Tuple [ float , float ]: \"\"\"Calculate the centroid of spherical polygon(s) using 3D averaging. This handles wrap-around at \u00b1180\u00b0 correctly by converting to 3D Cartesian coordinates, averaging in 3D space, and converting back. Args: polygons: List of polygons, each polygon is a list of (yaw, pitch) tuples in degrees. Returns: Tuple of (center_yaw, center_pitch) in degrees. \"\"\" all_points = [ pt for polygon in polygons for pt in polygon ] if not all_points : return 0.0 , 0.0 sum_x , sum_y , sum_z = 0.0 , 0.0 , 0.0 for yaw_deg , pitch_deg in all_points : yaw_rad = math . radians ( yaw_deg ) pitch_rad = math . radians ( pitch_deg ) # Spherical to Cartesian (pitch = latitude, yaw = longitude) # x = cos(pitch) * sin(yaw) [East direction] # y = sin(pitch) [Up direction] # z = cos(pitch) * cos(yaw) [North direction] x = math . cos ( pitch_rad ) * math . sin ( yaw_rad ) y = math . sin ( pitch_rad ) z = math . cos ( pitch_rad ) * math . cos ( yaw_rad ) sum_x += x sum_y += y sum_z += z n = len ( all_points ) avg_x = sum_x / n avg_y = sum_y / n avg_z = sum_z / n magnitude = math . sqrt ( avg_x ** 2 + avg_y ** 2 + avg_z ** 2 ) if magnitude < 1e-10 : # Degenerate case: points are symmetrically distributed. center_yaw = sum ( p [ 0 ] for p in all_points ) / n center_pitch = sum ( p [ 1 ] for p in all_points ) / n return center_yaw , center_pitch avg_x /= magnitude avg_y /= magnitude avg_z /= magnitude center_yaw = math . degrees ( math . atan2 ( avg_x , avg_z )) center_pitch = math . degrees ( math . asin ( max ( - 1.0 , min ( 1.0 , avg_y )))) return center_yaw , center_pitch","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;calculate_spherical_centroid"},{"location":"api/geometry/#yaw-normalization","text":"","title":"Yaw Normalization"},{"location":"api/geometry/#panoocr.geometry.normalize_yaw","text":"normalize_yaw ( yaw : float ) -> float Normalize yaw angle to -180 to 180 range. Parameters: Name Type Description Default yaw float Yaw angle in degrees. required Returns: Type Description float Normalized yaw in -180 to 180 range. Source code in src/panoocr/geometry.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def normalize_yaw ( yaw : float ) -> float : \"\"\"Normalize yaw angle to -180 to 180 range. Args: yaw: Yaw angle in degrees. Returns: Normalized yaw in -180 to 180 range. \"\"\" while yaw > 180 : yaw -= 360 while yaw < - 180 : yaw += 360 return yaw","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;normalize_yaw"},{"location":"api/geometry/#equirectangular-conversion","text":"","title":"Equirectangular Conversion"},{"location":"api/geometry/#panoocr.geometry.yaw_to_equirectangular_x","text":"yaw_to_equirectangular_x ( yaw : float , image_width : int ) -> float Convert yaw angle to equirectangular image x coordinate. Parameters: Name Type Description Default yaw float Yaw angle in degrees (-180 to 180). required image_width int Width of the equirectangular image. required Returns: Type Description float X coordinate in pixels. Source code in src/panoocr/geometry.py 117 118 119 120 121 122 123 124 125 126 127 128 def yaw_to_equirectangular_x ( yaw : float , image_width : int ) -> float : \"\"\"Convert yaw angle to equirectangular image x coordinate. Args: yaw: Yaw angle in degrees (-180 to 180). image_width: Width of the equirectangular image. Returns: X coordinate in pixels. \"\"\" normalized_yaw = normalize_yaw ( yaw ) return ( normalized_yaw + 180 ) / 360 * image_width","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;yaw_to_equirectangular_x"},{"location":"api/geometry/#panoocr.geometry.pitch_to_equirectangular_y","text":"pitch_to_equirectangular_y ( pitch : float , image_height : int ) -> float Convert pitch angle to equirectangular image y coordinate. Parameters: Name Type Description Default pitch float Pitch angle in degrees (-90 to 90). required image_height int Height of the equirectangular image. required Returns: Type Description float Y coordinate in pixels. Source code in src/panoocr/geometry.py 131 132 133 134 135 136 137 138 139 140 141 142 143 def pitch_to_equirectangular_y ( pitch : float , image_height : int ) -> float : \"\"\"Convert pitch angle to equirectangular image y coordinate. Args: pitch: Pitch angle in degrees (-90 to 90). image_height: Height of the equirectangular image. Returns: Y coordinate in pixels. \"\"\" # Clamp pitch to valid range pitch = max ( - 90 , min ( 90 , pitch )) return ( 90 - pitch ) / 180 * image_height","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;pitch_to_equirectangular_y"},{"location":"api/geometry/#panoocr.geometry.equirectangular_x_to_yaw","text":"equirectangular_x_to_yaw ( x : float , image_width : int ) -> float Convert equirectangular image x coordinate to yaw angle. Parameters: Name Type Description Default x float X coordinate in pixels. required image_width int Width of the equirectangular image. required Returns: Type Description float Yaw angle in degrees (-180 to 180). Source code in src/panoocr/geometry.py 146 147 148 149 150 151 152 153 154 155 156 def equirectangular_x_to_yaw ( x : float , image_width : int ) -> float : \"\"\"Convert equirectangular image x coordinate to yaw angle. Args: x: X coordinate in pixels. image_width: Width of the equirectangular image. Returns: Yaw angle in degrees (-180 to 180). \"\"\" return ( x / image_width ) * 360 - 180","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;equirectangular_x_to_yaw"},{"location":"api/geometry/#panoocr.geometry.equirectangular_y_to_pitch","text":"equirectangular_y_to_pitch ( y : float , image_height : int ) -> float Convert equirectangular image y coordinate to pitch angle. Parameters: Name Type Description Default y float Y coordinate in pixels. required image_height int Height of the equirectangular image. required Returns: Type Description float Pitch angle in degrees (-90 to 90). Source code in src/panoocr/geometry.py 159 160 161 162 163 164 165 166 167 168 169 def equirectangular_y_to_pitch ( y : float , image_height : int ) -> float : \"\"\"Convert equirectangular image y coordinate to pitch angle. Args: y: Y coordinate in pixels. image_height: Height of the equirectangular image. Returns: Pitch angle in degrees (-90 to 90). \"\"\" return 90 - ( y / image_height ) * 180","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;equirectangular_y_to_pitch"},{"location":"api/image/","text":"Image # Classes for equirectangular panoramas and perspective projections. PanoramaImage # PanoramaImage # PanoramaImage ( panorama_id : str , image : Union [ str , Image , ndarray ]) An equirectangular panorama image. Attributes: Name Type Description panorama_id Identifier for this panorama. loaded_image The panorama as PIL Image. loaded_image_array The panorama as numpy array. Initialize a panorama image. Parameters: Name Type Description Default panorama_id str Identifier for this panorama. required image Union [ str , Image , ndarray ] Path to image file, PIL Image, or numpy array. required Raises: Type Description ValueError If image type is not supported. Source code in src/panoocr/image/models.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def __init__ ( self , panorama_id : str , image : Union [ str , Image . Image , np . ndarray ], ): \"\"\"Initialize a panorama image. Args: panorama_id: Identifier for this panorama. image: Path to image file, PIL Image, or numpy array. Raises: ValueError: If image type is not supported. \"\"\" self . panorama_id = panorama_id if isinstance ( image , str ): self . loaded_image = Image . open ( image ) self . loaded_image_array = np . array ( self . loaded_image ) elif isinstance ( image , Image . Image ): self . loaded_image = image self . loaded_image_array = np . array ( self . loaded_image ) elif isinstance ( image , np . ndarray ): self . loaded_image_array = image self . loaded_image = Image . fromarray ( self . loaded_image_array ) else : raise ValueError ( \"Input image must be a path string, PIL Image, or numpy array\" ) generate_perspective_image # generate_perspective_image ( perspective : PerspectiveMetadata ) -> PerspectiveImage Generate a perspective projection from this panorama. Parameters: Name Type Description Default perspective PerspectiveMetadata Configuration for the perspective projection. required Returns: Type Description PerspectiveImage PerspectiveImage with the generated projection. Raises: Type Description ValueError If the panorama image has not been loaded. Source code in src/panoocr/image/models.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def generate_perspective_image ( self , perspective : PerspectiveMetadata ) -> PerspectiveImage : \"\"\"Generate a perspective projection from this panorama. Args: perspective: Configuration for the perspective projection. Returns: PerspectiveImage with the generated projection. Raises: ValueError: If the panorama image has not been loaded. \"\"\" if self . loaded_image is None : raise ValueError ( \"Image has not been loaded\" ) perspective_image = PerspectiveImage ( source_panorama_image_array = self . loaded_image_array , panorama_id = self . panorama_id , perspective_metadata = perspective , ) return perspective_image get_image # get_image () -> Image . Image Get the panorama as PIL Image. Source code in src/panoocr/image/models.py 197 198 199 def get_image ( self ) -> Image . Image : \"\"\"Get the panorama as PIL Image.\"\"\" return self . loaded_image get_image_array # get_image_array () -> np . ndarray Get the panorama as numpy array. Source code in src/panoocr/image/models.py 201 202 203 def get_image_array ( self ) -> np . ndarray : \"\"\"Get the panorama as numpy array.\"\"\" return self . loaded_image_array PerspectiveImage # PerspectiveImage # PerspectiveImage ( panorama_id : str , source_panorama_image_array : ndarray , perspective_metadata : PerspectiveMetadata ) A perspective projection from an equirectangular panorama. Attributes: Name Type Description panorama_id Identifier for the source panorama. perspective_metadata Configuration for the perspective projection. perspective_image The generated perspective image as PIL Image. perspective_image_array The generated perspective image as numpy array. Initialize a perspective image from a panorama. Parameters: Name Type Description Default panorama_id str Identifier for the source panorama. required source_panorama_image_array ndarray The equirectangular panorama as numpy array. required perspective_metadata PerspectiveMetadata Configuration for the perspective projection. required Source code in src/panoocr/image/models.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def __init__ ( self , panorama_id : str , source_panorama_image_array : np . ndarray , perspective_metadata : PerspectiveMetadata , ): \"\"\"Initialize a perspective image from a panorama. Args: panorama_id: Identifier for the source panorama. source_panorama_image_array: The equirectangular panorama as numpy array. perspective_metadata: Configuration for the perspective projection. \"\"\" self . source_panorama_image_array = source_panorama_image_array self . panorama_id = panorama_id self . perspective_metadata = perspective_metadata # Generate perspective projection using py360convert self . perspective_image_array = py360convert . e2p ( e_img = self . source_panorama_image_array , fov_deg = ( self . perspective_metadata . horizontal_fov , self . perspective_metadata . vertical_fov , ), u_deg = self . perspective_metadata . yaw_offset , v_deg = self . perspective_metadata . pitch_offset , out_hw = ( self . perspective_metadata . pixel_height , self . perspective_metadata . pixel_width , ), in_rot_deg = 0 , mode = \"bilinear\" , ) self . perspective_image = Image . fromarray ( self . perspective_image_array ) get_perspective_metadata # get_perspective_metadata () -> PerspectiveMetadata Get the perspective metadata. Source code in src/panoocr/image/models.py 121 122 123 def get_perspective_metadata ( self ) -> PerspectiveMetadata : \"\"\"Get the perspective metadata.\"\"\" return self . perspective_metadata get_perspective_image_array # get_perspective_image_array () -> np . ndarray Get the perspective image as numpy array. Source code in src/panoocr/image/models.py 125 126 127 def get_perspective_image_array ( self ) -> np . ndarray : \"\"\"Get the perspective image as numpy array.\"\"\" return self . perspective_image_array get_perspective_image # get_perspective_image () -> Image . Image Get the perspective image as PIL Image. Source code in src/panoocr/image/models.py 129 130 131 def get_perspective_image ( self ) -> Image . Image : \"\"\"Get the perspective image as PIL Image.\"\"\" return self . perspective_image PerspectiveMetadata # PerspectiveMetadata dataclass # PerspectiveMetadata ( pixel_width : int , pixel_height : int , horizontal_fov : float , vertical_fov : float , yaw_offset : float , pitch_offset : float ) Metadata for a perspective projection from an equirectangular panorama. Attributes: Name Type Description pixel_width int Width of the perspective image in pixels. pixel_height int Height of the perspective image in pixels. horizontal_fov float Horizontal field of view in degrees. vertical_fov float Vertical field of view in degrees. yaw_offset float Horizontal rotation offset in degrees (-180 to 180). pitch_offset float Vertical rotation offset in degrees (-90 to 90). to_file_suffix # to_file_suffix () -> str Generate a unique file suffix for this perspective configuration. Source code in src/panoocr/image/models.py 43 44 45 46 47 48 49 def to_file_suffix ( self ) -> str : \"\"\"Generate a unique file suffix for this perspective configuration.\"\"\" return ( f \" { self . pixel_width } _ { self . pixel_height } _\" f \" { self . horizontal_fov } _ { self . vertical_fov } _\" f \" { self . yaw_offset } _ { self . pitch_offset } \" ) to_dict # to_dict () -> dict Convert to dictionary. Source code in src/panoocr/image/models.py 51 52 53 54 55 56 57 58 59 60 def to_dict ( self ) -> dict : \"\"\"Convert to dictionary.\"\"\" return { \"pixel_width\" : self . pixel_width , \"pixel_height\" : self . pixel_height , \"horizontal_fov\" : self . horizontal_fov , \"vertical_fov\" : self . vertical_fov , \"yaw_offset\" : self . yaw_offset , \"pitch_offset\" : self . pitch_offset , } from_dict classmethod # from_dict ( data : dict ) -> 'PerspectiveMetadata' Create from dictionary. Source code in src/panoocr/image/models.py 62 63 64 65 66 67 68 69 70 71 72 @classmethod def from_dict ( cls , data : dict ) -> \"PerspectiveMetadata\" : \"\"\"Create from dictionary.\"\"\" return cls ( pixel_width = data [ \"pixel_width\" ], pixel_height = data [ \"pixel_height\" ], horizontal_fov = data [ \"horizontal_fov\" ], vertical_fov = data [ \"vertical_fov\" ], yaw_offset = data [ \"yaw_offset\" ], pitch_offset = data [ \"pitch_offset\" ], ) Perspective Generation # generate_perspectives # generate_perspectives ( fov : float = 45 , resolution : int = 2048 , overlap : float = 0.5 , pitch_angles : Optional [ List [ float ]] = None , vertical_fov : Optional [ float ] = None ) -> List [ PerspectiveMetadata ] Generate a set of perspective views covering 360\u00b0 horizontally. This is the main API for creating custom perspective configurations. Parameters: Name Type Description Default fov float Horizontal field of view in degrees (default: 45\u00b0). 45 resolution int Pixel width and height of each perspective (default: 2048). 2048 overlap float Overlap ratio between adjacent perspectives, 0-1 (default: 0.5). - 0.0 = no overlap (perspectives touch at edges) - 0.5 = 50% overlap (recommended for good coverage) - 1.0 = 100% overlap (each point covered by 2 perspectives) 0.5 pitch_angles Optional [ List [ float ]] List of pitch angles in degrees (default: [0]). Use multiple values to cover up/down, e.g., [-30, 0, 30]. None vertical_fov Optional [ float ] Vertical field of view in degrees (default: same as fov). None Returns: Type Description List [ PerspectiveMetadata ] List of PerspectiveMetadata objects covering the panorama. Examples: >>> # Standard 45\u00b0 FOV with 50% overlap (16 perspectives) >>> perspectives = generate_perspectives ( fov = 45 ) >>> # Wide angle for large text (8 perspectives) >>> perspectives = generate_perspectives ( fov = 90 , resolution = 2500 ) >>> # Zoomed in for small text (32 perspectives) >>> perspectives = generate_perspectives ( fov = 22.5 , resolution = 1024 ) >>> # Cover ceiling and floor too >>> perspectives = generate_perspectives ( fov = 60 , pitch_angles = [ - 45 , 0 , 45 ]) >>> # Dense coverage with 75% overlap >>> perspectives = generate_perspectives ( fov = 45 , overlap = 0.75 ) Source code in src/panoocr/image/perspectives.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def generate_perspectives ( fov : float = 45 , resolution : int = 2048 , overlap : float = 0.5 , pitch_angles : Optional [ List [ float ]] = None , vertical_fov : Optional [ float ] = None , ) -> List [ PerspectiveMetadata ]: \"\"\"Generate a set of perspective views covering 360\u00b0 horizontally. This is the main API for creating custom perspective configurations. Args: fov: Horizontal field of view in degrees (default: 45\u00b0). resolution: Pixel width and height of each perspective (default: 2048). overlap: Overlap ratio between adjacent perspectives, 0-1 (default: 0.5). - 0.0 = no overlap (perspectives touch at edges) - 0.5 = 50% overlap (recommended for good coverage) - 1.0 = 100% overlap (each point covered by 2 perspectives) pitch_angles: List of pitch angles in degrees (default: [0]). Use multiple values to cover up/down, e.g., [-30, 0, 30]. vertical_fov: Vertical field of view in degrees (default: same as fov). Returns: List of PerspectiveMetadata objects covering the panorama. Examples: >>> # Standard 45\u00b0 FOV with 50% overlap (16 perspectives) >>> perspectives = generate_perspectives(fov=45) >>> # Wide angle for large text (8 perspectives) >>> perspectives = generate_perspectives(fov=90, resolution=2500) >>> # Zoomed in for small text (32 perspectives) >>> perspectives = generate_perspectives(fov=22.5, resolution=1024) >>> # Cover ceiling and floor too >>> perspectives = generate_perspectives(fov=60, pitch_angles=[-45, 0, 45]) >>> # Dense coverage with 75% overlap >>> perspectives = generate_perspectives(fov=45, overlap=0.75) \"\"\" if pitch_angles is None : pitch_angles = [ 0 ] if vertical_fov is None : vertical_fov = fov # Calculate yaw interval based on FOV and overlap # With 50% overlap, interval = FOV / 2 # With 0% overlap, interval = FOV yaw_interval = fov * ( 1 - overlap ) if yaw_interval <= 0 : yaw_interval = fov * 0.1 # Minimum 10% step to avoid infinite loop # Generate yaw angles centered at 0 num_yaw = int ( round ( 360 / yaw_interval )) yaw_angles = [ i * ( 360 / num_yaw ) - 180 for i in range ( num_yaw )] perspectives = [] for yaw in yaw_angles : for pitch in pitch_angles : perspectives . append ( PerspectiveMetadata ( pixel_width = resolution , pixel_height = resolution , horizontal_fov = fov , vertical_fov = vertical_fov , yaw_offset = yaw , pitch_offset = pitch , ) ) return perspectives combine_perspectives # combine_perspectives ( * perspective_lists : List [ PerspectiveMetadata ]) -> List [ PerspectiveMetadata ] Combine multiple perspective lists into a single list. Useful for multi-scale detection at different FOV settings. Parameters: Name Type Description Default *perspective_lists List [ PerspectiveMetadata ] Variable number of perspective lists to combine. () Returns: Type Description List [ PerspectiveMetadata ] Combined list of all perspectives. Source code in src/panoocr/image/perspectives.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def combine_perspectives ( * perspective_lists : List [ PerspectiveMetadata ], ) -> List [ PerspectiveMetadata ]: \"\"\"Combine multiple perspective lists into a single list. Useful for multi-scale detection at different FOV settings. Args: *perspective_lists: Variable number of perspective lists to combine. Returns: Combined list of all perspectives. \"\"\" combined = [] for perspective_list in perspective_lists : combined . extend ( perspective_list ) return combined Presets # Pre-configured perspective sets: from panoocr import ( DEFAULT_IMAGE_PERSPECTIVES , ZOOMED_IN_IMAGE_PERSPECTIVES , ZOOMED_OUT_IMAGE_PERSPECTIVES , WIDEANGLE_IMAGE_PERSPECTIVES , ) DEFAULT_IMAGE_PERSPECTIVES # 16 perspectives, 45\u00b0 FOV, 2048x2048 ZOOMED_IN_IMAGE_PERSPECTIVES # 32 perspectives, 22.5\u00b0 FOV, 1024x1024 ZOOMED_OUT_IMAGE_PERSPECTIVES # 12 perspectives, 60\u00b0 FOV, 2500x2500 WIDEANGLE_IMAGE_PERSPECTIVES # 8 perspectives, 90\u00b0 FOV, 2500x2500","title":"Image"},{"location":"api/image/#image","text":"Classes for equirectangular panoramas and perspective projections.","title":"Image"},{"location":"api/image/#panoramaimage","text":"","title":"PanoramaImage"},{"location":"api/image/#panoocr.image.models.PanoramaImage","text":"PanoramaImage ( panorama_id : str , image : Union [ str , Image , ndarray ]) An equirectangular panorama image. Attributes: Name Type Description panorama_id Identifier for this panorama. loaded_image The panorama as PIL Image. loaded_image_array The panorama as numpy array. Initialize a panorama image. Parameters: Name Type Description Default panorama_id str Identifier for this panorama. required image Union [ str , Image , ndarray ] Path to image file, PIL Image, or numpy array. required Raises: Type Description ValueError If image type is not supported. Source code in src/panoocr/image/models.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def __init__ ( self , panorama_id : str , image : Union [ str , Image . Image , np . ndarray ], ): \"\"\"Initialize a panorama image. Args: panorama_id: Identifier for this panorama. image: Path to image file, PIL Image, or numpy array. Raises: ValueError: If image type is not supported. \"\"\" self . panorama_id = panorama_id if isinstance ( image , str ): self . loaded_image = Image . open ( image ) self . loaded_image_array = np . array ( self . loaded_image ) elif isinstance ( image , Image . Image ): self . loaded_image = image self . loaded_image_array = np . array ( self . loaded_image ) elif isinstance ( image , np . ndarray ): self . loaded_image_array = image self . loaded_image = Image . fromarray ( self . loaded_image_array ) else : raise ValueError ( \"Input image must be a path string, PIL Image, or numpy array\" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;PanoramaImage"},{"location":"api/image/#panoocr.image.models.PanoramaImage.generate_perspective_image","text":"generate_perspective_image ( perspective : PerspectiveMetadata ) -> PerspectiveImage Generate a perspective projection from this panorama. Parameters: Name Type Description Default perspective PerspectiveMetadata Configuration for the perspective projection. required Returns: Type Description PerspectiveImage PerspectiveImage with the generated projection. Raises: Type Description ValueError If the panorama image has not been loaded. Source code in src/panoocr/image/models.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def generate_perspective_image ( self , perspective : PerspectiveMetadata ) -> PerspectiveImage : \"\"\"Generate a perspective projection from this panorama. Args: perspective: Configuration for the perspective projection. Returns: PerspectiveImage with the generated projection. Raises: ValueError: If the panorama image has not been loaded. \"\"\" if self . loaded_image is None : raise ValueError ( \"Image has not been loaded\" ) perspective_image = PerspectiveImage ( source_panorama_image_array = self . loaded_image_array , panorama_id = self . panorama_id , perspective_metadata = perspective , ) return perspective_image","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;generate_perspective_image"},{"location":"api/image/#panoocr.image.models.PanoramaImage.get_image","text":"get_image () -> Image . Image Get the panorama as PIL Image. Source code in src/panoocr/image/models.py 197 198 199 def get_image ( self ) -> Image . Image : \"\"\"Get the panorama as PIL Image.\"\"\" return self . loaded_image","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;get_image"},{"location":"api/image/#panoocr.image.models.PanoramaImage.get_image_array","text":"get_image_array () -> np . ndarray Get the panorama as numpy array. Source code in src/panoocr/image/models.py 201 202 203 def get_image_array ( self ) -> np . ndarray : \"\"\"Get the panorama as numpy array.\"\"\" return self . loaded_image_array","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;get_image_array"},{"location":"api/image/#perspectiveimage","text":"","title":"PerspectiveImage"},{"location":"api/image/#panoocr.image.models.PerspectiveImage","text":"PerspectiveImage ( panorama_id : str , source_panorama_image_array : ndarray , perspective_metadata : PerspectiveMetadata ) A perspective projection from an equirectangular panorama. Attributes: Name Type Description panorama_id Identifier for the source panorama. perspective_metadata Configuration for the perspective projection. perspective_image The generated perspective image as PIL Image. perspective_image_array The generated perspective image as numpy array. Initialize a perspective image from a panorama. Parameters: Name Type Description Default panorama_id str Identifier for the source panorama. required source_panorama_image_array ndarray The equirectangular panorama as numpy array. required perspective_metadata PerspectiveMetadata Configuration for the perspective projection. required Source code in src/panoocr/image/models.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def __init__ ( self , panorama_id : str , source_panorama_image_array : np . ndarray , perspective_metadata : PerspectiveMetadata , ): \"\"\"Initialize a perspective image from a panorama. Args: panorama_id: Identifier for the source panorama. source_panorama_image_array: The equirectangular panorama as numpy array. perspective_metadata: Configuration for the perspective projection. \"\"\" self . source_panorama_image_array = source_panorama_image_array self . panorama_id = panorama_id self . perspective_metadata = perspective_metadata # Generate perspective projection using py360convert self . perspective_image_array = py360convert . e2p ( e_img = self . source_panorama_image_array , fov_deg = ( self . perspective_metadata . horizontal_fov , self . perspective_metadata . vertical_fov , ), u_deg = self . perspective_metadata . yaw_offset , v_deg = self . perspective_metadata . pitch_offset , out_hw = ( self . perspective_metadata . pixel_height , self . perspective_metadata . pixel_width , ), in_rot_deg = 0 , mode = \"bilinear\" , ) self . perspective_image = Image . fromarray ( self . perspective_image_array )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;PerspectiveImage"},{"location":"api/image/#panoocr.image.models.PerspectiveImage.get_perspective_metadata","text":"get_perspective_metadata () -> PerspectiveMetadata Get the perspective metadata. Source code in src/panoocr/image/models.py 121 122 123 def get_perspective_metadata ( self ) -> PerspectiveMetadata : \"\"\"Get the perspective metadata.\"\"\" return self . perspective_metadata","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;get_perspective_metadata"},{"location":"api/image/#panoocr.image.models.PerspectiveImage.get_perspective_image_array","text":"get_perspective_image_array () -> np . ndarray Get the perspective image as numpy array. Source code in src/panoocr/image/models.py 125 126 127 def get_perspective_image_array ( self ) -> np . ndarray : \"\"\"Get the perspective image as numpy array.\"\"\" return self . perspective_image_array","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;get_perspective_image_array"},{"location":"api/image/#panoocr.image.models.PerspectiveImage.get_perspective_image","text":"get_perspective_image () -> Image . Image Get the perspective image as PIL Image. Source code in src/panoocr/image/models.py 129 130 131 def get_perspective_image ( self ) -> Image . Image : \"\"\"Get the perspective image as PIL Image.\"\"\" return self . perspective_image","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;get_perspective_image"},{"location":"api/image/#perspectivemetadata","text":"","title":"PerspectiveMetadata"},{"location":"api/image/#panoocr.image.models.PerspectiveMetadata","text":"PerspectiveMetadata ( pixel_width : int , pixel_height : int , horizontal_fov : float , vertical_fov : float , yaw_offset : float , pitch_offset : float ) Metadata for a perspective projection from an equirectangular panorama. Attributes: Name Type Description pixel_width int Width of the perspective image in pixels. pixel_height int Height of the perspective image in pixels. horizontal_fov float Horizontal field of view in degrees. vertical_fov float Vertical field of view in degrees. yaw_offset float Horizontal rotation offset in degrees (-180 to 180). pitch_offset float Vertical rotation offset in degrees (-90 to 90).","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;PerspectiveMetadata"},{"location":"api/image/#panoocr.image.models.PerspectiveMetadata.to_file_suffix","text":"to_file_suffix () -> str Generate a unique file suffix for this perspective configuration. Source code in src/panoocr/image/models.py 43 44 45 46 47 48 49 def to_file_suffix ( self ) -> str : \"\"\"Generate a unique file suffix for this perspective configuration.\"\"\" return ( f \" { self . pixel_width } _ { self . pixel_height } _\" f \" { self . horizontal_fov } _ { self . vertical_fov } _\" f \" { self . yaw_offset } _ { self . pitch_offset } \" )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;to_file_suffix"},{"location":"api/image/#panoocr.image.models.PerspectiveMetadata.to_dict","text":"to_dict () -> dict Convert to dictionary. Source code in src/panoocr/image/models.py 51 52 53 54 55 56 57 58 59 60 def to_dict ( self ) -> dict : \"\"\"Convert to dictionary.\"\"\" return { \"pixel_width\" : self . pixel_width , \"pixel_height\" : self . pixel_height , \"horizontal_fov\" : self . horizontal_fov , \"vertical_fov\" : self . vertical_fov , \"yaw_offset\" : self . yaw_offset , \"pitch_offset\" : self . pitch_offset , }","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;to_dict"},{"location":"api/image/#panoocr.image.models.PerspectiveMetadata.from_dict","text":"from_dict ( data : dict ) -> 'PerspectiveMetadata' Create from dictionary. Source code in src/panoocr/image/models.py 62 63 64 65 66 67 68 69 70 71 72 @classmethod def from_dict ( cls , data : dict ) -> \"PerspectiveMetadata\" : \"\"\"Create from dictionary.\"\"\" return cls ( pixel_width = data [ \"pixel_width\" ], pixel_height = data [ \"pixel_height\" ], horizontal_fov = data [ \"horizontal_fov\" ], vertical_fov = data [ \"vertical_fov\" ], yaw_offset = data [ \"yaw_offset\" ], pitch_offset = data [ \"pitch_offset\" ], )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;from_dict"},{"location":"api/image/#perspective-generation","text":"","title":"Perspective Generation"},{"location":"api/image/#panoocr.image.perspectives.generate_perspectives","text":"generate_perspectives ( fov : float = 45 , resolution : int = 2048 , overlap : float = 0.5 , pitch_angles : Optional [ List [ float ]] = None , vertical_fov : Optional [ float ] = None ) -> List [ PerspectiveMetadata ] Generate a set of perspective views covering 360\u00b0 horizontally. This is the main API for creating custom perspective configurations. Parameters: Name Type Description Default fov float Horizontal field of view in degrees (default: 45\u00b0). 45 resolution int Pixel width and height of each perspective (default: 2048). 2048 overlap float Overlap ratio between adjacent perspectives, 0-1 (default: 0.5). - 0.0 = no overlap (perspectives touch at edges) - 0.5 = 50% overlap (recommended for good coverage) - 1.0 = 100% overlap (each point covered by 2 perspectives) 0.5 pitch_angles Optional [ List [ float ]] List of pitch angles in degrees (default: [0]). Use multiple values to cover up/down, e.g., [-30, 0, 30]. None vertical_fov Optional [ float ] Vertical field of view in degrees (default: same as fov). None Returns: Type Description List [ PerspectiveMetadata ] List of PerspectiveMetadata objects covering the panorama. Examples: >>> # Standard 45\u00b0 FOV with 50% overlap (16 perspectives) >>> perspectives = generate_perspectives ( fov = 45 ) >>> # Wide angle for large text (8 perspectives) >>> perspectives = generate_perspectives ( fov = 90 , resolution = 2500 ) >>> # Zoomed in for small text (32 perspectives) >>> perspectives = generate_perspectives ( fov = 22.5 , resolution = 1024 ) >>> # Cover ceiling and floor too >>> perspectives = generate_perspectives ( fov = 60 , pitch_angles = [ - 45 , 0 , 45 ]) >>> # Dense coverage with 75% overlap >>> perspectives = generate_perspectives ( fov = 45 , overlap = 0.75 ) Source code in src/panoocr/image/perspectives.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def generate_perspectives ( fov : float = 45 , resolution : int = 2048 , overlap : float = 0.5 , pitch_angles : Optional [ List [ float ]] = None , vertical_fov : Optional [ float ] = None , ) -> List [ PerspectiveMetadata ]: \"\"\"Generate a set of perspective views covering 360\u00b0 horizontally. This is the main API for creating custom perspective configurations. Args: fov: Horizontal field of view in degrees (default: 45\u00b0). resolution: Pixel width and height of each perspective (default: 2048). overlap: Overlap ratio between adjacent perspectives, 0-1 (default: 0.5). - 0.0 = no overlap (perspectives touch at edges) - 0.5 = 50% overlap (recommended for good coverage) - 1.0 = 100% overlap (each point covered by 2 perspectives) pitch_angles: List of pitch angles in degrees (default: [0]). Use multiple values to cover up/down, e.g., [-30, 0, 30]. vertical_fov: Vertical field of view in degrees (default: same as fov). Returns: List of PerspectiveMetadata objects covering the panorama. Examples: >>> # Standard 45\u00b0 FOV with 50% overlap (16 perspectives) >>> perspectives = generate_perspectives(fov=45) >>> # Wide angle for large text (8 perspectives) >>> perspectives = generate_perspectives(fov=90, resolution=2500) >>> # Zoomed in for small text (32 perspectives) >>> perspectives = generate_perspectives(fov=22.5, resolution=1024) >>> # Cover ceiling and floor too >>> perspectives = generate_perspectives(fov=60, pitch_angles=[-45, 0, 45]) >>> # Dense coverage with 75% overlap >>> perspectives = generate_perspectives(fov=45, overlap=0.75) \"\"\" if pitch_angles is None : pitch_angles = [ 0 ] if vertical_fov is None : vertical_fov = fov # Calculate yaw interval based on FOV and overlap # With 50% overlap, interval = FOV / 2 # With 0% overlap, interval = FOV yaw_interval = fov * ( 1 - overlap ) if yaw_interval <= 0 : yaw_interval = fov * 0.1 # Minimum 10% step to avoid infinite loop # Generate yaw angles centered at 0 num_yaw = int ( round ( 360 / yaw_interval )) yaw_angles = [ i * ( 360 / num_yaw ) - 180 for i in range ( num_yaw )] perspectives = [] for yaw in yaw_angles : for pitch in pitch_angles : perspectives . append ( PerspectiveMetadata ( pixel_width = resolution , pixel_height = resolution , horizontal_fov = fov , vertical_fov = vertical_fov , yaw_offset = yaw , pitch_offset = pitch , ) ) return perspectives","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;generate_perspectives"},{"location":"api/image/#panoocr.image.perspectives.combine_perspectives","text":"combine_perspectives ( * perspective_lists : List [ PerspectiveMetadata ]) -> List [ PerspectiveMetadata ] Combine multiple perspective lists into a single list. Useful for multi-scale detection at different FOV settings. Parameters: Name Type Description Default *perspective_lists List [ PerspectiveMetadata ] Variable number of perspective lists to combine. () Returns: Type Description List [ PerspectiveMetadata ] Combined list of all perspectives. Source code in src/panoocr/image/perspectives.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def combine_perspectives ( * perspective_lists : List [ PerspectiveMetadata ], ) -> List [ PerspectiveMetadata ]: \"\"\"Combine multiple perspective lists into a single list. Useful for multi-scale detection at different FOV settings. Args: *perspective_lists: Variable number of perspective lists to combine. Returns: Combined list of all perspectives. \"\"\" combined = [] for perspective_list in perspective_lists : combined . extend ( perspective_list ) return combined","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;combine_perspectives"},{"location":"api/image/#presets","text":"Pre-configured perspective sets: from panoocr import ( DEFAULT_IMAGE_PERSPECTIVES , ZOOMED_IN_IMAGE_PERSPECTIVES , ZOOMED_OUT_IMAGE_PERSPECTIVES , WIDEANGLE_IMAGE_PERSPECTIVES , ) DEFAULT_IMAGE_PERSPECTIVES # 16 perspectives, 45\u00b0 FOV, 2048x2048 ZOOMED_IN_IMAGE_PERSPECTIVES # 32 perspectives, 22.5\u00b0 FOV, 1024x1024 ZOOMED_OUT_IMAGE_PERSPECTIVES # 12 perspectives, 60\u00b0 FOV, 2500x2500 WIDEANGLE_IMAGE_PERSPECTIVES # 8 perspectives, 90\u00b0 FOV, 2500x2500","title":"Presets"},{"location":"api/ocr/","text":"OCR Models # Data classes for OCR results in different coordinate systems. BoundingBox # BoundingBox dataclass # BoundingBox ( left : float , top : float , right : float , bottom : float , width : float , height : float ) Normalized bounding box with coordinates in 0-1 range. Coordinates are relative to image dimensions: - (0, 0) is top-left - (1, 1) is bottom-right Attributes: Name Type Description left float Distance from left edge (0-1). top float Distance from top edge (0-1). right float Distance from left edge to right side (0-1). bottom float Distance from top edge to bottom side (0-1). width float Box width (0-1). height float Box height (0-1). to_dict # to_dict () -> dict Convert to dictionary. Source code in src/panoocr/ocr/models.py 34 35 36 37 38 39 40 41 42 43 def to_dict ( self ) -> dict : \"\"\"Convert to dictionary.\"\"\" return { \"left\" : self . left , \"top\" : self . top , \"right\" : self . right , \"bottom\" : self . bottom , \"width\" : self . width , \"height\" : self . height , } from_dict classmethod # from_dict ( data : dict ) -> 'BoundingBox' Create from dictionary. Source code in src/panoocr/ocr/models.py 45 46 47 48 49 50 51 52 53 54 55 @classmethod def from_dict ( cls , data : dict ) -> \"BoundingBox\" : \"\"\"Create from dictionary.\"\"\" return cls ( left = data [ \"left\" ], top = data [ \"top\" ], right = data [ \"right\" ], bottom = data [ \"bottom\" ], width = data [ \"width\" ], height = data [ \"height\" ], ) FlatOCRResult # OCR result from a flat (perspective) image with normalized bounding box coordinates. FlatOCRResult dataclass # FlatOCRResult ( text : str , confidence : float , bounding_box : BoundingBox , engine : Optional [ str ] = None ) OCR result from a flat (perspective) image. Attributes: Name Type Description text str Recognized text content. confidence float Recognition confidence (0-1). bounding_box BoundingBox Normalized bounding box in image coordinates. engine Optional [ str ] Name of the OCR engine used. text instance-attribute # text : str confidence instance-attribute # confidence : float bounding_box instance-attribute # bounding_box : BoundingBox engine class-attribute instance-attribute # engine : Optional [ str ] = None to_dict # to_dict () -> dict Convert to dictionary. Source code in src/panoocr/ocr/models.py 74 75 76 77 78 79 80 81 def to_dict ( self ) -> dict : \"\"\"Convert to dictionary.\"\"\" return { \"text\" : self . text , \"confidence\" : self . confidence , \"bounding_box\" : self . bounding_box . to_dict (), \"engine\" : self . engine , } from_dict classmethod # from_dict ( data : dict ) -> 'FlatOCRResult' Create from dictionary. Source code in src/panoocr/ocr/models.py 83 84 85 86 87 88 89 90 91 @classmethod def from_dict ( cls , data : dict ) -> \"FlatOCRResult\" : \"\"\"Create from dictionary.\"\"\" return cls ( text = data [ \"text\" ], confidence = data [ \"confidence\" ], bounding_box = BoundingBox . from_dict ( data [ \"bounding_box\" ]), engine = data . get ( \"engine\" ), ) to_sphere # to_sphere ( horizontal_fov : float , vertical_fov : float , yaw_offset : float , pitch_offset : float ) -> 'SphereOCRResult' Convert to spherical OCR result using camera parameters. All parameters are in degrees. Parameters: Name Type Description Default horizontal_fov float Horizontal field of view of the camera. required vertical_fov float Vertical field of view of the camera. required yaw_offset float Horizontal offset of the camera. required pitch_offset float Vertical offset of the camera. required Returns: Type Description 'SphereOCRResult' SphereOCRResult with spherical coordinates. Source code in src/panoocr/ocr/models.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def to_sphere ( self , horizontal_fov : float , vertical_fov : float , yaw_offset : float , pitch_offset : float , ) -> \"SphereOCRResult\" : \"\"\"Convert to spherical OCR result using camera parameters. All parameters are in degrees. Args: horizontal_fov: Horizontal field of view of the camera. vertical_fov: Vertical field of view of the camera. yaw_offset: Horizontal offset of the camera. pitch_offset: Vertical offset of the camera. Returns: SphereOCRResult with spherical coordinates. \"\"\" if horizontal_fov <= 0 or vertical_fov <= 0 : raise ValueError ( \"FOV must be positive\" ) # Calculate center point center_x = ( self . bounding_box . left + self . bounding_box . right ) * 0.5 center_y = ( self . bounding_box . top + self . bounding_box . bottom ) * 0.5 center_yaw , center_pitch = self . _uv_to_yaw_pitch ( horizontal_fov , vertical_fov , center_x , center_y ) # Calculate corners for width/height left_yaw , top_pitch = self . _uv_to_yaw_pitch ( horizontal_fov , vertical_fov , self . bounding_box . left , self . bounding_box . top ) right_yaw , bottom_pitch = self . _uv_to_yaw_pitch ( horizontal_fov , vertical_fov , self . bounding_box . right , self . bounding_box . bottom , ) width = right_yaw - left_yaw height = top_pitch - bottom_pitch return SphereOCRResult ( text = self . text , confidence = self . confidence , yaw = center_yaw + yaw_offset , pitch = center_pitch + pitch_offset , width = width , height = height , engine = self . engine , ) SphereOCRResult # OCR result in spherical (panorama) coordinates. SphereOCRResult dataclass # SphereOCRResult ( text : str , confidence : float , yaw : float , pitch : float , width : float , height : float , engine : Optional [ str ] = None ) OCR result in spherical (panorama) coordinates. Attributes: Name Type Description text str Recognized text content. confidence float Recognition confidence (0-1). yaw float Horizontal angle in degrees (-180 to 180). pitch float Vertical angle in degrees (-90 to 90). width float Angular width in degrees. height float Angular height in degrees. engine Optional [ str ] Name of the OCR engine used. text instance-attribute # text : str confidence instance-attribute # confidence : float yaw instance-attribute # yaw : float pitch instance-attribute # pitch : float width instance-attribute # width : float height instance-attribute # height : float engine class-attribute instance-attribute # engine : Optional [ str ] = None to_dict # to_dict () -> dict Convert to dictionary. Source code in src/panoocr/ocr/models.py 200 201 202 203 204 205 206 207 208 209 210 def to_dict ( self ) -> dict : \"\"\"Convert to dictionary.\"\"\" return { \"text\" : self . text , \"confidence\" : self . confidence , \"yaw\" : self . yaw , \"pitch\" : self . pitch , \"width\" : self . width , \"height\" : self . height , \"engine\" : self . engine , } from_dict classmethod # from_dict ( data : dict ) -> 'SphereOCRResult' Create from dictionary. Source code in src/panoocr/ocr/models.py 212 213 214 215 216 217 218 219 220 221 222 223 @classmethod def from_dict ( cls , data : dict ) -> \"SphereOCRResult\" : \"\"\"Create from dictionary.\"\"\" return cls ( text = data [ \"text\" ], confidence = data [ \"confidence\" ], yaw = data [ \"yaw\" ], pitch = data [ \"pitch\" ], width = data [ \"width\" ], height = data [ \"height\" ], engine = data . get ( \"engine\" ), )","title":"OCR Models"},{"location":"api/ocr/#ocr-models","text":"Data classes for OCR results in different coordinate systems.","title":"OCR Models"},{"location":"api/ocr/#boundingbox","text":"","title":"BoundingBox"},{"location":"api/ocr/#panoocr.ocr.models.BoundingBox","text":"BoundingBox ( left : float , top : float , right : float , bottom : float , width : float , height : float ) Normalized bounding box with coordinates in 0-1 range. Coordinates are relative to image dimensions: - (0, 0) is top-left - (1, 1) is bottom-right Attributes: Name Type Description left float Distance from left edge (0-1). top float Distance from top edge (0-1). right float Distance from left edge to right side (0-1). bottom float Distance from top edge to bottom side (0-1). width float Box width (0-1). height float Box height (0-1).","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;BoundingBox"},{"location":"api/ocr/#panoocr.ocr.models.BoundingBox.to_dict","text":"to_dict () -> dict Convert to dictionary. Source code in src/panoocr/ocr/models.py 34 35 36 37 38 39 40 41 42 43 def to_dict ( self ) -> dict : \"\"\"Convert to dictionary.\"\"\" return { \"left\" : self . left , \"top\" : self . top , \"right\" : self . right , \"bottom\" : self . bottom , \"width\" : self . width , \"height\" : self . height , }","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;to_dict"},{"location":"api/ocr/#panoocr.ocr.models.BoundingBox.from_dict","text":"from_dict ( data : dict ) -> 'BoundingBox' Create from dictionary. Source code in src/panoocr/ocr/models.py 45 46 47 48 49 50 51 52 53 54 55 @classmethod def from_dict ( cls , data : dict ) -> \"BoundingBox\" : \"\"\"Create from dictionary.\"\"\" return cls ( left = data [ \"left\" ], top = data [ \"top\" ], right = data [ \"right\" ], bottom = data [ \"bottom\" ], width = data [ \"width\" ], height = data [ \"height\" ], )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;from_dict"},{"location":"api/ocr/#flatocrresult","text":"OCR result from a flat (perspective) image with normalized bounding box coordinates.","title":"FlatOCRResult"},{"location":"api/ocr/#panoocr.ocr.models.FlatOCRResult","text":"FlatOCRResult ( text : str , confidence : float , bounding_box : BoundingBox , engine : Optional [ str ] = None ) OCR result from a flat (perspective) image. Attributes: Name Type Description text str Recognized text content. confidence float Recognition confidence (0-1). bounding_box BoundingBox Normalized bounding box in image coordinates. engine Optional [ str ] Name of the OCR engine used.","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;FlatOCRResult"},{"location":"api/ocr/#panoocr.ocr.models.FlatOCRResult.text","text":"text : str","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-attribute\"></code>&nbsp;text"},{"location":"api/ocr/#panoocr.ocr.models.FlatOCRResult.confidence","text":"confidence : float","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-attribute\"></code>&nbsp;confidence"},{"location":"api/ocr/#panoocr.ocr.models.FlatOCRResult.bounding_box","text":"bounding_box : BoundingBox","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-attribute\"></code>&nbsp;bounding_box"},{"location":"api/ocr/#panoocr.ocr.models.FlatOCRResult.engine","text":"engine : Optional [ str ] = None","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-attribute\"></code>&nbsp;engine"},{"location":"api/ocr/#panoocr.ocr.models.FlatOCRResult.to_dict","text":"to_dict () -> dict Convert to dictionary. Source code in src/panoocr/ocr/models.py 74 75 76 77 78 79 80 81 def to_dict ( self ) -> dict : \"\"\"Convert to dictionary.\"\"\" return { \"text\" : self . text , \"confidence\" : self . confidence , \"bounding_box\" : self . bounding_box . to_dict (), \"engine\" : self . engine , }","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;to_dict"},{"location":"api/ocr/#panoocr.ocr.models.FlatOCRResult.from_dict","text":"from_dict ( data : dict ) -> 'FlatOCRResult' Create from dictionary. Source code in src/panoocr/ocr/models.py 83 84 85 86 87 88 89 90 91 @classmethod def from_dict ( cls , data : dict ) -> \"FlatOCRResult\" : \"\"\"Create from dictionary.\"\"\" return cls ( text = data [ \"text\" ], confidence = data [ \"confidence\" ], bounding_box = BoundingBox . from_dict ( data [ \"bounding_box\" ]), engine = data . get ( \"engine\" ), )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;from_dict"},{"location":"api/ocr/#panoocr.ocr.models.FlatOCRResult.to_sphere","text":"to_sphere ( horizontal_fov : float , vertical_fov : float , yaw_offset : float , pitch_offset : float ) -> 'SphereOCRResult' Convert to spherical OCR result using camera parameters. All parameters are in degrees. Parameters: Name Type Description Default horizontal_fov float Horizontal field of view of the camera. required vertical_fov float Vertical field of view of the camera. required yaw_offset float Horizontal offset of the camera. required pitch_offset float Vertical offset of the camera. required Returns: Type Description 'SphereOCRResult' SphereOCRResult with spherical coordinates. Source code in src/panoocr/ocr/models.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def to_sphere ( self , horizontal_fov : float , vertical_fov : float , yaw_offset : float , pitch_offset : float , ) -> \"SphereOCRResult\" : \"\"\"Convert to spherical OCR result using camera parameters. All parameters are in degrees. Args: horizontal_fov: Horizontal field of view of the camera. vertical_fov: Vertical field of view of the camera. yaw_offset: Horizontal offset of the camera. pitch_offset: Vertical offset of the camera. Returns: SphereOCRResult with spherical coordinates. \"\"\" if horizontal_fov <= 0 or vertical_fov <= 0 : raise ValueError ( \"FOV must be positive\" ) # Calculate center point center_x = ( self . bounding_box . left + self . bounding_box . right ) * 0.5 center_y = ( self . bounding_box . top + self . bounding_box . bottom ) * 0.5 center_yaw , center_pitch = self . _uv_to_yaw_pitch ( horizontal_fov , vertical_fov , center_x , center_y ) # Calculate corners for width/height left_yaw , top_pitch = self . _uv_to_yaw_pitch ( horizontal_fov , vertical_fov , self . bounding_box . left , self . bounding_box . top ) right_yaw , bottom_pitch = self . _uv_to_yaw_pitch ( horizontal_fov , vertical_fov , self . bounding_box . right , self . bounding_box . bottom , ) width = right_yaw - left_yaw height = top_pitch - bottom_pitch return SphereOCRResult ( text = self . text , confidence = self . confidence , yaw = center_yaw + yaw_offset , pitch = center_pitch + pitch_offset , width = width , height = height , engine = self . engine , )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;to_sphere"},{"location":"api/ocr/#sphereocrresult","text":"OCR result in spherical (panorama) coordinates.","title":"SphereOCRResult"},{"location":"api/ocr/#panoocr.ocr.models.SphereOCRResult","text":"SphereOCRResult ( text : str , confidence : float , yaw : float , pitch : float , width : float , height : float , engine : Optional [ str ] = None ) OCR result in spherical (panorama) coordinates. Attributes: Name Type Description text str Recognized text content. confidence float Recognition confidence (0-1). yaw float Horizontal angle in degrees (-180 to 180). pitch float Vertical angle in degrees (-90 to 90). width float Angular width in degrees. height float Angular height in degrees. engine Optional [ str ] Name of the OCR engine used.","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-class\"></code>&nbsp;SphereOCRResult"},{"location":"api/ocr/#panoocr.ocr.models.SphereOCRResult.text","text":"text : str","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-attribute\"></code>&nbsp;text"},{"location":"api/ocr/#panoocr.ocr.models.SphereOCRResult.confidence","text":"confidence : float","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-attribute\"></code>&nbsp;confidence"},{"location":"api/ocr/#panoocr.ocr.models.SphereOCRResult.yaw","text":"yaw : float","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-attribute\"></code>&nbsp;yaw"},{"location":"api/ocr/#panoocr.ocr.models.SphereOCRResult.pitch","text":"pitch : float","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-attribute\"></code>&nbsp;pitch"},{"location":"api/ocr/#panoocr.ocr.models.SphereOCRResult.width","text":"width : float","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-attribute\"></code>&nbsp;width"},{"location":"api/ocr/#panoocr.ocr.models.SphereOCRResult.height","text":"height : float","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-attribute\"></code>&nbsp;height"},{"location":"api/ocr/#panoocr.ocr.models.SphereOCRResult.engine","text":"engine : Optional [ str ] = None","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-attribute\"></code>&nbsp;engine"},{"location":"api/ocr/#panoocr.ocr.models.SphereOCRResult.to_dict","text":"to_dict () -> dict Convert to dictionary. Source code in src/panoocr/ocr/models.py 200 201 202 203 204 205 206 207 208 209 210 def to_dict ( self ) -> dict : \"\"\"Convert to dictionary.\"\"\" return { \"text\" : self . text , \"confidence\" : self . confidence , \"yaw\" : self . yaw , \"pitch\" : self . pitch , \"width\" : self . width , \"height\" : self . height , \"engine\" : self . engine , }","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;to_dict"},{"location":"api/ocr/#panoocr.ocr.models.SphereOCRResult.from_dict","text":"from_dict ( data : dict ) -> 'SphereOCRResult' Create from dictionary. Source code in src/panoocr/ocr/models.py 212 213 214 215 216 217 218 219 220 221 222 223 @classmethod def from_dict ( cls , data : dict ) -> \"SphereOCRResult\" : \"\"\"Create from dictionary.\"\"\" return cls ( text = data [ \"text\" ], confidence = data [ \"confidence\" ], yaw = data [ \"yaw\" ], pitch = data [ \"pitch\" ], width = data [ \"width\" ], height = data [ \"height\" ], engine = data . get ( \"engine\" ), )","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-method\"></code>&nbsp;from_dict"},{"location":"api/visualization/","text":"Visualization # OCR visualization utilities. Requires the [viz] extra. pip install \"panoocr[viz]\" Functions # visualize_ocr_results # visualize_ocr_results ( image : Image , ocr_results : List [ FlatOCRResult ], font_size : int = 16 , highlight_color : str = 'red' , stroke_width : int = 2 ) -> Image . Image Visualize flat OCR results on an image. Draws bounding boxes and labels on a copy of the image. Parameters: Name Type Description Default image Image The image to visualize OCR results on. required ocr_results List [ FlatOCRResult ] List of FlatOCRResult objects to visualize. required font_size int Font size for labels. 16 highlight_color str Color for boxes and text. 'red' stroke_width int Width of bounding box lines. 2 Returns: Type Description Image Copy of the image with OCR results visualized. Source code in src/panoocr/ocr/utils.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def visualize_ocr_results ( image : Image . Image , ocr_results : List [ FlatOCRResult ], font_size : int = 16 , highlight_color : str = \"red\" , stroke_width : int = 2 , ) -> Image . Image : \"\"\"Visualize flat OCR results on an image. Draws bounding boxes and labels on a copy of the image. Args: image: The image to visualize OCR results on. ocr_results: List of FlatOCRResult objects to visualize. font_size: Font size for labels. highlight_color: Color for boxes and text. stroke_width: Width of bounding box lines. Returns: Copy of the image with OCR results visualized. \"\"\" # Make a copy to avoid modifying the original result_image = image . copy () draw = ImageDraw . Draw ( result_image ) width , height = result_image . size try : font = ImageFont . load_default ( size = font_size ) except TypeError : # Older Pillow versions don't support size parameter font = ImageFont . load_default () for ocr_result in ocr_results : # Draw bounding box bbox = ocr_result . bounding_box draw . rectangle ( [ bbox . left * width , bbox . top * height , bbox . right * width , bbox . bottom * height , ], outline = highlight_color , width = 3 , ) # Draw text label draw . text ( ( bbox . left * width , bbox . top * height - font_size - stroke_width , ), ocr_result . text , fill = highlight_color , stroke_fill = \"white\" , stroke_width = stroke_width , font = font , ) # Draw confidence score draw . text ( ( bbox . left * width , bbox . bottom * height + stroke_width , ), f \" { ocr_result . confidence : .2f } \" , fill = highlight_color , stroke_fill = \"white\" , stroke_width = stroke_width , font = font , ) return result_image visualize_sphere_ocr_results # visualize_sphere_ocr_results ( image : Image , ocr_results : List [ SphereOCRResult ], font_size : int = 16 , highlight_color : str = 'red' , stroke_width : int = 2 , inplace : bool = False ) -> Image . Image Visualize spherical OCR results on an equirectangular image. Projects OCR result labels back onto the panorama image. This is SLOW and should only be used for debugging purposes. Parameters: Name Type Description Default image Image The equirectangular panorama image. required ocr_results List [ SphereOCRResult ] List of SphereOCRResult objects to visualize. required font_size int Font size for labels (unused, size is automatic). 16 highlight_color str Color for boxes and text. 'red' stroke_width int Width of text stroke. 2 inplace bool If True, modify the input image directly (faster). False Returns: Type Description Image Image with OCR results visualized. Raises: Type Description ImportError If visualization dependencies are not installed. Source code in src/panoocr/ocr/utils.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def visualize_sphere_ocr_results ( image : Image . Image , ocr_results : List [ SphereOCRResult ], font_size : int = 16 , highlight_color : str = \"red\" , stroke_width : int = 2 , inplace : bool = False , ) -> Image . Image : \"\"\"Visualize spherical OCR results on an equirectangular image. Projects OCR result labels back onto the panorama image. This is SLOW and should only be used for debugging purposes. Args: image: The equirectangular panorama image. ocr_results: List of SphereOCRResult objects to visualize. font_size: Font size for labels (unused, size is automatic). highlight_color: Color for boxes and text. stroke_width: Width of text stroke. inplace: If True, modify the input image directly (faster). Returns: Image with OCR results visualized. Raises: ImportError: If visualization dependencies are not installed. \"\"\" _check_viz_dependencies () import numpy as np from scipy.ndimage import map_coordinates # Convert image to RGBA for alpha compositing image = image . convert ( \"RGBA\" ) def get_ocr_result_image ( ocr_result : SphereOCRResult ) -> Image . Image : \"\"\"Create an image for a single OCR result.\"\"\" PIXEL_PER_DEGREE = 300 text_image = Image . new ( \"RGBA\" , ( int ( ocr_result . width * PIXEL_PER_DEGREE ), int ( ocr_result . height * PIXEL_PER_DEGREE ), ), ( 255 , 255 , 255 , 0 ), ) draw = ImageDraw . Draw ( text_image ) try : font = ImageFont . load_default ( size = int ( ocr_result . height * PIXEL_PER_DEGREE * 0.2 ) ) except TypeError : font = ImageFont . load_default () # Draw bounding box draw . rectangle ( [ 0 , 0 , text_image . width , text_image . height ], outline = highlight_color , width = 3 , fill = ( 255 , 255 , 255 , 0 ), ) # Draw text draw . text ( ( text_image . width / 2 , text_image . height / 2 ), ocr_result . text , fill = highlight_color , anchor = \"mm\" , stroke_fill = \"white\" , stroke_width = stroke_width , font = font , ) return text_image def place_ocr_result_on_panorama ( panorama_array : np . ndarray , ocr_result : SphereOCRResult ) -> np . ndarray : \"\"\"Project an OCR result onto the panorama.\"\"\" ocr_result_image = np . array ( get_ocr_result_image ( ocr_result )) pano_height , pano_width = panorama_array . shape [: 2 ] ocr_height , ocr_width = ocr_result_image . shape [: 2 ] yaw_rad = np . radians ( - ocr_result . yaw ) pitch_rad = np . radians ( ocr_result . pitch ) width_rad = np . radians ( ocr_result . width ) height_rad = np . radians ( ocr_result . height ) # Create coordinate mappings for the panorama y_pano , x_pano = np . mgrid [ 0 : pano_height , 0 : pano_width ] # Convert panorama coordinates to spherical coordinates lon = ( x_pano / pano_width - 0.5 ) * 2 * np . pi lat = ( 0.5 - y_pano / pano_height ) * np . pi # Calculate 3D coordinates on the unit sphere x = np . cos ( lat ) * np . sin ( lon ) y = np . sin ( lat ) z = np . cos ( lat ) * np . cos ( lon ) # Combine rotation matrices sin_yaw , cos_yaw = np . sin ( yaw_rad ), np . cos ( yaw_rad ) sin_pitch , cos_pitch = np . sin ( pitch_rad ), np . cos ( pitch_rad ) # Apply rotation x_rot = cos_yaw * x + sin_yaw * z y_rot = sin_pitch * sin_yaw * x + cos_pitch * y - sin_pitch * cos_yaw * z z_rot = - cos_pitch * sin_yaw * x + sin_pitch * y + cos_pitch * cos_yaw * z # Project onto the plane epsilon = 1e-8 x_proj = x_rot / ( z_rot + epsilon ) y_proj = y_rot / ( z_rot + epsilon ) # Scale and shift to image coordinates x_img = ( x_proj / np . tan ( width_rad / 2 ) + 1 ) * ocr_width / 2 y_img = ( - y_proj / np . tan ( height_rad / 2 ) + 1 ) * ocr_height / 2 # Create mask for valid coordinates mask = ( ( x_img >= 0 ) & ( x_img < ocr_width ) & ( y_img >= 0 ) & ( y_img < ocr_height ) & ( z_rot > 0 ) ) # Sample from the OCR result image warped_channels = [] for channel in range ( ocr_result_image . shape [ 2 ]): warped_channel = map_coordinates ( ocr_result_image [:, :, channel ], [ y_img , x_img ], order = 1 , mode = \"constant\" , cval = 0 , ) warped_channels . append ( warped_channel ) warped_image = np . stack ( warped_channels , axis =- 1 ) if not inplace : result = panorama_array . copy () else : result = panorama_array # Apply alpha compositing alpha = warped_image [:, :, 3 ] / 255.0 for c in range ( 3 ): # RGB channels result [:, :, c ] = ( result [:, :, c ] * ( 1 - alpha * mask ) + warped_image [:, :, c ] * ( alpha * mask ) ) # Update alpha channel result [:, :, 3 ] = np . maximum ( result [:, :, 3 ], warped_image [:, :, 3 ] * mask ) return result import numpy as np new_image = np . array ( image ) for ocr_result in ocr_results : if inplace : place_ocr_result_on_panorama ( new_image , ocr_result ) else : new_image = place_ocr_result_on_panorama ( new_image , ocr_result ) new_image = Image . fromarray ( new_image ) # Convert back to RGB new_image = new_image . convert ( \"RGB\" ) return new_image","title":"Visualization"},{"location":"api/visualization/#visualization","text":"OCR visualization utilities. Requires the [viz] extra. pip install \"panoocr[viz]\"","title":"Visualization"},{"location":"api/visualization/#functions","text":"","title":"Functions"},{"location":"api/visualization/#panoocr.ocr.utils.visualize_ocr_results","text":"visualize_ocr_results ( image : Image , ocr_results : List [ FlatOCRResult ], font_size : int = 16 , highlight_color : str = 'red' , stroke_width : int = 2 ) -> Image . Image Visualize flat OCR results on an image. Draws bounding boxes and labels on a copy of the image. Parameters: Name Type Description Default image Image The image to visualize OCR results on. required ocr_results List [ FlatOCRResult ] List of FlatOCRResult objects to visualize. required font_size int Font size for labels. 16 highlight_color str Color for boxes and text. 'red' stroke_width int Width of bounding box lines. 2 Returns: Type Description Image Copy of the image with OCR results visualized. Source code in src/panoocr/ocr/utils.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def visualize_ocr_results ( image : Image . Image , ocr_results : List [ FlatOCRResult ], font_size : int = 16 , highlight_color : str = \"red\" , stroke_width : int = 2 , ) -> Image . Image : \"\"\"Visualize flat OCR results on an image. Draws bounding boxes and labels on a copy of the image. Args: image: The image to visualize OCR results on. ocr_results: List of FlatOCRResult objects to visualize. font_size: Font size for labels. highlight_color: Color for boxes and text. stroke_width: Width of bounding box lines. Returns: Copy of the image with OCR results visualized. \"\"\" # Make a copy to avoid modifying the original result_image = image . copy () draw = ImageDraw . Draw ( result_image ) width , height = result_image . size try : font = ImageFont . load_default ( size = font_size ) except TypeError : # Older Pillow versions don't support size parameter font = ImageFont . load_default () for ocr_result in ocr_results : # Draw bounding box bbox = ocr_result . bounding_box draw . rectangle ( [ bbox . left * width , bbox . top * height , bbox . right * width , bbox . bottom * height , ], outline = highlight_color , width = 3 , ) # Draw text label draw . text ( ( bbox . left * width , bbox . top * height - font_size - stroke_width , ), ocr_result . text , fill = highlight_color , stroke_fill = \"white\" , stroke_width = stroke_width , font = font , ) # Draw confidence score draw . text ( ( bbox . left * width , bbox . bottom * height + stroke_width , ), f \" { ocr_result . confidence : .2f } \" , fill = highlight_color , stroke_fill = \"white\" , stroke_width = stroke_width , font = font , ) return result_image","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;visualize_ocr_results"},{"location":"api/visualization/#panoocr.ocr.utils.visualize_sphere_ocr_results","text":"visualize_sphere_ocr_results ( image : Image , ocr_results : List [ SphereOCRResult ], font_size : int = 16 , highlight_color : str = 'red' , stroke_width : int = 2 , inplace : bool = False ) -> Image . Image Visualize spherical OCR results on an equirectangular image. Projects OCR result labels back onto the panorama image. This is SLOW and should only be used for debugging purposes. Parameters: Name Type Description Default image Image The equirectangular panorama image. required ocr_results List [ SphereOCRResult ] List of SphereOCRResult objects to visualize. required font_size int Font size for labels (unused, size is automatic). 16 highlight_color str Color for boxes and text. 'red' stroke_width int Width of text stroke. 2 inplace bool If True, modify the input image directly (faster). False Returns: Type Description Image Image with OCR results visualized. Raises: Type Description ImportError If visualization dependencies are not installed. Source code in src/panoocr/ocr/utils.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def visualize_sphere_ocr_results ( image : Image . Image , ocr_results : List [ SphereOCRResult ], font_size : int = 16 , highlight_color : str = \"red\" , stroke_width : int = 2 , inplace : bool = False , ) -> Image . Image : \"\"\"Visualize spherical OCR results on an equirectangular image. Projects OCR result labels back onto the panorama image. This is SLOW and should only be used for debugging purposes. Args: image: The equirectangular panorama image. ocr_results: List of SphereOCRResult objects to visualize. font_size: Font size for labels (unused, size is automatic). highlight_color: Color for boxes and text. stroke_width: Width of text stroke. inplace: If True, modify the input image directly (faster). Returns: Image with OCR results visualized. Raises: ImportError: If visualization dependencies are not installed. \"\"\" _check_viz_dependencies () import numpy as np from scipy.ndimage import map_coordinates # Convert image to RGBA for alpha compositing image = image . convert ( \"RGBA\" ) def get_ocr_result_image ( ocr_result : SphereOCRResult ) -> Image . Image : \"\"\"Create an image for a single OCR result.\"\"\" PIXEL_PER_DEGREE = 300 text_image = Image . new ( \"RGBA\" , ( int ( ocr_result . width * PIXEL_PER_DEGREE ), int ( ocr_result . height * PIXEL_PER_DEGREE ), ), ( 255 , 255 , 255 , 0 ), ) draw = ImageDraw . Draw ( text_image ) try : font = ImageFont . load_default ( size = int ( ocr_result . height * PIXEL_PER_DEGREE * 0.2 ) ) except TypeError : font = ImageFont . load_default () # Draw bounding box draw . rectangle ( [ 0 , 0 , text_image . width , text_image . height ], outline = highlight_color , width = 3 , fill = ( 255 , 255 , 255 , 0 ), ) # Draw text draw . text ( ( text_image . width / 2 , text_image . height / 2 ), ocr_result . text , fill = highlight_color , anchor = \"mm\" , stroke_fill = \"white\" , stroke_width = stroke_width , font = font , ) return text_image def place_ocr_result_on_panorama ( panorama_array : np . ndarray , ocr_result : SphereOCRResult ) -> np . ndarray : \"\"\"Project an OCR result onto the panorama.\"\"\" ocr_result_image = np . array ( get_ocr_result_image ( ocr_result )) pano_height , pano_width = panorama_array . shape [: 2 ] ocr_height , ocr_width = ocr_result_image . shape [: 2 ] yaw_rad = np . radians ( - ocr_result . yaw ) pitch_rad = np . radians ( ocr_result . pitch ) width_rad = np . radians ( ocr_result . width ) height_rad = np . radians ( ocr_result . height ) # Create coordinate mappings for the panorama y_pano , x_pano = np . mgrid [ 0 : pano_height , 0 : pano_width ] # Convert panorama coordinates to spherical coordinates lon = ( x_pano / pano_width - 0.5 ) * 2 * np . pi lat = ( 0.5 - y_pano / pano_height ) * np . pi # Calculate 3D coordinates on the unit sphere x = np . cos ( lat ) * np . sin ( lon ) y = np . sin ( lat ) z = np . cos ( lat ) * np . cos ( lon ) # Combine rotation matrices sin_yaw , cos_yaw = np . sin ( yaw_rad ), np . cos ( yaw_rad ) sin_pitch , cos_pitch = np . sin ( pitch_rad ), np . cos ( pitch_rad ) # Apply rotation x_rot = cos_yaw * x + sin_yaw * z y_rot = sin_pitch * sin_yaw * x + cos_pitch * y - sin_pitch * cos_yaw * z z_rot = - cos_pitch * sin_yaw * x + sin_pitch * y + cos_pitch * cos_yaw * z # Project onto the plane epsilon = 1e-8 x_proj = x_rot / ( z_rot + epsilon ) y_proj = y_rot / ( z_rot + epsilon ) # Scale and shift to image coordinates x_img = ( x_proj / np . tan ( width_rad / 2 ) + 1 ) * ocr_width / 2 y_img = ( - y_proj / np . tan ( height_rad / 2 ) + 1 ) * ocr_height / 2 # Create mask for valid coordinates mask = ( ( x_img >= 0 ) & ( x_img < ocr_width ) & ( y_img >= 0 ) & ( y_img < ocr_height ) & ( z_rot > 0 ) ) # Sample from the OCR result image warped_channels = [] for channel in range ( ocr_result_image . shape [ 2 ]): warped_channel = map_coordinates ( ocr_result_image [:, :, channel ], [ y_img , x_img ], order = 1 , mode = \"constant\" , cval = 0 , ) warped_channels . append ( warped_channel ) warped_image = np . stack ( warped_channels , axis =- 1 ) if not inplace : result = panorama_array . copy () else : result = panorama_array # Apply alpha compositing alpha = warped_image [:, :, 3 ] / 255.0 for c in range ( 3 ): # RGB channels result [:, :, c ] = ( result [:, :, c ] * ( 1 - alpha * mask ) + warped_image [:, :, c ] * ( alpha * mask ) ) # Update alpha channel result [:, :, 3 ] = np . maximum ( result [:, :, 3 ], warped_image [:, :, 3 ] * mask ) return result import numpy as np new_image = np . array ( image ) for ocr_result in ocr_results : if inplace : place_ocr_result_on_panorama ( new_image , ocr_result ) else : new_image = place_ocr_result_on_panorama ( new_image , ocr_result ) new_image = Image . fromarray ( new_image ) # Convert back to RGB new_image = new_image . convert ( \"RGB\" ) return new_image","title":"<code class=\"doc-symbol doc-symbol-toc doc-symbol-function\"></code>&nbsp;visualize_sphere_ocr_results"}]}